{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "#LSTM GENERATOR\n",
    "import numpy as np\n",
    "import gzip\n",
    "import nltk\n",
    "from random import randint\n",
    "import torch\n",
    "import gzip\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "print \"here\"\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import linalg as LA\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_lookup_table(filename):\n",
    "    lookup={}\n",
    "    text_token_list=convert_to_list(filename)\n",
    "    for token in text_token_list:\n",
    "        lookup[token[0]] = {'title':token[1],'question':token[2]}\n",
    "    return lookup\n",
    "def convert_to_list(filename, is_android = False):\n",
    "    if filename.endswith('gz'):\n",
    "        with gzip.open(filename,'r')as f:\n",
    "            text_tokens = f.readlines()\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            text_tokens = f.readlines()\n",
    "    \n",
    "    text_tokens = [token.replace('\\n','').split('\\t') for token in text_tokens]\n",
    "    if is_android:\n",
    "        text_tokens = [token[0].split(' ') for token in text_tokens]\n",
    "    else:\n",
    "        text_tokens = [[token[0], token[1].split(' '), token[2].split(' ')] for token in text_tokens]\n",
    "                   \n",
    "    return text_tokens\n",
    "def convert_to_samples_android(pos_filename, neg_filename):\n",
    "    pos_list = convert_to_list(pos_filename, is_android = True)\n",
    "    neg_list = convert_to_list(neg_filename, is_android = True)\n",
    "\n",
    "    target_id_to_list_of_negative_ids = {}\n",
    "    for id_pair in neg_list:\n",
    "        target_id, negative_id = id_pair\n",
    "        if target_id in target_id_to_list_of_negative_ids:\n",
    "            target_id_to_list_of_negative_ids[target_id].append(negative_id)\n",
    "        else:\n",
    "            target_id_to_list_of_negative_ids[target_id] = [negative_id]\n",
    "    new_samples = []\n",
    "    for id_pair in pos_list:\n",
    "        target_id, positive_id = id_pair\n",
    "        negative_ids = target_id_to_list_of_negative_ids[target_id]\n",
    "        new_sample = [target_id, positive_id, negative_ids]\n",
    "        new_samples.append(new_sample)\n",
    "    return new_samples\n",
    "#takes  sample_ids of [[q1,p1,n1],[q2,p2,n2]....]\n",
    "#outputs titles like [[q1_title, p1_title, n1_title],[q2_title,p2_title,n2_title]...]\n",
    "def convert_sampleids_to_titles(sample_ids,lookup):\n",
    "    #each sample_id [question_id, pos_id, [neg_ids]]\n",
    "    #print type(sample_ids)==list, \"first\"\n",
    "   \n",
    "    titles = []\n",
    "    for sample_id in sample_ids:\n",
    "         #flatten list: [question_id, pos_id, [neg_ids]] --> [question_id, pos_id, neg_id1, neg_id2, ...]\n",
    "        sample_id= sample_id[:2]+sample_id[2][:]\n",
    "        #sample_id : question_id, similar_question_id, negative_question_id\n",
    "        try:\n",
    "            titles.append([[item.lower() for item in lookup[str(_)]['title']] for _ in sample_id])\n",
    "        except:\n",
    "            print \"Lookup failed\"\n",
    "    return titles\n",
    "def flatten_titles(titles):\n",
    "    return [\"\" + word for q_list in titles for q in q_list for word in q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ubuntu_text_tokenized_filename = 'askubuntu/text_tokenized.txt.gz'\n",
    "android_text_tokenized_filename = 'Android/corpus.tsv.gz'\n",
    "#ubuntu_data_lookup_table = make_lookup_table(ubuntu_text_tokenized_filename)\n",
    "android_data_lookup_table = make_lookup_table(android_text_tokenized_filename)\n",
    "\n",
    "android_dev_pos_filename = \"Android/dev.pos.txt\"\n",
    "android_dev_neg_filename = \"Android/dev.neg.txt\"\n",
    "\n",
    "android_test_pos_filename = \"Android/test.pos.txt\"\n",
    "android_test_neg_filename = \"Android/test.neg.txt\"\n",
    "\n",
    "android_dev_samples = convert_to_samples_android(android_dev_pos_filename, android_dev_neg_filename)\n",
    "android_test_samples = convert_to_samples_android(android_test_pos_filename, android_test_neg_filename)\n",
    "\n",
    "android_dev_titles_only = convert_sampleids_to_titles(android_dev_samples, android_data_lookup_table)\n",
    "android_test_titles_only = convert_sampleids_to_titles(android_test_samples, android_data_lookup_table)\n",
    "\n",
    "android_dev_words = flatten_titles(android_dev_titles_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# android_dev_words_targets_and_positives_only = [word for q_list in android_dev_titles_only for q in q_list[:2] for word in q]\n",
    "bigrams = nltk.bigrams(android_dev_words)\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 7\n",
    "def generate_question(cfdist, target_question):\n",
    "    generated_question = []\n",
    "    word = target_question[0]\n",
    "    generated_question.append(word)\n",
    "    for idx in range(len(target_question)):\n",
    "        try:\n",
    "            word2 = select_random_word_from_top_n(cfdist[word], N)\n",
    "            generated_question.append(word2)\n",
    "        except:\n",
    "            print word\n",
    "        \n",
    "    return generated_question\n",
    "\n",
    "def select_random_word_from_top_n(freq_dist_dict, n):\n",
    "    rand_idx = randint(0, n-1)\n",
    "    top_n = sorted(freq_dist_dict.iteritems(), key = lambda x: x[1], reverse = True)[:n]\n",
    "    return top_n[min(rand_idx, len(top_n)- 1)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['where', 'to', 'find', 'an', 'original', 'android', 'image', '?'], ['where', 'can', 'i', 'find', 'stock', 'or', 'custom', 'roms', 'for', 'my', 'android', 'device', '?'], ['increase', 'multi-touch', 'points'], ['cross-device', 'game', 'saves'], ['if', 'i', 'do', 'a', 'factory', 'reset', 'on', 'my', 'android', ',', 'will', 'i', 'lose', 'my', 'tracfone', 'talk', ',', 'text', ',', 'and', 'data', 'minutes', '?'], ['how', 'do', 'i', 'unlock', 'my', 'bootloader', 'with', 'omnius', '?'], ['android', 'restore', 'bootstrap', 'nandroid', 'backup', 'after', 'a', 'failed', 'rom', 'flash'], ['how', 'do', 'i', 'unlock', 'my', 'samsung', 'galaxy', 's2', 'after', 'many', 'attempts'], ['my', 'sim', 'is', 'not', 'recognized', 'after', 'installing', 'cyanogenmod', ';', 'what', 'should', 'i', 'do', '?'], ['nexus', 'one', 'stuck', 'in', 'loading', 'screen', '-', 'how', 'to', 'save', 'application', 'data', '?', '(', 'rooted', ')'], ['tablet', 'wo', \"n't\", 'connect', 'to', 'any', 'wifi', 'network', 'anymore', ',', 'always', 'shows', '``', 'saved', \"''\"], ['is', 'it', 'possible', 'to', 'do', 'an', 'advanced', 'compare', 'of', 'two', 'apps', 'based', 'on', 'instalation', 'history', '?'], ['my', 'samsumg', 's2', 'locked', '.', 'how', 'can', 'i', 'enble', 'the', 'usb', 'debugging', '?'], ['how', 'to', 'configure', 'simplenote', 'client', 'to', 'use', 'external', 'storage', '?'], ['define', 'gesture', 'in', 'es', 'file', 'explorer'], ['why', 'doesnt', 'my', 'phone', 'vibrate', 'for', 'messages'], ['upgrade', 'ram', 'memory', 'in', 'galaxy', 'y'], ['bluestacks', 'coc', 'stuck', 'at', 'loading'], ['ios', \"'\", 'quick', 'reply', 'feature', 'on', 'whatsapp', 'for', 'android'], ['galaxy', 's2', 'stuck', 'at', 'boot', '.', 'please', 'help'], ['samsung', 'galaxy', 's2', 'not', 'connecting', 'to', 'wifi', 'or', 'bluetooth'], ['rooting', 'desire', 'permanently'], ['touchpad', 'buttons', 'do', 'not', 'respond', 'after', 'flashing', 'kernel'], ['at', '&', 't', 'gs5', '(', 'g900a', ')', 'blank', 'screen'], ['best', 'codec', 'to', 'use', 'for', 'rockplayer', '?'], ['my', 'galaxy', 'tab', '10.1', 'fell', 'straight', 'from', 'the', 'bed', '.', 'will', 'this', 'influence', 'the', 'tab', 'life', '?'], ['video', 'stream', 'from', 'android', 'phone', 'to', 'laptop', 'over', 'the', 'internet'], ['android', 'phone', '+', 'mobile', 'network', '+', 'chromecast'], ['how', 'can', 'i', 'access', 'epst', 'on', 'the', 'motorola', 'droid', '1', '(', 'verizon', ')', '?'], ['cloning', 'room', 'key', 'card', 'by', 'using', 'nfc'], ['nexus', '5', 'updated', 'to', '5.0.1', 'using', 'ota', 'but', 'is', 'now', 'stuck', 'at', 'boot', 'animation', 'for', 'more', 'than', '10', 'minutes'], ['how', 'to', 'download', 'apps', 'without', 'a', 'wireless', 'data', 'plan'], ['tried', 'to', 'edit', 'framework-res.apk', 'on', 'kyocera', 'rise', 'stuck', 'in', 'bootloop'], ['how', 'do', 'i', 'flash', 'to', 'a', 'newer', 'android', 'version', 'when', 'twrp', 'has', 'trashed', 'recovery', 'image', 'and', 'boot', '?'], ['usb', 'storage', 'damaged-it', 'may', 'need', 'reformating', 'samsung', 'galaxy', 'shw-m110s'], ['detect', 'when', 'someone', 'has', 'answered', 'the', 'call'], ['how', 'to', 'boot', 'on', 'android', 'if', 'on', 'windows', '10', '?'], ['locked', 'samsung', 's5', 'with', 'adm'], ['control', 'android', 'tablet', \"'s\", 'interface', 'functions', 'via', 'ssh'], ['video', 'capturing', 'in', 'byte', 'format'], ['rooted', 'sgs3', 'and', 'kies', 'does', \"n't\", 'work', 'with', 'my', 'rom', ',', 'looking', 'for', 'contact', 'list', 'files'], ['how', 'do', 'i', 'get', 'private', 'bookmark', 'from', 'web', 'google', 'maps', 'to', 'android', 'google', 'maps', '?'], ['adb', 'push', 'does', 'not', 'work'], ['nowsecure', 'agent', 'non-stop', 'crashing', 'on', 'nexus', '5'], ['use', 'android', 'fully', 'independent', 'from', 'google', 'products'], ['android', 'updates', 'availability', 'in', 'different', 'countries'], ['htc', '626', 'dualsim', 'doesnt', 'turn', 'on'], ['configure', 'google', 'voice/hangouts', 'to', 'work', 'like', 't-mobile', \"'s\", 'wifi', 'calling', '?'], ['how', 'can', 'i', 'email', 'large', 'videos', 'to', 'someone', 'else', '?'], ['deleted', 'my', 'default', 'keyboard', 'program', 'and', 'i', 'no', 'longer', 'have', 'a', 'keyboard', '?'], ['sgs2', 'i9100g', 'keeps', 'disconnecting', 'from', 'wifi', 'after', 'installing', 'firmware', 'xxlpq'], ['reformatting', 'tablet', 'from', 'zip', 'file'], ['how', 'can', 'i', 'view', 'an', 'excel', 'file', 'in', 'ms', 'outlook', 'onedrive', 'on', 'my', 'phone', '?'], ['i', 'need', 'lenovo', 'a5500', 'nvram'], ['my', 'phone', 'glaxey', 's4', 'had', 'its', 'screen', 'crashed', 'and', 'ive', 'changed', 'it', 'and', 'now', 'it', 'wont', 'charge'], ['failed', 'to', 'update', 'htc', 'desire', 's', 'to', 'latest', 'firmware', 'with', 'android', '2.3.5'], ['can', 'i', 'get', 'android', 'to', 'not', 'expand', 'the', 'top', 'notification', '?'], ['how', 'do', 'i', 'hard', 'reset', 'my', 'samsung', 'galaxy', 's2', '?'], ['how', 'come', 'microsoft', 'products', 'can', 'be', 'used', 'on', 'android', '?'], ['lost', 'android', 'with', 'no', 'internet', 'connection'], ['galaxy', 's3', 'not', 'detected', 'as', 'media', 'device'], ['nexus', '5', ':', '/storage/emulated/0', 'not', 'mounted'], ['google', 'play', 'taking', 'forever', 'to', 'install', 'apps'], ['htc', 'one', 'v', 'really', 'slow'], ['whatsapp', '``', 'last', 'seen', \"''\", 'issue', '?'], ['how', 'to', 'control', 'additional', 'tethering', 'settings', 'on', 'android', 'phones', '?'], ['play', 'songs', 'randomly', 'and', 'endlessly'], ['headphones', 'buttons', 'do', \"n't\", 'operate', 'on', 'current', 'app'], ['devices', 'disconnect', 'from', 'cwm', 'when', 'trying', 'to', 'sideload'], ['trigger', '``', 'mount', 'as', 'disk', 'drive', \"''\", '(', 'aka', '``', 'usb', 'mass', 'storage', \"''\", 'mode', ')', 'from', 'pc'], ['how', 'to', 'enable', 'device', 'administrator', 'for', 'exchange', 'after', 'user', 'has', 'declined'], ['how', 'to', 'find', 'the', 'screen-size-in-points', 'of', 'a', 'samsung', 'galaxy', 's7'], ['question', 'regarding', 'slowdowns', 'and', 'swiping'], ['uninstall', 'the', 'applications', 'provided', 'by', 'the', 'company'], ['where', 'does', 'facebook', 'cache', 'video', '?'], ['can', 'not', 'copy', 'files', 'from', 'pc', 'to', 'galaxy', 'siii'], ['how', 'to', 'copy', 'text', 'on', 'android', 'like', 'on', 'a', 'computer', '?'], ['play', 'store', 'not', 'installed', 'error'], ['is', 'nexus', '7', '2013', 'lte', 'usable', 'as', 'a', 'phablet', '?'], ['set', 'the', 'calendar', 'display', 'to', 'the', 'correct', 'timezone', 'on', 'samsung', 'galaxy', 'nexus'], ['android', 'wo', \"n't\", 'factory', 'reset'], ['updating', 'apps', 'without', 'play', 'store'], ['process', 'com.android.systemui', 'has', 'stopped'], ['how', 'to', 'unlock', 'google', 'device', 'manager', 'remote', 'lock', '?'], ['connect', 'to', 'bluetooth', 'devices', 'via', 'android', 'connected', 'via', 'usb', '(', 'or', 'atleast', 'connect', 'to', 'a', 'bluetooth', 'headset', ')'], ['showing', 'both', '``', 'adb', 'status-window', \"''\", 'and', '``', 'adb', 'logcat', \"''\", 'on', 'the', 'same', 'terminal'], ['session', 'logout', 'on', 'device', 'with', 'multiple', 'users', '?'], ['imei', '=', 'null', 'in', 'samsung', 'galaxy', 'note', '3'], ['how', 'can', 'i', 'add', 'an', 'icon', 'to', 'the', 'miui', 'home', 'screen', '?'], ['how', 'do', 'i', 'mute', 'the', 'sound', 'when', 'opening', 'a', 'folder', 'in', 'the', 'home', 'screen', '?'], ['how', 'do', 'i', 'troubleshoot', 'my', 'ir', 'blaster', '(', 'samsung', 'galaxy', 'tab', '4', ')'], ['some', '2.1/2.2', 'apps', 'not', 'working'], ['how', 'to', 'disable', 'auto', 'hide', 'navigation', 'bar', 'in', 'cyanogenmod', '11', '?'], ['notification', 'icon', 'that', 'looks', 'like', \"'b\", \"'\", 'with', '3', 'dots'], ['is', 'wiping', 'the', 'data', 'partition', 'totally', 'unnecessary', 'after', 'this', 'adb', 'fail', 'and', 'stock', 'restore', '?'], ['play', 'store', 'downloading', 'of', 'updates', 'stuck', 'part', 'way', 'through', 'download'], ['app', 'to', 'external', 'sd', 'on', 'samsung', 'i9003'], ['how', 'do', 'i', 'protect', 'my', 'phone', 'from', 'malicious', 'apps', 'once', 'it', 'is', 'rooted', '?'], ['why', 'wo', \"n't\", 'my', 'samsung', 'galaxy', 'ace', 'let', 'me', 'update', 'my', 'apps', '?'], ['how', 'do', 'you', 'change', 'which', 'app', 'opens', 'when', 'usb', 'is', 'connected', '?'], ['how', 'to', 'create', 'a', 'new', 'gmail', 'contact', 'or', 'a', 'yahoo', 'mail', 'contact', 'from', 'android', 'device'], ['google', 'hangouts', 'keeps', 'on', 'adding', 'a', 'default', 'country', 'code', 'when', 'i', \"'m\", 'trying', 'to', 'make', 'an', 'international', 'call']]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_question' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f6f0790375ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mq_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtarget_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mgenerated_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mgenerated_questions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_question' is not defined"
     ]
    }
   ],
   "source": [
    "generated_questions = []\n",
    "for q_list in android_dev_titles_only:\n",
    "    print q_list\n",
    "    target_q = q_list[0]\n",
    "    generated_question = generate_question(cfd, target_q)\n",
    "    generated_questions.append(generated_question)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------HELPER FUNCTIONS-------------------------------------#\n",
    "NUM_NEGATIVE_SAMPLES=20\n",
    "\n",
    "def N_random_values_in_list(full_list, N):\n",
    "    x=0\n",
    "    lower_bound  = 0\n",
    "    upper_bound = len(full_list)-1\n",
    "    sample_list=[]\n",
    "    random_nums=[]\n",
    "    while x < min(N,len(full_list)):\n",
    "        random_num = randint(lower_bound, upper_bound) # inclusive range\n",
    "        if random_num in random_nums:\n",
    "            continue\n",
    "        else:\n",
    "            random_nums.append(random_num)\n",
    "            x += 1\n",
    "    return [full_list[i] for i in random_nums]\n",
    "\n",
    "def convert_to_list(filename, is_android = False):\n",
    "    if filename.endswith('gz'):\n",
    "        with gzip.open(filename,'r')as f:\n",
    "            text_tokens = f.readlines()\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            text_tokens = f.readlines()\n",
    "    \n",
    "    text_tokens = [token.replace('\\n','').split('\\t') for token in text_tokens]\n",
    "    if is_android:\n",
    "        text_tokens = [token[0].split(' ') for token in text_tokens]\n",
    "    else:\n",
    "        text_tokens = [[token[0], token[1].split(' '), token[2].split(' ')] for token in text_tokens]\n",
    "                   \n",
    "    return text_tokens\n",
    "\n",
    "#Sample:question_id, similar_question_id, negative_question_id\n",
    "def convert_to_samples(filename, is_dev_or_test = False):\n",
    "    my_list=convert_to_list(filename)\n",
    "    new_samples=[]\n",
    "    for original_sample in my_list:\n",
    "        if is_dev_or_test:\n",
    "            new_samples.append([original_sample[0], similar, negative_samples])\n",
    "        else:\n",
    "            for similar in original_sample[1]:\n",
    "                random_negative_samples = N_random_values_in_list(original_sample[2],NUM_NEGATIVE_SAMPLES)\n",
    "                new_samples.append([original_sample[0], similar, random_negative_samples])# change this to include all negative \n",
    "                                                                                     # examples later\n",
    "    return new_samples\n",
    "def make_lookup_table_for_training_data(filename):\n",
    "    lookup={}\n",
    "    text_token_list=convert_to_list(filename)\n",
    "    for token in text_token_list:\n",
    "        lookup[token[0]] = {'title':token[1],'question':token[2]}\n",
    "    return lookup\n",
    "        \n",
    "#takes  sample_ids of [[q1,p1,n1],[q2,p2,n2]....]\n",
    "#outputs titles like [[q1_title, p1_title, n1_title],[q2_title,p2_title,n2_title]...]\n",
    "def convert_sampleids_to_titles(sample_ids,lookup, is_dev_or_test = False):\n",
    "    #each sample_id [question_id, pos_id, [neg_ids]]\n",
    "    #print type(sample_ids)==list, \"first\"\n",
    "   \n",
    "    titles = []\n",
    "   # print \"sample_ids\", sample_ids\n",
    "    for sample_id in sample_ids:\n",
    "        if is_dev_or_test:\n",
    "            #omit similar questions, only needed for evaluation\n",
    "            sample_id=[sample_id[0]]+sample_id[2][:]\n",
    "         #flatten list: [question_id, pos_id, [neg_ids]] --> [question_id, pos_id, neg_id1, neg_id2, ...]\n",
    "        else:\n",
    "            sample_id= sample_id[:2]+sample_id[2][:]\n",
    "        #sample_id : question_id, similar_question_id, negative_question_id\n",
    "        #try:\n",
    "        current=[]\n",
    "        for identity in sample_id:\n",
    "            try:\n",
    "                current.append(lookup[str(identity)]['title'])\n",
    "            except:\n",
    "                print identity\n",
    "        titles.append(current)\n",
    "           # print type(sample_id)==list\n",
    "        #except:\n",
    "        #    print sample_id, \"is sample id\", type(sample_id)==list\n",
    "    return titles\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "def extract_features(word):\n",
    "    try:\n",
    "        word=remove_non_ascii(word)\n",
    "        word=word.encode('utf-8')\n",
    "    except:\n",
    "        print(word)\n",
    "    return word_to_vec.get(word,[0.0 for i in range(300)])\n",
    "\n",
    "def find_maximum_title_and_body_length(lookup_table):\n",
    "    max_len_title = -1\n",
    "    max_len_question = -1\n",
    "    max_len_question_id = 0\n",
    "    for key, dict_val in lookup_table.iteritems():\n",
    "        len_title = len(dict_val['title'])\n",
    "        len_question = len(dict_val['question'])\n",
    "        if len_title > max_len_title:\n",
    "             max_len_title = len_title\n",
    "        if len_question > max_len_question:\n",
    "            max_len_question = len_question\n",
    "            max_len_question_id = key\n",
    "    return max_len_title, max_len_question\n",
    "\n",
    "def title_to_feature_matrix(title_word_list):\n",
    "    feature_matrix = []\n",
    "    for idx, word in enumerate(title_word_list):\n",
    "        if idx == PARAMETER_MAX_TITLE_LENGTH:\n",
    "            break\n",
    "        else:\n",
    "            word_features = extract_features(word)\n",
    "            feature_matrix.append(word_features)\n",
    "        \n",
    "    #Pad the feature with zeros to ensure all inputs to the net have the same dimension\n",
    "    feature_matrix += [[0] * NUM_FEATURES_PER_WORD] * (PARAMETER_MAX_TITLE_LENGTH - len(title_word_list))\n",
    "    #print np.array(feature_matrix).T.shape\n",
    "    return np.array(feature_matrix).T\n",
    "\n",
    "\n",
    "\n",
    "#array is structured like a batch of features 50x200x38\n",
    "def find_start_of_padding_for_batch(batch):\n",
    "    vec_lengths_in_batch = []\n",
    "    for batch_num in range(0, len(batch)):\n",
    "        single_vec = batch[batch_num]\n",
    "        length = find_start_of_padding_single_vec(single_vec) + 1\n",
    "        vec_lengths_in_batch.append(length)\n",
    "    return vec_lengths_in_batch\n",
    "\n",
    "#batch = 200x38\n",
    "def find_start_of_padding_single_vec(single_vec):\n",
    "    for idx in range(len(single_vec[0])-1, -1, -1):\n",
    "        if single_vec[0][idx] != 0.:\n",
    "            return idx\n",
    "    #if the whole sequence is 0s\n",
    "    return 0\n",
    "def create_mask(word_length):\n",
    "    return np.array([[1. / word_length] * HIDDEN_SIZE] * word_length + [[0] * HIDDEN_SIZE] * (MAX_TITLE_LENGTH - word_length)).T\n",
    "#-------------------------------------CREATE DATA BATCHER-------------------------------------#\n",
    "# where samples[0] = 1 (target) + 1 (positive) + n (negative)\n",
    "# \n",
    "def create_data_loader(samples, shuffle_data = True, is_test_or_dev = False):\n",
    "    features = []\n",
    "    for sample in samples:\n",
    "        target_title = sample[0]\n",
    "        positive_title = sample[1]\n",
    "        negative_titles = sample[2:]\n",
    "\n",
    "        target_features = title_to_feature_matrix(target_title)\n",
    "        print target_features, \"is taret features\"\n",
    "        positive_features = title_to_feature_matrix(positive_title)\n",
    "        if len(negative_titles)==0:\n",
    "            all_features=[target_features, positive_features]\n",
    "        else:\n",
    "            n_negative_features = [title_to_feature_matrix(negative_title) for negative_title in negative_titles]\n",
    "\n",
    "            all_features = [target_features, positive_features] + n_negative_features\n",
    "        #print all_features, \"is all\"\n",
    "       # if is_test_or_dev:\n",
    "       #     all_features = [target_features,]\n",
    "        features.append(all_features)\n",
    "\n",
    "            \n",
    "    targets = torch.LongTensor(len(features)).zero_()\n",
    "    dataset = data_utils.TensorDataset(torch.FloatTensor(features), targets)\n",
    "    data_loader = data_utils.DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = shuffle_data)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                    #---------------TAKES 7 MINUTES ---------------#\n",
    "#-------------------------------------GENERATE EMBEDDINGS-------------------------------------#\n",
    "import zipfile\n",
    "# word_embeddings = 'askubuntu/vector/vectors_pruned.200.txt.gz'\n",
    "word_embeddings = 'glove.txt'\n",
    "# f = gzip.open(word_embeddings, 'r')\n",
    "f = open(word_embeddings, 'r')\n",
    "wv_text = []\n",
    "lines = f.readlines()\n",
    "#%%%%%%%%%% TAKES ABOUT 2 MINUTES\n",
    "for idx, line in enumerate(lines):\n",
    "    wv_text.append(line.strip())\n",
    "\n",
    "word_to_vec = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'HI'.islower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#%%%%%%%%%% TAKES ABOUT 4 MINUTES\n",
    "i=0\n",
    "for line in wv_text:\n",
    "    if i%100000==0:\n",
    "        print i\n",
    "    parts = line.split()\n",
    "    word = parts[0]\n",
    "    vector = np.array([float(v) for v in parts[1:]])\n",
    "    if word.islower():\n",
    "        word_to_vec[word] = vector\n",
    "    i+=1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------CNN-------------------------------------#\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(200, hidden_dim, KERNEL_SIZE, padding = PADDING),\n",
    "#             nn.ReLU()\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data_loader(samples, shuffle_data = True, is_test_or_dev = False):\n",
    "    features = []\n",
    "    for sample in samples:\n",
    "        target_title = sample[0]\n",
    "        positive_title = sample[1]\n",
    "        negative_titles = sample[2:]\n",
    "\n",
    "        target_features = title_to_feature_matrix(target_title)\n",
    "        positive_features = title_to_feature_matrix(positive_title)\n",
    "        if len(negative_titles)==0:\n",
    "            all_features=[target_features, positive_features]\n",
    "        else:\n",
    "            n_negative_features = [title_to_feature_matrix(negative_title) for negative_title in negative_titles]\n",
    "\n",
    "            all_features = [target_features, positive_features] + n_negative_features\n",
    "        #print all_features, \"is all\"\n",
    "       # if is_test_or_dev:\n",
    "       #     all_features = [target_features,]\n",
    "        features.append(all_features)\n",
    "        \n",
    "\n",
    "    print(len(features), \"is feature len\")\n",
    "    np_array=np.array(features)\n",
    "    print (np_array.shape, \"is features shape\")\n",
    "    targets = torch.LongTensor(len(features)).zero_()\n",
    "    dataset = data_utils.TensorDataset(torch.FloatTensor(features), targets)\n",
    "    data_loader = data_utils.DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = shuffle_data)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAINING_EXAMPLES=1000\n",
    "print \"ok\"\n",
    "KERNEL_SIZE = 3 #MAKE SURE THIS NUMBER IS ODD SO THAT THE PADDING MAKES SENSE\n",
    "PADDING = (KERNEL_SIZE - 1) / 2\n",
    "INPUT_SIZE = 300\n",
    "HIDDEN_SIZE = 600\n",
    "LEARNING_RATE = 1e-4\n",
    "MARGIN = 0.2\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 50\n",
    "print \"hi\"\n",
    "NUM_BATCHES = NUM_TRAINING_EXAMPLES/BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_lookup_table(filename):\n",
    "    lookup={}\n",
    "    text_token_list=convert_to_list(filename)\n",
    "    for token in text_token_list:\n",
    "        lookup[token[0]] = {'title':token[1],'question':token[2]}\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def create_android_generated_samples(num_samples,text_token_list):\n",
    "    print \"done\"\n",
    "    samples = []\n",
    "    for i in range(num_samples):\n",
    "        id_num=text_token_list[i]\n",
    "        #print id_num\n",
    "        target_title = android_data_lookup_table[id_num[0]]['title']\n",
    "        target_title = [item.lower() for item in target_title]\n",
    "       #print target_title\n",
    "        positive_title = generate_question(cfd, target_title)\n",
    "        negative_ids = [text_token_list[random.randint(0,len(text_token_list)-1)] for j in range(20)]\n",
    "        #print negative_ids\n",
    "        negative_titles = [android_data_lookup_table[id_number[0]]['title'] for id_number in negative_ids]\n",
    "        sample_to_add = [target_title]+[positive_title]+negative_titles\n",
    "        print sample_to_add, \"is sample\"\n",
    "        samples.append(sample_to_add)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_token_list=convert_to_list(android_text_tokenized_filename, is_android=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_token_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "undeleteable\n",
      "undeleteable\n",
      "undeleteable\n",
      "undeleteable\n",
      "undeleteable\n",
      "undeleteable\n"
     ]
    }
   ],
   "source": [
    "samples = create_android_generated_samples(1000, text_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_to_feature_matrix(samples[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARAMETER_MAX_TITLE_LENGTH=38\n",
    "NUM_FEATURES_PER_WORD=300\n",
    "final_train_data_loader=create_data_loader(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "0"
     ]
    }
   ],
   "source": [
    "MAX_TITLE_LENGTH=38\n",
    "NUM_FEATURES_PER_WORD=300\n",
    "INPUT_DIM = (MAX_TITLE_LENGTH, NUM_FEATURES_PER_WORD)\n",
    "net = CNN(INPUT_SIZE, HIDDEN_SIZE)\n",
    "#net=torch.load(\"Net_Epoch_8\")\n",
    "criterion = nn.MultiMarginLoss(p=1, margin=MARGIN, weight = None, size_average=True) #HAHA just put these in to look smart \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "print \"Beginning training...\"\n",
    "# ----TRAINING\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for idx,(sample, label) in enumerate(final_train_data_loader):\n",
    "        print idx,\n",
    "#     for idx in range(NUM_BATCHES):\n",
    "        #Sample shape: [50, 22, 200, 38]\n",
    "        #50 - Batch size, 22 - Num questions per data point\n",
    "        sample = Variable(sample, requires_grad = True)\n",
    "        label = Variable(label, requires_grad = True)\n",
    "\n",
    "        #RE-ORDER DIMENSIONS OF THE SAMPLE\n",
    "        sample = sample.permute(1, 0, 2, 3)\n",
    "        \n",
    "        target_question_features = sample[0] # 50 x 200 x 38\n",
    "        positive_question_features = sample[1] # 50 x 200 x 38\n",
    "        N_negative_question_features = sample[2:] #20 x 50 x 200 x 38\n",
    "        \n",
    "        #Determine lengths to know how many vectors to take the average across.\n",
    "        target_question_lengths = find_start_of_padding_for_batch(target_question_features.data)\n",
    "        positive_question_lengths = find_start_of_padding_for_batch(positive_question_features.data)\n",
    "        N_negative_questions_lengths = [find_start_of_padding_for_batch(negative.data) for negative in N_negative_question_features]\n",
    "        \n",
    "        #RUN THROUGH NET\n",
    "        target_question_net_output = net(target_question_features)\n",
    "        positive_question_net_output = net(positive_question_features)\n",
    "        N_negative_question_net_outputs = [net(negative) for negative in N_negative_question_features]\n",
    "        \n",
    "        #CREATE MASKS\n",
    "        target_questions_masks = [create_mask(_) for _ in target_question_lengths] #DIM = 50 x 100 x 38\n",
    "        positive_questions_masks = [create_mask(_) for _ in positive_question_lengths] #DIM = 50 x 100 x 38\n",
    "        N_negative_questions_masks = [[create_mask(length) for length in length_list] #DIM = 50 x 20 x 100 x 38\n",
    "                                      for length_list in N_negative_questions_lengths]\n",
    "        \n",
    "        #APPLY MASKS\n",
    "        #Should the multiplicands, the masks, be Float Tensors or Variables? May have to be float tensors to ensure\n",
    "        #    pytorch's directed graph back-prop is maintained.\n",
    "\n",
    "        target_question_net_output_masked = target_question_net_output * Variable(torch.FloatTensor(target_questions_masks))\n",
    "        positive_question_net_output_masked = positive_question_net_output * Variable(torch.FloatTensor(positive_questions_masks))\n",
    "        N_negative_questions_net_output_masked = [N_negative_question_net_outputs[idx] * \n",
    "                                                  Variable(torch.FloatTensor(N_negative_questions_masks[idx]))\n",
    "                                                  for idx in range(NUM_NEGATIVE_SAMPLES)]\n",
    "\n",
    "        #SUM OVER WORDS\n",
    "        target_question_net_output_masked_summed = torch.sum(target_question_net_output_masked, dim = 2) #DIM = 50 x 100\n",
    "        positive_question_net_output_masked_summed = torch.sum(positive_question_net_output_masked, dim = 2) #DIM = 50 x 100\n",
    "        N_negative_questions_net_output_masked_summed = [torch.sum(\n",
    "                                                        N_negative_questions_net_output_masked[idx], dim = 2\n",
    "                                                        )for idx in range(NUM_NEGATIVE_SAMPLES)] #DIM = 20 x 50 x 100\n",
    "\n",
    "        \n",
    "        cosine_similarity_pos = cos(target_question_net_output_masked_summed, positive_question_net_output_masked_summed)\n",
    "        # ^ DIM = 50\n",
    "        cosine_similarities_neg = [cos(target_question_net_output_masked_summed, N_negative_questions_net_output_masked_summed[idx])\n",
    "                                  for idx in range(NUM_NEGATIVE_SAMPLES)]\n",
    "        # ^ DIM = 20 x 50\n",
    "        #\n",
    "\n",
    "        cosine_similarities = torch.stack([cosine_similarity_pos] + cosine_similarities_neg) # DIM = 21 x 50\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cosine_similarities = torch.t(cosine_similarities)\n",
    "        label = torch.squeeze(label)\n",
    "        loss = criterion(cosine_similarities, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    torch.save(net, \"Net_Epoch_1e5_{}\".format(epoch))\n",
    "    print \"\"\n",
    "    print \"Loss after epoch \" + str(epoch) + \" :\" + str(running_loss)\n",
    "    data = evaluate_model_on_test(net, 'Android/dev.txt')\n",
    "    eval = Evaluation(data)\n",
    "    print \"MAP: \",eval.MAP()\n",
    "    print \"MRR: \", eval.MRR()\n",
    "    print \"P@1: \",eval.Precision(1)\n",
    "    print \"P@5: \",eval.Precision(5)\n",
    "    print \"test\"\n",
    "    data = evaluate_model_on_test(net, 'askubuntu/test.txt')\n",
    "    eval = Evaluation(data)\n",
    "    print \"MAP: \",eval.MAP()\n",
    "    print \"MRR: \", eval.MRR()\n",
    "    print \"P@1: \",eval.Precision(1)\n",
    "    print \"P@5: \",eval.Precision(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_token_list=convert_to_list(android_text_tokenized_filename, is_android=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Code took from PyTorchNet (https://github.com/pytorch/tnt)\n",
    "\n",
    "'''\n",
    "\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Meter(object):\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def add(self):\n",
    "        pass\n",
    "\n",
    "    def value(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AUCMeter(Meter):\n",
    "    \"\"\"\n",
    "    The AUCMeter measures the area under the receiver-operating characteristic\n",
    "    (ROC) curve for binary classification problems. The area under the curve (AUC)\n",
    "    can be interpreted as the probability that, given a randomly selected positive\n",
    "    example and a randomly selected negative example, the positive example is\n",
    "    assigned a higher score by the classification model than the negative example.\n",
    "\n",
    "    The AUCMeter is designed to operate on one-dimensional Tensors `output`\n",
    "    and `target`, where (1) the `output` contains model output scores that ought to\n",
    "    be higher when the model is more convinced that the example should be positively\n",
    "    labeled, and smaller when the model believes the example should be negatively\n",
    "    labeled (for instance, the output of a signoid function); and (2) the `target`\n",
    "    contains only values 0 (for negative examples) and 1 (for positive examples).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AUCMeter, self).__init__()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # self.scores = torch.DoubleTensor(torch.DoubleStorage()).numpy()\n",
    "        # self.targets = torch.LongTensor(torch.LongStorage()).numpy()\n",
    "        self.scores = []\n",
    "        self.targets = []\n",
    "\n",
    "    def add(self, output, target):\n",
    "        if torch.is_tensor(output):\n",
    "            output = output.cpu().squeeze().numpy()\n",
    "        if torch.is_tensor(target):\n",
    "            target = target.cpu().squeeze().numpy()\n",
    "        # elif isinstance(target, numbers.Number):\n",
    "        #     target = np.asarray([target])\n",
    "        # assert np.ndim(output) == 1, \\\n",
    "        #     'wrong output size (1D expected)'\n",
    "        # assert np.ndim(target) == 1, \\\n",
    "        #     'wrong target size (1D expected)'\n",
    "        # assert output.shape[0] == target.shape[0], \\\n",
    "        #     'number of outputs and targets does not match'\n",
    "        # assert np.all(np.add(np.equal(target, 1), np.equal(target, 0))), \\\n",
    "        #     'targets should be binary (0, 1)'\n",
    "\n",
    "        # self.scores = np.append(self.scores, output)\n",
    "        # self.targets = np.append(self.targets, target)\n",
    "        self.sortind = None\n",
    "        self.scores.append(output)\n",
    "        self.scores.append(target)\n",
    "\n",
    "\n",
    "    def value(self, max_fpr=1.0):\n",
    "#         self.scores = torch.FloatTensor(self.scores)\n",
    "#         self.target = torch.LongTensor(self.targets)\n",
    "        \n",
    "        assert max_fpr > 0\n",
    "\n",
    "        # case when number of elements added are 0\n",
    "#         if self.scores.shape[0] == 0:\n",
    "#             return 0.5\n",
    "\n",
    "        # sorting the arrays\n",
    "        if self.sortind is None:\n",
    "            scores, sortind = torch.sort(torch.from_numpy(self.scores), dim=0, descending=True)\n",
    "            scores = scores.numpy()\n",
    "            self.sortind = sortind.numpy()\n",
    "        else:\n",
    "            scores, sortind = self.scores, self.sortind\n",
    "\n",
    "        # creating the roc curve\n",
    "        tpr = np.zeros(shape=(scores.size + 1), dtype=np.float64)\n",
    "        fpr = np.zeros(shape=(scores.size + 1), dtype=np.float64)\n",
    "\n",
    "        for i in range(1, scores.size + 1):\n",
    "            if self.targets[sortind[i - 1]] == 1:\n",
    "                tpr[i] = tpr[i - 1] + 1\n",
    "                fpr[i] = fpr[i - 1]\n",
    "            else:\n",
    "                tpr[i] = tpr[i - 1]\n",
    "                fpr[i] = fpr[i - 1] + 1\n",
    "\n",
    "        tpr /= (self.targets.sum() * 1.0)\n",
    "        fpr /= ((self.targets - 1.0).sum() * -1.0)\n",
    "\n",
    "        for n in range(1, scores.size + 1):\n",
    "            if fpr[n] >= max_fpr:\n",
    "                break\n",
    "\n",
    "        # calculating area under curve using trapezoidal rule\n",
    "        #n = tpr.shape[0]\n",
    "        h = fpr[1:n] - fpr[0:n - 1]\n",
    "        sum_h = np.zeros(fpr.shape)\n",
    "        sum_h[0:n - 1] = h\n",
    "        sum_h[1:n] += h\n",
    "        area = (sum_h * tpr).sum() / 2.0\n",
    "\n",
    "        return area / max_fpr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
