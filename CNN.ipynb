{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_list(filename):\n",
    "    if filename.endswith('gz'):\n",
    "        with gzip.open(filename,'r')as f:\n",
    "            text_tokens = f.readlines()\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            text_tokens = f.readlines()\n",
    "    text_tokens = [token.replace('\\n','').split('\\t') for token in text_tokens]\n",
    "    text_tokens = [[token[0], token[1].split(' '), token[2].split(' ')] for token in text_tokens]\n",
    "                   \n",
    "    return text_tokens\n",
    "\n",
    "#Sample:question_id, similar_question_id, negative_question_id\n",
    "def convert_to_samples(filename):\n",
    "    my_list=convert_to_list(filename)\n",
    "    new_samples=[]\n",
    "    for original_sample in my_list:\n",
    "        for similar in original_sample[1]:\n",
    "            new_samples.append([original_sample[0], similar, original_sample[2][0]])# change this to include all negative \n",
    "                                                                                     # examples later\n",
    "    return new_samples\n",
    "def make_lookup_table_for_training_data(filename):\n",
    "    lookup={}\n",
    "    text_token_list=convert_to_list(filename)\n",
    "    for token in text_token_list:\n",
    "        lookup[token[0]] = {'title':token[1],'question':token[2]}\n",
    "    return lookup\n",
    "        \n",
    "#takes  sample_ids of [[q1,p1,n1],[q2,p2,n2]....]\n",
    "#outputs titles like [[q1_title, p1_title, n1_title],[q2_title,p2_title,n2_title]...]\n",
    "def convert_sampleids_to_titles(sample_ids,lookup):\n",
    "    #each sample_id [question_id, pos_id, neg_id]\n",
    "    titles = []\n",
    "    for sample_id in sample_ids:\n",
    "        #sample_id : question_id, similar_question_id, negative_question_id\n",
    "        try:\n",
    "            titles.append([lookup[str(identity)]['title'] for identity in sample_id])\n",
    "        except:\n",
    "            print sample_id\n",
    "    return titles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text_tokenized.txt.gz has id \\t title \\t question body\n",
    "text_tokenized='askubuntu/text_tokenized.txt.gz'\n",
    "\n",
    "#train_random.txt\n",
    "#(1) the query question ID, (2) the list of similar question IDs, and (3) the list of randomly selected question IDs.\n",
    "train_random_filename='askubuntu/train_random.txt'\n",
    "\n",
    "#Each line contains (1) the query question ID, (2) the list of similar question IDs, (3) the list of 20 candidate question IDs and (4) the associated BM25 scores of these questions computed by the Lucene search engine. The second field (the set of similar questions) is a subset of the third field.\n",
    "dev_filename='askubuntu/dev.txt'\n",
    "test_filename='askubuntu/test.txt'\n",
    "\n",
    "train_samples = convert_to_samples(train_random_filename)\n",
    "dev_samples = convert_to_samples(dev_filename)\n",
    "test_samples = convert_to_samples(test_filename)\n",
    "\n",
    "lookup = make_lookup_table_for_training_data(text_tokenized)\n",
    "train_list = convert_to_list(train_random_filename)\n",
    "train_titles_only = convert_sampleids_to_titles(train_samples, lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "word_embeddings = 'askubuntu/vector/vectors_pruned.200.txt.gz'\n",
    "f = gzip.open(word_embeddings, 'r')\n",
    "wv_text = [ ]\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    wv_text.append(line.strip())\n",
    "\n",
    "word_to_vec = {}\n",
    "\n",
    "for line in wv_text:\n",
    "    parts = line.split()\n",
    "    word = parts[0]\n",
    "    vector = np.array([float(v) for v in parts[1:]])\n",
    "    word_to_vec[word] = vector\n",
    "f.close()\n",
    "\n",
    "def extract_features(word):\n",
    "    try:\n",
    "        word=remove_non_ascii(word)\n",
    "        word=word.encode('utf-8')\n",
    "    except:\n",
    "        print(word)\n",
    "    return word_to_vec.get(word,[0.0 for i in range(200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.81100000e-03,  -1.91690000e-02,   3.72350000e-02,\n",
       "         1.88340000e-02,   7.88700000e-03,  -6.64640000e-02,\n",
       "         8.59540000e-02,  -8.24730000e-02,   1.83440000e-02,\n",
       "        -8.74610000e-02,  -1.11447000e-01,  -3.71180000e-02,\n",
       "        -8.30300000e-03,   1.40190000e-02,   8.45250000e-02,\n",
       "         2.03400000e-03,  -2.82450000e-02,  -1.62900000e-02,\n",
       "         7.20170000e-02,   6.04480000e-02,  -1.39264000e-01,\n",
       "        -1.63540000e-02,   1.74352000e-01,   3.68500000e-03,\n",
       "        -9.88580000e-02,  -4.12110000e-02,  -4.93310000e-02,\n",
       "        -2.85100000e-02,  -9.08550000e-02,  -2.40180000e-02,\n",
       "        -9.21600000e-03,  -3.20450000e-02,  -3.30730000e-02,\n",
       "        -2.61440000e-02,   1.22893000e-01,  -3.29470000e-02,\n",
       "        -9.37550000e-02,   1.87461000e-01,   1.35400000e-03,\n",
       "         7.04130000e-02,   3.89100000e-03,  -2.50700000e-02,\n",
       "        -1.38250000e-02,   5.29970000e-02,  -9.65200000e-02,\n",
       "         5.73710000e-02,  -1.60480000e-02,   3.27330000e-02,\n",
       "         5.89300000e-02,   4.26530000e-02,  -1.06370000e-02,\n",
       "         4.93350000e-02,   8.89100000e-02,  -2.64200000e-03,\n",
       "         2.14790000e-02,  -1.32110000e-01,   8.38400000e-02,\n",
       "        -1.45993000e-01,  -5.50810000e-02,  -4.46940000e-02,\n",
       "         2.99750000e-02,   7.34200000e-03,  -1.41483000e-01,\n",
       "        -1.34992000e-01,   7.93700000e-03,   1.45378000e-01,\n",
       "        -3.50880000e-02,   6.77400000e-02,  -1.71040000e-02,\n",
       "         1.76290000e-02,   4.40340000e-02,  -7.18210000e-02,\n",
       "         1.02433000e-01,   3.35520000e-02,   2.11950000e-02,\n",
       "         2.98280000e-02,  -1.54515000e-01,  -2.36050000e-02,\n",
       "         3.20910000e-02,  -1.22121000e-01,  -4.24300000e-02,\n",
       "         1.00603000e-01,  -6.18580000e-02,   3.81820000e-02,\n",
       "        -4.36510000e-02,  -3.56280000e-02,  -6.03380000e-02,\n",
       "         1.80000000e-04,   2.48590000e-02,   7.88110000e-02,\n",
       "        -4.44400000e-02,  -2.40390000e-02,  -4.79100000e-02,\n",
       "         1.14525000e-01,   5.66190000e-02,  -5.52450000e-02,\n",
       "        -4.29180000e-02,  -1.61904000e-01,   1.26530000e-02,\n",
       "        -6.81030000e-02,  -3.02740000e-02,   5.56300000e-03,\n",
       "        -2.04090000e-01,   3.45900000e-03,  -4.42810000e-02,\n",
       "        -3.35830000e-02,   1.15700000e-03,  -3.03010000e-02,\n",
       "        -1.24050000e-02,  -9.53550000e-02,  -3.93550000e-02,\n",
       "         6.22650000e-02,  -4.42020000e-02,   8.81300000e-03,\n",
       "        -4.53580000e-02,  -5.34300000e-03,  -1.47241000e-01,\n",
       "         1.08890000e-02,   1.23100000e-02,   1.21187000e-01,\n",
       "         1.01320000e-02,  -3.62640000e-02,  -6.49760000e-02,\n",
       "         5.65480000e-02,   4.79890000e-02,   1.92400000e-03,\n",
       "         3.64970000e-02,  -2.26600000e-03,   1.00555000e-01,\n",
       "         3.12180000e-02,  -4.39490000e-02,  -1.24266000e-01,\n",
       "        -1.14630000e-02,  -1.15645000e-01,  -9.02710000e-02,\n",
       "        -8.23100000e-03,   3.45000000e-04,  -4.42670000e-02,\n",
       "        -5.25660000e-02,  -3.55760000e-02,  -7.44500000e-03,\n",
       "        -8.52170000e-02,  -1.80300000e-02,  -6.08760000e-02,\n",
       "         1.16299000e-01,   7.59800000e-03,  -2.05000000e-02,\n",
       "        -7.56390000e-02,  -9.87520000e-02,  -1.76250000e-02,\n",
       "        -1.02854000e-01,  -1.51860000e-01,  -9.26320000e-02,\n",
       "         6.58400000e-03,  -2.72720000e-02,  -5.68420000e-02,\n",
       "        -5.13840000e-02,  -4.56000000e-03,  -1.53380000e-02,\n",
       "         4.77810000e-02,   1.58880000e-02,   9.67640000e-02,\n",
       "         3.90780000e-02,   5.76590000e-02,  -1.34597000e-01,\n",
       "         5.30760000e-02,   1.08641000e-01,  -1.65740000e-02,\n",
       "        -3.55110000e-02,   1.21346000e-01,  -5.96160000e-02,\n",
       "        -2.46370000e-02,   9.78130000e-02,  -9.35810000e-02,\n",
       "        -1.11922000e-01,  -1.29078000e-01,   1.11417000e-01,\n",
       "        -1.19060000e-02,  -9.07660000e-02,  -6.25830000e-02,\n",
       "        -7.62640000e-02,   1.75780000e-02,  -1.09919000e-01,\n",
       "        -3.83820000e-02,   4.49640000e-02,  -1.06710000e-02,\n",
       "         3.48420000e-02,   8.67690000e-02,  -1.06745000e-01,\n",
       "         2.37510000e-02,   1.18238000e-01,   4.80480000e-02,\n",
       "         5.87720000e-02,  -8.39270000e-02,  -9.64800000e-03,\n",
       "        -3.85590000e-02,  -1.38668000e-01,   2.74070000e-02,\n",
       "        -6.86650000e-02,   3.87500000e-03])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features('laptop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_maximum_title_and_body_length(lookup_table):\n",
    "    max_len_title = -1\n",
    "    max_len_question = -1\n",
    "    max_len_question_id = 0\n",
    "    for key, dict_val in lookup_table.iteritems():\n",
    "        len_title = len(dict_val['title'])\n",
    "        len_question = len(dict_val['question'])\n",
    "        if len_title > max_len_title:\n",
    "             max_len_title = len_title\n",
    "        if len_question > max_len_question:\n",
    "            max_len_question = len_question\n",
    "            max_len_question_id = key\n",
    "    return max_len_title, max_len_question\n",
    "\n",
    "def title_to_feature_matrix(title_word_list):\n",
    "    feature_matrix = []\n",
    "    for word in title_word_list:\n",
    "        word_features = extract_features(word)\n",
    "        feature_matrix.append(word_features)\n",
    "    #Pad the feature with zeros to ensure all inputs to the net have the same dimension\n",
    "    feature_matrix += [[0.] * NUM_FEATURES_PER_WORD] * (MAX_TITLE_LENGTH - len(title_word_list))\n",
    "    #print np.array(feature_matrix).T.shape\n",
    "    return np.array(feature_matrix).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_TITLE_LENGTH, MAX_BODY_LENGTH = find_maximum_title_and_body_length(lookup)\n",
    "NUM_FEATURES_PER_WORD = 200\n",
    "INPUT_DIM = (MAX_TITLE_LENGTH, NUM_FEATURES_PER_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system', 'running', 'in', 'low', 'graphic', 'mode', '(', 'ubuntu', 'without', 'monitor', ')']\n",
      "(200, 38)\n",
      "[[ 0.101999  0.00388  -0.026436 ...,  0.        0.        0.      ]\n",
      " [-0.104434 -0.07965   0.013091 ...,  0.        0.        0.      ]\n",
      " [-0.012801 -0.044619 -0.037213 ...,  0.        0.        0.      ]\n",
      " ..., \n",
      " [ 0.034353 -0.021587 -0.059916 ...,  0.        0.        0.      ]\n",
      " [-0.013605  0.023161  0.027431 ...,  0.        0.        0.      ]\n",
      " [-0.037034 -0.135637  0.020814 ...,  0.        0.        0.      ]]\n"
     ]
    }
   ],
   "source": [
    "print train_titles_only[0][0]\n",
    "print title_to_feature_matrix(train_titles_only[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(200, 200, KERNEL_SIZE)\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(200, 200, KERNEL_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(KERNEL_SIZE)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: target out of range at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/THNN/generic/MultiMarginCriterion.c:43",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-6c3775fb50f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/modules/loss.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         return F.multi_margin_loss(input, target, self.p, self.margin,\n\u001b[0;32m--> 595\u001b[0;31m                                    self.weight, self.size_average)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mmulti_margin_loss\u001b[0;34m(input, target, p, margin, weight, size_average)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight must be one-dimensional'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiMarginLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/_functions/thnn/auto.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, target, *args)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         getattr(ctx._backend, update_output.name)(ctx._backend.library_state, input, target,\n\u001b[0;32m---> 47\u001b[0;31m                                                   output, *ctx.additional_args)\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 3: target out of range at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/THNN/generic/MultiMarginCriterion.c:43"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "KERNEL_SIZE = 3\n",
    "INPUT_SIZE = 200\n",
    "HIDDEN_SIZE = 400\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 1\n",
    "net = CNN(INPUT_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "criterion = nn.MultiMarginLoss(p=1, margin=1, weight=None, size_average=True) #HAHA just put these in to look smart \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# ----TRAINING\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for sample in train_titles_only:\n",
    "        target_title = sample[0]\n",
    "        positive_title = sample[1]\n",
    "        negative_title = sample[2]\n",
    "        \n",
    "        target_features = title_to_feature_matrix(target_title)\n",
    "        positive_features = title_to_feature_matrix(positive_title)\n",
    "        negative_features = title_to_feature_matrix(negative_title)\n",
    "        \n",
    "        target_features = Variable(\n",
    "                        torch.FloatTensor(\n",
    "                            [target_features]\n",
    "                        )\n",
    "                        )\n",
    "        positive_features = Variable(\n",
    "                        torch.FloatTensor(\n",
    "                            [positive_features]\n",
    "                        )\n",
    "                        )\n",
    "        negative_features = Variable(\n",
    "                        torch.FloatTensor(\n",
    "                            [negative_features]\n",
    "                        )\n",
    "                        )\n",
    "\n",
    "        target_matrix = net(target_features)\n",
    "        positive_matrix = net(positive_features)\n",
    "        negative_matrix = net(negative_features)\n",
    "                \n",
    "        #target_vec = [item for sublist in target_matrix.data.numpy()[0] for item in sublist]\n",
    "        target_vec=target_matrix.data.numpy()[0].reshape(1,-1)\n",
    "        positive_vec=target_matrix.data.numpy()[0].reshape(1,-1)\n",
    "        negative_vec=target_matrix.data.numpy()[0].reshape(1,-1)\n",
    "        #positive_vec = [item for sublist in positive_matrix.data.numpy()[0] for item in sublist]\n",
    "        #negative_vec = [item for sublist in negative_matrix.data.numpy()[0] for item in sublist]\n",
    "        #print \"target_vec: \" + str(target_vec)\n",
    "        \n",
    "        cos_sim_positive = cosine_similarity(target_vec, positive_vec)\n",
    "        cos_sim_negative = cosine_similarity(target_vec, negative_vec)\n",
    "        \n",
    "        cos_sims = [cos_sim_positive, cos_sim_negative]\n",
    "        #print \"cos_sims: \" + str(cos_sims)\n",
    "        #max_idx = Variable(torch.FloatTensor([np.argmax(cos_sims)]), requires_grad=True) #use axis = 1 when we use more negative examples later\n",
    "        max_idx = Variable(torch.FloatTensor([np.argmaxcos_sims]), requires_grad=True)\n",
    "        #print \"max_idx: \" + str(max_idx)\n",
    "        y = Variable(torch.LongTensor([1]),requires_grad=True)\n",
    "        #print y\n",
    "        \n",
    "        #print type(max_idx)\n",
    "        #print type(y)\n",
    "        \"\"\"\n",
    "        because we know the 0th index in cos_sims is always the example we expect to most closely match \n",
    "        the target question\n",
    "        \"\"\"\n",
    "        \n",
    "        running_loss = 0.0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(max_idx, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print \"Loss after epoch \" + str(epoch) + \" :\" + str(loss.data[0])\n",
    "# ----END TRAINING\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c213b1b31885>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/functional.pyc\u001b[0m in \u001b[0;36mstack\u001b[0;34m(sequence, dim, out)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "X_scores = torch.stack(torch.FloatTensor([torch.FloatTensor(1),2,3]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
