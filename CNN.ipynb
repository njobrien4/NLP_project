{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_NEGATIVE_SAMPLES=20\n",
    "\n",
    "from random import randint\n",
    "def N_random_values_in_list(full_list, N):\n",
    "    x=0\n",
    "    lower_bound  = 0\n",
    "    upper_bound = len(full_list)-1\n",
    "    sample_list=[]\n",
    "    random_nums=[]\n",
    "    while x < min(N,len(full_list)):\n",
    "        random_num = randint(lower_bound, upper_bound) # inclusive range\n",
    "        if random_num in random_nums:\n",
    "            continue\n",
    "        else:\n",
    "            random_nums.append(random_num)\n",
    "            x += 1\n",
    "    return [full_list[i] for i in random_nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_list(filename):\n",
    "    if filename.endswith('gz'):\n",
    "        with gzip.open(filename,'r')as f:\n",
    "            text_tokens = f.readlines()\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            text_tokens = f.readlines()\n",
    "    text_tokens = [token.replace('\\n','').split('\\t') for token in text_tokens]\n",
    "    text_tokens = [[token[0], token[1].split(' '), token[2].split(' ')] for token in text_tokens]\n",
    "                   \n",
    "    return text_tokens\n",
    "\n",
    "#Sample:question_id, similar_question_id, negative_question_id\n",
    "def convert_to_samples(filename):\n",
    "    my_list=convert_to_list(filename)\n",
    "    new_samples=[]\n",
    "    for original_sample in my_list:\n",
    "        for similar in original_sample[1]:\n",
    "            random_negative_samples = N_random_values_in_list(original_sample[2],NUM_NEGATIVE_SAMPLES)\n",
    "            new_samples.append([original_sample[0], similar, random_negative_samples])# change this to include all negative \n",
    "                                                                                     # examples later\n",
    "    return new_samples\n",
    "def make_lookup_table_for_training_data(filename):\n",
    "    lookup={}\n",
    "    text_token_list=convert_to_list(filename)\n",
    "    for token in text_token_list:\n",
    "        lookup[token[0]] = {'title':token[1],'question':token[2]}\n",
    "    return lookup\n",
    "        \n",
    "#takes  sample_ids of [[q1,p1,n1],[q2,p2,n2]....]\n",
    "#outputs titles like [[q1_title, p1_title, n1_title],[q2_title,p2_title,n2_title]...]\n",
    "def convert_sampleids_to_titles(sample_ids,lookup):\n",
    "    #each sample_id [question_id, pos_id, [neg_ids]]\n",
    "    #print type(sample_ids)==list, \"first\"\n",
    "   \n",
    "    titles = []\n",
    "    for sample_id in sample_ids:\n",
    "        \n",
    "         #flatten list: [question_id, pos_id, [neg_ids]] --> [question_id, pos_id, neg_id1, neg_id2, ...]\n",
    "        sample_id= sample_id[:2]+sample_id[2][:]\n",
    "        #sample_id : question_id, similar_question_id, negative_question_id\n",
    "        try:\n",
    "            titles.append([lookup[str(identity)]['title'] for identity in sample_id])\n",
    "           # print type(sample_id)==list\n",
    "        except:\n",
    "            print sample_id, \"is sample id\", type(sample_id)==list\n",
    "    return titles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text_tokenized.txt.gz has id \\t title \\t question body\n",
    "text_tokenized='askubuntu/text_tokenized.txt.gz'\n",
    "\n",
    "#train_random.txt\n",
    "#(1) the query question ID, (2) the list of similar question IDs, and (3) the list of randomly selected question IDs.\n",
    "train_random_filename='askubuntu/train_random.txt'\n",
    "\n",
    "#Each line contains (1) the query question ID, (2) the list of similar question IDs, (3) the list of 20 candidate question IDs and (4) the associated BM25 scores of these questions computed by the Lucene search engine. The second field (the set of similar questions) is a subset of the third field.\n",
    "dev_filename='askubuntu/dev.txt'\n",
    "test_filename='askubuntu/test.txt'\n",
    "\n",
    "train_samples = convert_to_samples(train_random_filename)\n",
    "dev_samples = convert_to_samples(dev_filename)\n",
    "test_samples = convert_to_samples(test_filename)\n",
    "\n",
    "lookup = make_lookup_table_for_training_data(text_tokenized)\n",
    "train_list = convert_to_list(train_random_filename)\n",
    "train_titles_only = convert_sampleids_to_titles(train_samples, lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "word_embeddings = 'askubuntu/vector/vectors_pruned.200.txt.gz'\n",
    "f = gzip.open(word_embeddings, 'r')\n",
    "wv_text = [ ]\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    wv_text.append(line.strip())\n",
    "\n",
    "word_to_vec = {}\n",
    "\n",
    "for line in wv_text:\n",
    "    parts = line.split()\n",
    "    word = parts[0]\n",
    "    vector = np.array([float(v) for v in parts[1:]])\n",
    "    word_to_vec[word] = vector\n",
    "f.close()\n",
    "\n",
    "def extract_features(word):\n",
    "    try:\n",
    "        word=remove_non_ascii(word)\n",
    "        word=word.encode('utf-8')\n",
    "    except:\n",
    "        print(word)\n",
    "    return word_to_vec.get(word,[0.0 for i in range(200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_maximum_title_and_body_length(lookup_table):\n",
    "    max_len_title = -1\n",
    "    max_len_question = -1\n",
    "    max_len_question_id = 0\n",
    "    for key, dict_val in lookup_table.iteritems():\n",
    "        len_title = len(dict_val['title'])\n",
    "        len_question = len(dict_val['question'])\n",
    "        if len_title > max_len_title:\n",
    "             max_len_title = len_title\n",
    "        if len_question > max_len_question:\n",
    "            max_len_question = len_question\n",
    "            max_len_question_id = key\n",
    "    return max_len_title, max_len_question\n",
    "\n",
    "def title_to_feature_matrix(title_word_list):\n",
    "    feature_matrix = []\n",
    "    for word in title_word_list:\n",
    "        word_features = extract_features(word)\n",
    "        feature_matrix.append(word_features)\n",
    "    #Pad the feature with zeros to ensure all inputs to the net have the same dimension\n",
    "    feature_matrix += [[0.] * NUM_FEATURES_PER_WORD] * (MAX_TITLE_LENGTH - len(title_word_list))\n",
    "    #print np.array(feature_matrix).T.shape\n",
    "    return np.array(feature_matrix).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_TITLE_LENGTH, MAX_BODY_LENGTH = find_maximum_title_and_body_length(lookup)\n",
    "NUM_FEATURES_PER_WORD = 200\n",
    "INPUT_DIM = (MAX_TITLE_LENGTH, NUM_FEATURES_PER_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data_utils\n",
    "BATCH_SIZE=50\n",
    "\n",
    "#for each tuple of titles make a feature vector that is num_titles x 200 x 38\n",
    "# where num_titles = 1 (target) + 1 (positive) + n (negative) \n",
    "features = []\n",
    "for i in range(len(train_titles_only[:10000])):   # we should include all but this is just for simplicity \n",
    "    sample=train_titles_only[i]\n",
    "    if i%1000==0:\n",
    "        print i\n",
    "    target_title = sample[0]\n",
    "    positive_title = sample[1]\n",
    "    negative_titles = sample[2:]\n",
    "        \n",
    "    target_features = title_to_feature_matrix(target_title)\n",
    "    positive_features = title_to_feature_matrix(positive_title)\n",
    "    n_negative_features = [title_to_feature_matrix(negative_title) for negative_title in negative_titles]\n",
    "    \n",
    "    #all features should be size num_titles x feature size ....\n",
    "    # where num_titles = 1 (target) + 1 (positive) + n (negative) \n",
    "    all_features=[[[0]*MAX_TITLE_LENGTH] * NUM_FEATURES_PER_WORD]*(2+NUM_NEGATIVE_SAMPLES)\n",
    "\n",
    "    all_features[0]=target_features\n",
    "    all_features[1]=positive_features\n",
    "    all_features[2:len(n_negative_features)+2]=n_negative_features\n",
    "    features.append(all_features)\n",
    "print(\"created features\", len(all_features))\n",
    "\n",
    "print torch.FloatTensor(features).size(0)\n",
    "targets = torch.LongTensor(torch.FloatTensor(features).size(0), 1).zero_()\n",
    "train = data_utils.TensorDataset(torch.FloatTensor(features), targets)\n",
    "print(\"created features\")\n",
    "train_loader = data_utils.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(features[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(200, 200, KERNEL_SIZE)\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(200, hidden_dim, KERNEL_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(KERNEL_SIZE)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going in to training\n",
      "0\n",
      "Loss after epoch 0 :4.37520591915\n",
      "0\n",
      "Loss after epoch 1 :4.33634459972\n",
      "0\n",
      "Loss after epoch 2 :4.43634518981\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-dcda47c99ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#print target_matrix.data.numpy().shape, \"is target matrix shape\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mpositive_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mn_negative_matrices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnegative_features\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_negative_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#target_vec = target_matrix.data.numpy()[0].reshape(50,1,-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-c430761660e3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m         )\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/modules/container.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 154\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     81\u001b[0m     f = ConvNd(_single(stride), _single(padding), _single(dilation), False,\n\u001b[1;32m     82\u001b[0m                _single(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "KERNEL_SIZE = 3\n",
    "INPUT_SIZE = 200\n",
    "HIDDEN_SIZE = 600\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 25\n",
    "net = CNN(INPUT_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "criterion = nn.MultiMarginLoss(p=1, margin=0.2, weight=None, size_average=True) #HAHA just put these in to look smart \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "print \"going in to training\"\n",
    "# ----TRAINING\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "   # for sample in features:\n",
    "    for i,(sample,label) in enumerate(train_loader):\n",
    "        if i%25==0:\n",
    "            print i\n",
    "        target_features=Variable(sample[:,0])\n",
    "        positive_features=Variable(sample[:,1])\n",
    "        n_negative_features=[Variable(sample[:,i]) for i in range(2, len(sample[0]))]\n",
    "        #print target_features.data.numpy().shape, \"is target features shape\"\n",
    "        target_matrix = net(target_features)\n",
    "        #print target_matrix.data.numpy().shape, \"is target matrix shape\"\n",
    "        positive_matrix = net(positive_features)\n",
    "        n_negative_matrices = [net(negative_features) for negative_features in n_negative_features]\n",
    "        \n",
    "        #target_vec = target_matrix.data.numpy()[0].reshape(50,1,-1)\n",
    "        target_vec = target_matrix.data.numpy().flatten()\n",
    "        #print target_vec.shape, \"is target vec shape\"\n",
    "        positive_vec = positive_matrix.data.numpy().flatten()\n",
    "        n_negative_vecs = [negative_matrix.data.numpy().flatten() for negative_matrix in n_negative_matrices]\n",
    "        #print target_vec, \"is target_vec\"\n",
    "        #print target_vec.shape, \"is for target vec\"\n",
    "        #print positive_vec.shape, \"is for pos vec\"\n",
    "        cos_sim_positive = [cosine_similarity(target_vec[i], positive_vec[i]) for i in range(BATCH_SIZE)]\n",
    "        all_cos_sim_negative=[]\n",
    "        for i in range(BATCH_SIZE):\n",
    "            batch_of_cos_sims=[]\n",
    "            for j in range(len(n_negative_vecs)):\n",
    "                negative_vec=n_negative_vecs[j]\n",
    "                #print negative_vec[0], \"is negative_vec\", i\n",
    "                batch_of_cos_sims.append(cosine_similarity(target_vec[i], negative_vec[i]))\n",
    "            all_cos_sim_negative.append(batch_of_cos_sims)\n",
    "\n",
    "        all_cos_sim_negative=np.array(all_cos_sim_negative)[:,:,:,0]\n",
    "        cos_sim_positive = np.array([cos_sim_positive[i] for i in range(BATCH_SIZE)])\n",
    "        # cos sims should be BATCH_SIZE x num questions (i.e. 2 when only using 1 negative example or 21 when using 20)        \n",
    "        cos_sims=np.concatenate((cos_sim_positive, all_cos_sim_negative),axis=1)\n",
    "        # print cos_sims[0], \"is cos_sims for first in batch\"\n",
    "        max_idx = Variable(torch.FloatTensor(np.array(cos_sims)), requires_grad=True) #use axis = 1 when we use more negative examples later\n",
    "        max_idx=max_idx[:,:,0]\n",
    "       # y = Variable(torch.LongTensor([0]),requires_grad=True)\n",
    "        y = Variable(label)\n",
    "        y=y[:,0]\n",
    "        \"\"\"\n",
    "        because we know the 0th index in cos_sims is always the example we expect to most closely match \n",
    "        the target question\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = criterion(max_idx, y)\n",
    "        #print loss, max_idx, \"is loss and max idx\"\n",
    "        loss.backward()\n",
    "    \n",
    "        running_loss += loss.data[0]\n",
    "#         print running_loss\n",
    "        optimizer.step()\n",
    "\n",
    "    print \"Loss after epoch \" + str(epoch) + \" :\" + str(running_loss)\n",
    "# ----END TRAINING\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22853"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_titles_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(Variable(torch.FloatTensor([1,2,3,4,5,6,7,8,9,10,10.2])),Variable(torch.LongTensor([10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 ,.,.) = \n",
       "  0.1273  0.1425  0.1396  ...   0.0000  0.0000  0.0000\n",
       " -0.0432 -0.1104 -0.0985  ...   0.0000  0.0000  0.0000\n",
       "  0.0310  0.0730  0.0684  ...   0.0000  0.0000  0.0000\n",
       "           ...             ⋱             ...          \n",
       "  0.2174 -0.0451 -0.0313  ...   0.0000  0.0000  0.0000\n",
       " -0.0273  0.0040 -0.0797  ...   0.0000  0.0000  0.0000\n",
       " -0.0238  0.0159 -0.0015  ...   0.0000  0.0000  0.0000\n",
       "\n",
       "( 1 ,.,.) = \n",
       "  0.1273  0.1425  0.0631  ...   0.0000  0.0000  0.0000\n",
       " -0.0432 -0.1104 -0.0884  ...   0.0000  0.0000  0.0000\n",
       "  0.0310  0.0730 -0.0981  ...   0.0000  0.0000  0.0000\n",
       "           ...             ⋱             ...          \n",
       "  0.2174 -0.0451  0.0737  ...   0.0000  0.0000  0.0000\n",
       " -0.0273  0.0040 -0.0971  ...   0.0000  0.0000  0.0000\n",
       " -0.0238  0.0159 -0.0388  ...   0.0000  0.0000  0.0000\n",
       "\n",
       "( 2 ,.,.) = \n",
       "  0.0827 -0.0385  0.0000  ...   0.0000  0.0000  0.0000\n",
       " -0.0481 -0.0178  0.0000  ...   0.0000  0.0000  0.0000\n",
       "  0.0284 -0.1324  0.0000  ...   0.0000  0.0000  0.0000\n",
       "           ...             ⋱             ...          \n",
       "  0.1478 -0.0632  0.0000  ...   0.0000  0.0000  0.0000\n",
       " -0.0055  0.0141  0.0000  ...   0.0000  0.0000  0.0000\n",
       " -0.0846 -0.0471  0.0000  ...   0.0000  0.0000  0.0000\n",
       "[torch.FloatTensor of size 3x200x38]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(sample[0][0].numpy().flatten(), sample[0][1].numpy().flatten())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 38)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0][0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
