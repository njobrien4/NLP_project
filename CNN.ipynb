{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------IMPORTS-------------------------------------#\n",
    "import torch\n",
    "import gzip\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------HELPER FUNCTIONS-------------------------------------#\n",
    "NUM_NEGATIVE_SAMPLES=20\n",
    "\n",
    "def N_random_values_in_list(full_list, N):\n",
    "    x=0\n",
    "    lower_bound  = 0\n",
    "    upper_bound = len(full_list)-1\n",
    "    sample_list=[]\n",
    "    random_nums=[]\n",
    "    while x < min(N,len(full_list)):\n",
    "        random_num = randint(lower_bound, upper_bound) # inclusive range\n",
    "        if random_num in random_nums:\n",
    "            continue\n",
    "        else:\n",
    "            random_nums.append(random_num)\n",
    "            x += 1\n",
    "    return [full_list[i] for i in random_nums]\n",
    "\n",
    "def convert_to_list(filename):\n",
    "    if filename.endswith('gz'):\n",
    "        with gzip.open(filename,'r')as f:\n",
    "            text_tokens = f.readlines()\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            text_tokens = f.readlines()\n",
    "    text_tokens = [token.replace('\\n','').split('\\t') for token in text_tokens]\n",
    "    text_tokens = [[token[0], token[1].split(' '), token[2].split(' ')] for token in text_tokens]\n",
    "                   \n",
    "    return text_tokens\n",
    "\n",
    "#Sample:question_id, similar_question_id, negative_question_id\n",
    "def convert_to_samples(filename):\n",
    "    my_list=convert_to_list(filename)\n",
    "    new_samples=[]\n",
    "    for original_sample in my_list:\n",
    "        for similar in original_sample[1]:\n",
    "            random_negative_samples = N_random_values_in_list(original_sample[2],NUM_NEGATIVE_SAMPLES)\n",
    "            new_samples.append([original_sample[0], similar, random_negative_samples])# change this to include all negative \n",
    "                                                                                     # examples later\n",
    "    return new_samples\n",
    "def make_lookup_table_for_training_data(filename):\n",
    "    lookup={}\n",
    "    text_token_list=convert_to_list(filename)\n",
    "    for token in text_token_list:\n",
    "        lookup[token[0]] = {'title':token[1],'question':token[2]}\n",
    "    return lookup\n",
    "        \n",
    "#takes  sample_ids of [[q1,p1,n1],[q2,p2,n2]....]\n",
    "#outputs titles like [[q1_title, p1_title, n1_title],[q2_title,p2_title,n2_title]...]\n",
    "def convert_sampleids_to_titles(sample_ids,lookup):\n",
    "    #each sample_id [question_id, pos_id, [neg_ids]]\n",
    "    #print type(sample_ids)==list, \"first\"\n",
    "   \n",
    "    titles = []\n",
    "    for sample_id in sample_ids:\n",
    "        \n",
    "         #flatten list: [question_id, pos_id, [neg_ids]] --> [question_id, pos_id, neg_id1, neg_id2, ...]\n",
    "        sample_id= sample_id[:2]+sample_id[2][:]\n",
    "        #sample_id : question_id, similar_question_id, negative_question_id\n",
    "        try:\n",
    "            titles.append([lookup[str(identity)]['title'] for identity in sample_id])\n",
    "           # print type(sample_id)==list\n",
    "        except:\n",
    "            print sample_id, \"is sample id\", type(sample_id)==list\n",
    "    return titles\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "def extract_features(word):\n",
    "    try:\n",
    "        word=remove_non_ascii(word)\n",
    "        word=word.encode('utf-8')\n",
    "    except:\n",
    "        print(word)\n",
    "    return word_to_vec.get(word,[0.0 for i in range(200)])\n",
    "\n",
    "def find_maximum_title_and_body_length(lookup_table):\n",
    "    max_len_title = -1\n",
    "    max_len_question = -1\n",
    "    max_len_question_id = 0\n",
    "    for key, dict_val in lookup_table.iteritems():\n",
    "        len_title = len(dict_val['title'])\n",
    "        len_question = len(dict_val['question'])\n",
    "        if len_title > max_len_title:\n",
    "             max_len_title = len_title\n",
    "        if len_question > max_len_question:\n",
    "            max_len_question = len_question\n",
    "            max_len_question_id = key\n",
    "    return max_len_title, max_len_question\n",
    "\n",
    "def title_to_feature_matrix(title_word_list):\n",
    "    feature_matrix = []\n",
    "    for word in title_word_list:\n",
    "        word_features = extract_features(word)\n",
    "        feature_matrix.append(word_features)\n",
    "    #Pad the feature with zeros to ensure all inputs to the net have the same dimension\n",
    "    feature_matrix += [[0.] * NUM_FEATURES_PER_WORD] * (MAX_TITLE_LENGTH - len(title_word_list))\n",
    "    #print np.array(feature_matrix).T.shape\n",
    "    return np.array(feature_matrix).T\n",
    "\n",
    "\n",
    "\n",
    "#array is structured like a batch of features 50x200x38\n",
    "def find_start_of_padding_for_batch(batch):\n",
    "    vec_lengths_in_batch = []\n",
    "    for batch_num in range(0, len(batch)):\n",
    "        single_vec = batch[batch_num]\n",
    "        length = find_start_of_padding_single_vec(single_vec) + 1\n",
    "        vec_lengths_in_batch.append(length)\n",
    "    return vec_lengths_in_batch\n",
    "\n",
    "#batch = 200x38\n",
    "def find_start_of_padding_single_vec(single_vec):\n",
    "    for idx in range(len(single_vec[0])-1, -1, -1):\n",
    "        if single_vec[0][idx] != 0.:\n",
    "            return idx\n",
    "    #if the whole sequence is 0s\n",
    "    return 0\n",
    "# def cosine_similarity(question_one_batch_by_features, question_two_batch_by_features):\n",
    "#     cosine_similarities = []\n",
    "#     for idx in range(BATCH_SIZE):\n",
    "#         vec_one = question_one_batch_by_features[idx]\n",
    "#         vec_two = question_two_batch_by_features[idx]\n",
    "#         numerator = np.dot(vec_one, vec_two)\n",
    "#         denominator = LA.norm(vec_one)*LA.norm(vec_two)\n",
    "#         value = numerator/denominator\n",
    "#         cosine_similarities.append(value)\n",
    "#     return cosine_similarities\n",
    "\n",
    "# def cosine_similarity_worse(question_one_batch_by_features, question_two_batch_by_features):\n",
    "#     print cosine_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------LOAD DATA-------------------------------------#\n",
    "\n",
    "word_embeddings = 'askubuntu/vector/vectors_pruned.200.txt.gz'\n",
    "f = gzip.open(word_embeddings, 'r')\n",
    "wv_text = [ ]\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    wv_text.append(line.strip())\n",
    "\n",
    "word_to_vec = {}\n",
    "\n",
    "for line in wv_text:\n",
    "    parts = line.split()\n",
    "    word = parts[0]\n",
    "    vector = np.array([float(v) for v in parts[1:]])\n",
    "    word_to_vec[word] = vector\n",
    "f.close()\n",
    "\n",
    "#text_tokenized.txt.gz has id \\t title \\t question body\n",
    "text_tokenized='askubuntu/text_tokenized.txt.gz'\n",
    "\n",
    "#train_random.txt\n",
    "#(1) the query question ID, (2) the list of similar question IDs, and (3) the list of randomly selected question IDs.\n",
    "train_random_filename='askubuntu/train_random.txt'\n",
    "\n",
    "#Each line contains (1) the query question ID, (2) the list of similar question IDs, (3) the list of 20 candidate question IDs and (4) the associated BM25 scores of these questions computed by the Lucene search engine. The second field (the set of similar questions) is a subset of the third field.\n",
    "dev_filename='askubuntu/dev.txt'\n",
    "test_filename='askubuntu/test.txt'\n",
    "\n",
    "train_samples = convert_to_samples(train_random_filename)\n",
    "dev_samples = convert_to_samples(dev_filename)\n",
    "test_samples = convert_to_samples(test_filename)\n",
    "\n",
    "lookup = make_lookup_table_for_training_data(text_tokenized)\n",
    "train_list = convert_to_list(train_random_filename)\n",
    "train_titles_only = convert_sampleids_to_titles(train_samples, lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "Done Loop\n",
      "Succesfully made the data batcher\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#-------------------------------------CREATE DATA BATCHER-------------------------------------#\n",
    "#for each tuple of titles make a feature vector that is num_titles x 200 x 38\n",
    "# where num_titles = 1 (target) + 1 (positive) + n (negative) \n",
    "features = []\n",
    "for i in range(len(train_titles_only[:NUM_TRAINING_EXAMPLES])):   # we should include all but this is just for simplicity \n",
    "    if i%1000 == 0:\n",
    "        print i\n",
    "\n",
    "    sample = train_titles_only[i]\n",
    "    \n",
    "    target_title = sample[0]\n",
    "    positive_title = sample[1]\n",
    "    negative_titles = sample[2:]\n",
    "    \n",
    "#     print \"target_title: {}\".format(target_title)\n",
    "#     print \"positive title: {}\".format(positive_title)\n",
    "#     print \"negative titles: {}\".format(negative_titles)\n",
    "    \n",
    "    target_features = title_to_feature_matrix(target_title)\n",
    "    positive_features = title_to_feature_matrix(positive_title)\n",
    "    n_negative_features = [title_to_feature_matrix(negative_title) for negative_title in negative_titles]\n",
    "    \n",
    "#     print \"Target features shape: {}\".format(target_features.shape)\n",
    "#     print \"Positive features shape: {}\".format(positive_features.shape)\n",
    "#     print \"Negative features[0] shape: {}\".format(n_negative_features[0].shape)\n",
    "#     print \"Num negative features: {}\".format(len(n_negative_features)) \n",
    "    \n",
    "    all_features = [target_features, positive_features] + n_negative_features\n",
    "    features.append(all_features)\n",
    "\n",
    "start = time.time()\n",
    "print \"Done Loop\"\n",
    "features = np.array(features) #speed up the following operations\n",
    "targets = torch.LongTensor(len(features), 1).zero_()\n",
    "training_dataset = data_utils.TensorDataset(torch.FloatTensor(features), targets)\n",
    "train_loader = data_utils.DataLoader(training_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "print \"Succesfully made the data batcher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------GLOBAL VARIABLES-------------------------------------#\n",
    "NUM_TRAINING_EXAMPLES = 5000 #FOR DATA BATCHER, WHEN DEPLOYED SHOULD BE ALL TRAINING EXAMPLES\n",
    "\n",
    "KERNEL_SIZE = 3 #MAKE SURE THIS NUMBER IS ODD SO THAT THE PADDING MAKES SENSE\n",
    "PADDING = (KERNEL_SIZE - 1) / 2\n",
    "INPUT_SIZE = 200\n",
    "HIDDEN_SIZE = 600\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 25\n",
    "NUM_FEATURES_PER_WORD = 200 #DO NOT CHANGE. FIXED at 200\n",
    "BATCH_SIZE = 50\n",
    "MAX_TITLE_LENGTH, MAX_BODY_LENGTH = find_maximum_title_and_body_length(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------CNN-------------------------------------#\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(200, hidden_dim, KERNEL_SIZE, padding = PADDING),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "Loss after epoch 0 :0.800522348844\n",
      "Loss after epoch 1 :0.800522353966\n",
      "Loss after epoch 2 :0.800522349076\n",
      "Loss after epoch 3 :0.800522387261\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-a612a30aef90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mcosine_similarity_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_question_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_question_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mcosine_similarity_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_question_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnegative\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mN_negative_question_vectors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mcosine_sims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcosine_similarity_pos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcosine_similarity_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alexander/anaconda2/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_normalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alexander/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0mnorms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m             \u001b[0mnorms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m             \u001b[0mnorms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alexander/anaconda2/lib/python2.7/site-packages/sklearn/utils/extmath.pyc\u001b[0m in \u001b[0;36mrow_norms\u001b[0;34m(X, squared)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mnorms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_row_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mnorms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ij,ij->i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alexander/anaconda2/lib/python2.7/site-packages/numpy/core/einsumfunc.pyc\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;31m# If no optimization, run pure einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptimize_arg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mc_einsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0mvalid_einsum_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'order'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'casting'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "INPUT_DIM = (MAX_TITLE_LENGTH, NUM_FEATURES_PER_WORD)\n",
    "\n",
    "net = CNN(INPUT_SIZE, HIDDEN_SIZE)\n",
    "criterion = nn.MultiMarginLoss(p=1, margin=0.0, weight=None, size_average=True) #HAHA just put these in to look smart \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "print \"Beginning training...\"\n",
    "# ----TRAINING\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for idx,(sample, label) in enumerate(train_loader):\n",
    "        #Sample shape: [50, 22, 200, 38]\n",
    "        #50 - Batch size, 22 - Num questions per data point\n",
    "        batch_cos_similarities = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        target_question_features = sample[:, 0]\n",
    "        positive_question_features = sample[:, 1]\n",
    "        N_negative_question_features = sample[:, 2:]\n",
    "        \n",
    "        target_question_net_output = net(Variable(target_question_features))\n",
    "        positive_question_net_output = net(Variable(positive_question_features))\n",
    "        N_negative_question_net_outputs = [net(Variable(negative)) for negative in N_negative_question_features]\n",
    "        \n",
    "        #Determine lengths to know how many vectors to take the average across.\n",
    "        target_question_lengths = find_start_of_padding_for_batch(target_question_features)\n",
    "        positive_question_lengths = find_start_of_padding_for_batch(positive_question_features)\n",
    "        N_negative_questions_lengths = [find_start_of_padding_for_batch(negative) for negative in N_negative_question_features]\n",
    "                \n",
    "        batch_cos_sims = []\n",
    "        #50x600x38 --> 50x600x1. Averaging across the net outputs to get a single vector for each question.\n",
    "        for sample_num in range(BATCH_SIZE):\n",
    "            target_question_length = target_question_lengths[sample_num]\n",
    "            positive_question_length = positive_question_lengths[sample_num]\n",
    "            N_negative_question_lengths = N_negative_questions_lengths[sample_num]\n",
    "            N_negative_question_outputs = N_negative_question_net_outputs[sample_num]\n",
    "            \n",
    "            target_question_vector = target_question_net_output.data.numpy()[sample_num,:,:target_question_length].mean(axis = 1)\n",
    "            positive_question_vector = positive_question_net_output.data.numpy()[sample_num,:,:positive_question_length].mean(axis = 1)\n",
    "            \n",
    "            N_negative_question_vectors = [negative.data.numpy()[:,:N_negative_question_lengths[idx]].mean(axis = 1) for idx, negative in enumerate(N_negative_question_outputs)]\n",
    "        \n",
    "\n",
    "            cosine_similarity_pos = np.squeeze(cosine_similarity(target_question_vector.reshape(1, -1), positive_question_vector.reshape(1, -1)))\n",
    "            cosine_similarity_neg = [np.squeeze(cosine_similarity(target_question_vector.reshape(1, -1), negative.reshape(1, -1))) for negative in N_negative_question_vectors]\n",
    "            \n",
    "            cosine_sims = np.array([cosine_similarity_pos] + cosine_similarity_neg).flatten()\n",
    "            batch_cos_sims.append(cosine_sims)\n",
    "        \n",
    "        batch_labels = Variable(torch.from_numpy(label.numpy().squeeze()), requires_grad = True)\n",
    "        loss = criterion(Variable(torch.FloatTensor(np.array(batch_cos_sims))), batch_labels)\n",
    "            \n",
    "        loss.backward()\n",
    "            \n",
    "        running_loss += loss.data[0]\n",
    "            \n",
    "        optimizer.step()\n",
    "\n",
    "    print \"Loss after epoch \" + str(epoch) + \" :\" + str(running_loss)\n",
    "    \n",
    "        \n",
    "#         for batch_iter_idx in range(0, BATCH_SIZE):\n",
    "#             current_sample_in_batch = sample[batch_iter_idx]\n",
    "#             current_label = label[batch_iter_idx]\n",
    "\n",
    "#             target_word_features = current_sample_in_batch[0]\n",
    "#             positive_word_features = current_sample_in_batch[1]\n",
    "#             N_negative_word_features = current_sample_in_batch[2:]\n",
    "            \n",
    "#             #!!!!!!\n",
    "#             #FIX SHAPE OF FEATURES BECAUSE TORCH IS FUCKING WEIRD AND ONLY WANTS 3D INPUTS TO THE 1D CONV\n",
    "#             #!!!!!!\n",
    "#             target_word_features = torch.unsqueeze(target_word_features, 0)\n",
    "#             positive_word_features = torch.unsqueeze(positive_word_features, 0)\n",
    "#             N_negative_word_features = [torch.unsqueeze(negative_word_feature, 0) for negative_word_feature in N_negative_word_features]\n",
    "#             #FINISHED FIXING SHAPE. FUCK TORCH\n",
    "            \n",
    "#             target_net_output = net(Variable(target_word_features, requires_grad = True))\n",
    "#             positive_net_output = net(Variable(positive_word_features, requires_grad = True))\n",
    "#             N_negative_word_features_net_output = [net(Variable(negative_word_feature, requires_grad = True)) for negative_word_feature in N_negative_word_features]\n",
    "            \n",
    "\n",
    "#             length_target_question = find_start_of_padding_single_vec(target_word_features.numpy()[0])\n",
    "#             target_net_output_array = target_net_output.data.numpy().reshape((1, 21600)) / length_target_question\n",
    "            \n",
    "#             length_positive_question = find_start_of_padding_single_vec(positive_word_features.numpy()[0])\n",
    "#             positive_net_output_array = positive_net_output.data.numpy().reshape((1, 21600)) / length_positive_question\n",
    "            \n",
    "#             length_negative_questions = [find_start_of_padding_single_vec(negative.numpy()[0]) for negative in N_negative_word_features]\n",
    "#             N_negative_net_output_array = [negative.data.numpy().reshape((1, 21600)) / length_negative_questions[idx] for idx, negative in enumerate(N_negative_word_features_net_output)]\n",
    "            \n",
    "            \n",
    "#             positive_cos_similarity = cosine_similarity(target_net_output_array, positive_net_output_array)\n",
    "#             negative_cos_similarities = [cosine_similarity(target_net_output_array, negative) for negative in N_negative_net_output_array]\n",
    "            \n",
    "#             #WEIRD FORMATTING THINGS\n",
    "#             positive_cos_similarity = positive_cos_similarity[0][0]\n",
    "#             negative_cos_similarities = [negative[0][0] for negative in negative_cos_similarities]\n",
    "            \n",
    "#             cos_similarities = np.array([positive_cos_similarity] + negative_cos_similarities)\n",
    "            \n",
    "#             batch_cos_similarities.append(cos_similarities)\n",
    "#             batch_labels.append(current_label.numpy())\n",
    "\n",
    "        \n",
    "#         #MORE WEIRD SHIT WITH WHAT TORCH NEEDS            \n",
    "#         batch_cos_similarities = np.array(batch_cos_similarities)\n",
    "#         batch_labels = np.array(batch_labels).reshape((BATCH_SIZE))\n",
    "#         batch_cos_similarities = Variable(torch.from_numpy(batch_cos_similarities), requires_grad = True) \n",
    "#         batch_labels = Variable(torch.from_numpy(batch_labels), requires_grad = True)\n",
    "\n",
    "#         loss = criterion(batch_cos_similarities, batch_labels)\n",
    "            \n",
    "#         loss.backward()\n",
    "            \n",
    "#         running_loss += loss.data[0]\n",
    "            \n",
    "#         optimizer.step()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 600, 38)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_question_net_output.data.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
