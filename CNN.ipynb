{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------IMPORTS-------------------------------------#\n",
    "import torch\n",
    "import gzip\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------GLOBAL VARIABLES-------------------------------------#\n",
    "NUM_TRAINING_EXAMPLES = 22850 #FOR DATA BATCHER, WHEN DEPLOYED SHOULD BE ALL TRAINING EXAMPLES\n",
    "PARAMETER_MAX_TITLE_LENGTH = 38\n",
    "NUM_FEATURES_PER_WORD = 200 #DO NOT CHANGE. FIXED at 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------HELPER FUNCTIONS-------------------------------------#\n",
    "NUM_NEGATIVE_SAMPLES=20\n",
    "\n",
    "def N_random_values_in_list(full_list, N):\n",
    "    x=0\n",
    "    lower_bound  = 0\n",
    "    upper_bound = len(full_list)-1\n",
    "    sample_list=[]\n",
    "    random_nums=[]\n",
    "    while x < min(N,len(full_list)):\n",
    "        random_num = randint(lower_bound, upper_bound) # inclusive range\n",
    "        if random_num in random_nums:\n",
    "            continue\n",
    "        else:\n",
    "            random_nums.append(random_num)\n",
    "            x += 1\n",
    "    return [full_list[i] for i in random_nums]\n",
    "\n",
    "def convert_to_list(filename):\n",
    "    if filename.endswith('gz'):\n",
    "        with gzip.open(filename,'r')as f:\n",
    "            text_tokens = f.readlines()\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            text_tokens = f.readlines()\n",
    "    text_tokens = [token.replace('\\n','').split('\\t') for token in text_tokens]\n",
    "    text_tokens = [[token[0], token[1].split(' '), token[2].split(' ')] for token in text_tokens]\n",
    "    #print text_tokens[0]\n",
    "    return text_tokens\n",
    "\n",
    "#Sample:question_id, similar_question_id, negative_question_id\n",
    "def convert_to_samples(filename, is_dev_or_test = False):\n",
    "    my_list=convert_to_list(filename)\n",
    "    new_samples=[]\n",
    "    for original_sample in my_list:\n",
    "        if is_dev_or_test:\n",
    "            new_samples.append([original_sample[0], similar, negative_samples])\n",
    "        else:\n",
    "            for similar in original_sample[1]:\n",
    "                random_negative_samples = N_random_values_in_list(original_sample[2],NUM_NEGATIVE_SAMPLES)\n",
    "                new_samples.append([original_sample[0], similar, random_negative_samples])# change this to include all negative \n",
    "                                                                                     # examples later\n",
    "    return new_samples\n",
    "def make_lookup_table_for_training_data(filename):\n",
    "    lookup={}\n",
    "    text_token_list=convert_to_list(filename)\n",
    "    for token in text_token_list:\n",
    "        lookup[token[0]] = {'title':token[1],'question':token[2]}\n",
    "    return lookup\n",
    "        \n",
    "#takes  sample_ids of [[q1,p1,n1],[q2,p2,n2]....]\n",
    "#outputs titles like [[q1_title, p1_title, n1_title],[q2_title,p2_title,n2_title]...]\n",
    "def convert_sampleids_to_titles(sample_ids,lookup, is_dev_or_test = False):\n",
    "    #each sample_id [question_id, pos_id, [neg_ids]]\n",
    "    #print type(sample_ids)==list, \"first\"\n",
    "   \n",
    "    titles = []\n",
    "   # print \"sample_ids\", sample_ids\n",
    "    for sample_id in sample_ids:\n",
    "        if is_dev_or_test:\n",
    "            #omit similar questions, only needed for evaluation\n",
    "            sample_id=[sample_id[0]]+sample_id[2][:]\n",
    "         #flatten list: [question_id, pos_id, [neg_ids]] --> [question_id, pos_id, neg_id1, neg_id2, ...]\n",
    "        else:\n",
    "            sample_id= sample_id[:2]+sample_id[2][:]\n",
    "        #sample_id : question_id, similar_question_id, negative_question_id\n",
    "        #try:\n",
    "        current=[]\n",
    "        for identity in sample_id:\n",
    "            try:\n",
    "                current.append(lookup[str(identity)]['title'])\n",
    "            except:\n",
    "                print identity\n",
    "        titles.append(current)\n",
    "           # print type(sample_id)==list\n",
    "        #except:\n",
    "        #    print sample_id, \"is sample id\", type(sample_id)==list\n",
    "    return titles\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "def extract_features(word):\n",
    "    try:\n",
    "        word=remove_non_ascii(word)\n",
    "        word=word.encode('utf-8')\n",
    "    except:\n",
    "        print(word)\n",
    "    return word_to_vec.get(word,[0.0 for i in range(200)])\n",
    "\n",
    "def find_maximum_title_and_body_length(lookup_table):\n",
    "    max_len_title = -1\n",
    "    max_len_question = -1\n",
    "    max_len_question_id = 0\n",
    "    for key, dict_val in lookup_table.iteritems():\n",
    "        len_title = len(dict_val['title'])\n",
    "        len_question = len(dict_val['question'])\n",
    "        if len_title > max_len_title:\n",
    "             max_len_title = len_title\n",
    "        if len_question > max_len_question:\n",
    "            max_len_question = len_question\n",
    "            max_len_question_id = key\n",
    "    return max_len_title, max_len_question\n",
    "\n",
    "def title_to_feature_matrix(title_word_list):\n",
    "    feature_matrix = []\n",
    "    for idx, word in enumerate(title_word_list):\n",
    "        if idx == PARAMETER_MAX_TITLE_LENGTH:\n",
    "            break\n",
    "        else:\n",
    "            word_features = extract_features(word)\n",
    "            feature_matrix.append(word_features)\n",
    "        \n",
    "    #Pad the feature with zeros to ensure all inputs to the net have the same dimension\n",
    "    feature_matrix += [[0] * NUM_FEATURES_PER_WORD] * (PARAMETER_MAX_TITLE_LENGTH - len(title_word_list))\n",
    "    #print np.array(feature_matrix).T.shape\n",
    "    return np.array(feature_matrix).T\n",
    "\n",
    "\n",
    "\n",
    "#array is structured like a batch of features 50x200x38\n",
    "def find_start_of_padding_for_batch(batch):\n",
    "    vec_lengths_in_batch = []\n",
    "    for batch_num in range(0, len(batch)):\n",
    "        single_vec = batch[batch_num]\n",
    "        length = find_start_of_padding_single_vec(single_vec) + 1\n",
    "        vec_lengths_in_batch.append(length)\n",
    "    return vec_lengths_in_batch\n",
    "\n",
    "#batch = 200x38\n",
    "def find_start_of_padding_single_vec(single_vec):\n",
    "    for idx in range(len(single_vec[0])-1, -1, -1):\n",
    "        if single_vec[0][idx] != 0.:\n",
    "            return idx\n",
    "    #if the whole sequence is 0s\n",
    "    return 0\n",
    "def create_mask(word_length):\n",
    "    return np.array([[1. / word_length] * HIDDEN_SIZE] * word_length + [[0] * HIDDEN_SIZE] * (MAX_TITLE_LENGTH - word_length)).T\n",
    "#-------------------------------------CREATE DATA BATCHER-------------------------------------#\n",
    "# where samples[0] = 1 (target) + 1 (positive) + n (negative)\n",
    "# \n",
    "def create_data_loader(samples, shuffle_data = True, is_test_or_dev = False):\n",
    "    features = []\n",
    "    for sample in samples:\n",
    "        target_title = sample[0]\n",
    "        positive_title = sample[1]\n",
    "        negative_titles = sample[2:]\n",
    "\n",
    "        target_features = title_to_feature_matrix(target_title)\n",
    "        positive_features = title_to_feature_matrix(positive_title)\n",
    "        if len(negative_titles)==0:\n",
    "            all_features=[target_features, positive_features]\n",
    "        else:\n",
    "            n_negative_features = [title_to_feature_matrix(negative_title) for negative_title in negative_titles]\n",
    "\n",
    "            all_features = [target_features, positive_features] + n_negative_features\n",
    "        #print all_features, \"is all\"\n",
    "       # if is_test_or_dev:\n",
    "       #     all_features = [target_features,]\n",
    "        features.append(all_features)\n",
    "\n",
    "            \n",
    "    targets = torch.LongTensor(len(features)).zero_()\n",
    "    dataset = data_utils.TensorDataset(torch.FloatTensor(features), targets)\n",
    "    data_loader = data_utils.DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = shuffle_data)\n",
    "    return data_loader\n",
    "\n",
    "def create_dataset(samples, shuffle_data = True, is_test_or_dev = False):\n",
    "    features = []\n",
    "    for sample in samples:\n",
    "        target_title = sample[0]\n",
    "        positive_title = sample[1]\n",
    "        negative_titles = sample[2:]\n",
    "\n",
    "        target_features = title_to_feature_matrix(target_title)\n",
    "        positive_features = title_to_feature_matrix(positive_title)\n",
    "        if len(negative_titles)==0:\n",
    "            all_features=[target_features, positive_features]\n",
    "        else:\n",
    "            n_negative_features = [title_to_feature_matrix(negative_title) for negative_title in negative_titles]\n",
    "\n",
    "            all_features = [target_features, positive_features] + n_negative_features\n",
    "        #print all_features, \"is all\"\n",
    "       # if is_test_or_dev:\n",
    "       #     all_features = [target_features,]\n",
    "        features.append(all_features)\n",
    "\n",
    "            \n",
    "    targets = torch.LongTensor(len(features)).zero_()\n",
    "    dataset = data_utils.TensorDataset(torch.FloatTensor(features), targets)\n",
    "    #data_loader = data_utils.DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = shuffle_data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------NET GLOBAL VARIABLES-------------------------------------#\n",
    "#text_tokenized.txt.gz has id \\t title \\t question body\n",
    "text_tokenized='askubuntu/text_tokenized.txt.gz'\n",
    "lookup = make_lookup_table_for_training_data(text_tokenized)\n",
    "MAX_TITLE_LENGTH, MAX_BODY_LENGTH = find_maximum_title_and_body_length(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KERNEL_SIZE = 3 #MAKE SURE THIS NUMBER IS ODD SO THAT THE PADDING MAKES SENSE\n",
    "PADDING = (KERNEL_SIZE - 1) / 2\n",
    "INPUT_SIZE = 200\n",
    "HIDDEN_SIZE = 600\n",
    "LEARNING_RATE = 1e-5\n",
    "MARGIN = 0.2\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 50\n",
    "NUM_BATCHES = NUM_TRAINING_EXAMPLES/BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------LOAD DATA-------------------------------------#\n",
    "\n",
    "word_embeddings = 'askubuntu/vector/vectors_pruned.200.txt.gz'\n",
    "f = gzip.open(word_embeddings, 'r')\n",
    "wv_text = [ ]\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    wv_text.append(line.strip())\n",
    "\n",
    "word_to_vec = {}\n",
    "\n",
    "for line in wv_text:\n",
    "    parts = line.split()\n",
    "    word = parts[0]\n",
    "    vector = np.array([float(v) for v in parts[1:]])\n",
    "    word_to_vec[word] = vector\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " training\n",
      "made training data loader 1000\n",
      "made training data loader 1050\n",
      "made training data loader 1100\n",
      "made training data loader 1150\n",
      "made training data loader 1200\n",
      "made training data loader 1250\n",
      "made training data loader 1300\n",
      "made training data loader 1350\n",
      "made training data loader 1400\n",
      "made training data loader 1450\n",
      "made training data loader 1500\n",
      "made training data loader 1550\n",
      "made training data loader 1600\n",
      "made training data loader 1650\n",
      "made training data loader 1700\n",
      "made training data loader 1750\n",
      "made training data loader 1800\n",
      "made training data loader 1850\n",
      "made training data loader 1900\n",
      "made training data loader 1950\n"
     ]
    }
   ],
   "source": [
    "#train_random.txt\n",
    "#(1) the query question ID, (2) the list of similar question IDs, and (3) the list of randomly selected question IDs.\n",
    "\n",
    "NUM_TRAINING_EXAMPLES=2000\n",
    "\n",
    "train_random_filename='askubuntu/train_random.txt'\n",
    "\n",
    "#Each line contains (1) the query question ID, (2) the list of similar question IDs, (3) the list of 20 candidate question IDs and (4) the associated BM25 scores of these questions computed by the Lucene search engine. The second field (the set of similar questions) is a subset of the third field.\n",
    "dev_filename='askubuntu/dev.txt'\n",
    "test_filename='askubuntu/test.txt'\n",
    "\n",
    "train_samples = convert_to_samples(train_random_filename)\n",
    "#dev_samples = convert_to_samples(dev_filename, is_dev_or_test = True)\n",
    "#test_samples = convert_to_samples(test_filename, is_dev_or_test = True)\n",
    "print \"training\"\n",
    "train_titles_only = convert_sampleids_to_titles(train_samples, lookup)\n",
    "\n",
    "train_data_loaders=[]\n",
    "for i in range(1000,NUM_TRAINING_EXAMPLES,50):\n",
    "    train_data_loader = create_dataset(train_titles_only[i:i+50])\n",
    "    print \"made training data loader\", i\n",
    "    train_data_loaders.append(train_data_loader)\n",
    "final_train_dataset=data_utils.ConcatDataset(train_data_loaders)\n",
    "    #print \"dev\"\n",
    "#dev_titles_only = convert_sampleids_to_titles(dev_samples, lookup)\n",
    "#dev_data_loader = create_data_loader(dev_titles_only)\n",
    "#print \"test\"\n",
    "#test_titles_only = convert_sampleids_to_titles(test_samples, lookup)\n",
    "#test_data_loader = create_data_loader(test_titles_only, shuffle_data = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_data_loader=data_utils.DataLoader(final_train_dataset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------CNN-------------------------------------#\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(200, hidden_dim, KERNEL_SIZE, padding = PADDING),\n",
    "#             nn.ReLU()\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "\n",
    "def evaluate_model_on_test(model, test_data_filename):\n",
    "    test_data_loader =convert_to_list(test_data_filename)\n",
    "    \n",
    "    model.eval() #change model to 'eval' mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    p5 = 0\n",
    "    MRR = 0\n",
    "    all_data = []\n",
    "    for sample in test_data_loader:\n",
    "        #print sample\n",
    "        target_id=sample[0]\n",
    "        similar_ids=sample[1]\n",
    "        #print similar_ids, 'similar'\n",
    "        candidate_ids=sample[2]\n",
    "        #print candidate_ids, 'candidates'\n",
    "        titles = convert_sampleids_to_titles([sample], lookup,is_dev_or_test=True)[0]\n",
    "\n",
    "        #print titles\n",
    "        twenty_one_features = [title_to_feature_matrix(title) for title in titles]\n",
    "        \n",
    "        cosine_sims = sample_to_cosine_sims(twenty_one_features, model = model)\n",
    "        #print cosine_sims\n",
    "        #_, predicted = torch.max(cosine_sims, 0)\n",
    "       # total += label.size(0)\n",
    "       # label = label.squeeze()\n",
    "        \n",
    "        #correct += ((predicted == label).sum().data.numpy().squeeze())\n",
    "        cosine_sims = [cos_sim.data.numpy()[0] for cos_sim in cosine_sims]\n",
    "        #cosine_sims = cosine_sims.data.numpy().T\n",
    "        #print cosine_sims, \"is cos sims\"\n",
    "        #print candidate_ids, 'is candidates'\n",
    "        gold_labels = [1 if identity in similar_ids else 0 for identity in candidate_ids]\n",
    "        #print gold_labels, \"is gold\"\n",
    "        \n",
    "        index_order = sorted(range(len(cosine_sims)), reverse = True,key=lambda k: cosine_sims[k])\n",
    "        #print index_order, \"is index order\"\n",
    "        \n",
    "        data = [gold_labels[index] for index in index_order]\n",
    "        all_data.append(data)\n",
    "#         for instance_idx in range(len(cosine_sims)):\n",
    "#             cosine_sims_at_idx = cosine_sims[instance_idx]            \n",
    "#             print \"Cosine sims at idx: {}\".format(cosine_sims_at_idx)\n",
    "#             a\n",
    "#             loc = np.squeeze(np.where(cosine_sims_at_idx.argsort()[::-1] == 0))\n",
    "#             data_instance = [0] * len(cosine_sims_at_idx)\n",
    "#             data_instance[loc] = 1\n",
    "#             data.append(data_instance)\n",
    "        \n",
    "#         for instance_idx in range(len(cosine_sims)):\n",
    "#             cosine_sims_at_idx = cosine_sims[instance_idx]\n",
    "#             top_5_indices = cosine_sims_at_idx.argsort()[-5:][::-1]\n",
    "#             label_at_idx = 0\n",
    "#             if label_at_idx in top_5_indices:\n",
    "#                 p5 += 1\n",
    "            \n",
    "#             MRR += 1. / (1. + np.squeeze(np.where(cosine_sims_at_idx.argsort()[::-1] == 0)))\n",
    "    return all_data \n",
    "\n",
    "def sample_to_cosine_sims(sample, model = None):\n",
    "    #RE-ORDER DIMENSIONS OF THE SAMPLE\n",
    "    sample = Variable(torch.FloatTensor(sample))\n",
    "    #print sample.unsqueeze(0)\n",
    "    # 1x21x200x38\n",
    "    sample = sample.unsqueeze(0).permute(1, 0, 2, 3)\n",
    "    if model != None:\n",
    "        net = model\n",
    "    #21 x 1 x 200 x 38\n",
    "    target_question_features = sample[0] # 1 x 1 x 200 x 38\n",
    "   # print target_question_features.data.numpy()shape, 'Is target\"'\n",
    "    other_question_features = sample[1:] # 20 x 1 x 200 x 38\n",
    "   # print other_question_features.data.numpy()shape, \"is other\"\n",
    "\n",
    "    #Determine lengths to know how many vectors to take the average across.\n",
    "    target_question_lengths = find_start_of_padding_for_batch(target_question_features.data)\n",
    "    other_questions_lengths = [find_start_of_padding_for_batch(negative.data) for negative in other_question_features]\n",
    "\n",
    "    #RUN THROUGH NET\n",
    "    target_question_net_output = net(target_question_features)\n",
    "    other_question_net_outputs = [net(negative) for negative in other_question_features]\n",
    "\n",
    "    #CREATE MASKS\n",
    "    target_questions_masks = [create_mask(_) for _ in target_question_lengths] #DIM = 50 x 100 x 38\n",
    "    other_questions_masks = [[create_mask(length) for length in length_list] #DIM = 50 x 20 x 100 x 38\n",
    "                                  for length_list in other_questions_lengths]\n",
    "\n",
    "    #APPLY MASKS\n",
    "    #Should the multiplicands, the masks, be Float Tensors or Variables? May have to be float tensors to ensure\n",
    "    #    pytorch's directed graph back-prop is maintained.\n",
    "\n",
    "    target_question_net_output_masked = target_question_net_output * Variable(torch.FloatTensor(target_questions_masks))\n",
    "    other_questions_net_output_masked = [other_question_net_outputs[idx] * \n",
    "                                              Variable(torch.FloatTensor(other_questions_masks[idx]))\n",
    "                                              for idx in range(20)]\n",
    "\n",
    "    #SUM OVER WORDS\n",
    "    target_question_net_output_masked_summed = torch.sum(target_question_net_output_masked, dim = 2) #DIM = 50 x 100\n",
    "    other_questions_net_output_masked_summed = [torch.sum(\n",
    "                                                    other_questions_net_output_masked[idx], dim = 2\n",
    "                                                    )for idx in range(20)] #DIM = 20 x 50 x 100\n",
    "\n",
    "\n",
    "    #cosine_similarity_pos = cos(target_question_net_output_masked_summed, positive_question_net_output_masked_summed)\n",
    "    # ^ DIM = 50\n",
    "    cosine_similarities = [cos(target_question_net_output_masked_summed, other_questions_net_output_masked_summed[idx])\n",
    "                              for idx in range(20)]\n",
    "    # ^ DIM = 20 x 50\n",
    "    #\n",
    "\n",
    "    #cosine_similarities = torch.stack([cosine_similarity_pos] + cosine_similarities_neg) # DIM = 21 x 50\n",
    "    return cosine_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \n",
      "Loss after epoch 0 :0.196269263048\n",
      "MAP:  0.538974168588\n",
      "MRR:  0.667962543733\n",
      "P@1:  0.539682539683\n",
      "P@5:  0.431746031746\n",
      "test\n",
      "MAP:  0.548128579171\n",
      "MRR:  0.684542323676\n",
      "P@1:  0.55376344086\n",
      "P@5:  0.41935483871\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \n",
      "Loss after epoch 1 :0.194819496013\n",
      "MAP:  0.538923234318\n",
      "MRR:  0.667981995958\n",
      "P@1:  0.539682539683\n",
      "P@5:  0.430687830688\n",
      "test\n",
      "MAP:  0.547918275735\n",
      "MRR:  0.683646266329\n",
      "P@1:  0.55376344086\n",
      "P@5:  0.41935483871\n",
      "0 1 2 3"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-dfe1f1cd9ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mpositive_questions_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcreate_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_question_lengths\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#DIM = 50 x 100 x 38\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         N_negative_questions_masks = [[create_mask(length) for length in length_list] #DIM = 50 x 20 x 100 x 38\n\u001b[0;32m---> 41\u001b[0;31m                                       for length_list in N_negative_questions_lengths]\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m#APPLY MASKS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-45409ff1934a>\u001b[0m in \u001b[0;36mcreate_mask\u001b[0;34m(word_length)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mword_length\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mHIDDEN_SIZE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mHIDDEN_SIZE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMAX_TITLE_LENGTH\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mword_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;31m#-------------------------------------CREATE DATA BATCHER-------------------------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;31m# where samples[0] = 1 (target) + 1 (positive) + n (negative)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "INPUT_DIM = (MAX_TITLE_LENGTH, NUM_FEATURES_PER_WORD)\n",
    "#net = CNN(INPUT_SIZE, HIDDEN_SIZE)\n",
    "net=torch.load(\"Net_Epoch_1e5_1\")\n",
    "criterion = nn.MultiMarginLoss(p=1, margin=MARGIN, weight = None, size_average=True) #HAHA just put these in to look smart \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "print \"Beginning training...\"\n",
    "# ----TRAINING\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for idx,(sample, label) in enumerate(final_train_data_loader):\n",
    "        print idx,\n",
    "#     for idx in range(NUM_BATCHES):\n",
    "        #Sample shape: [50, 22, 200, 38]\n",
    "        #50 - Batch size, 22 - Num questions per data point\n",
    "        sample = Variable(sample, requires_grad = True)\n",
    "        label = Variable(label, requires_grad = True)\n",
    "\n",
    "        #RE-ORDER DIMENSIONS OF THE SAMPLE\n",
    "        sample = sample.permute(1, 0, 2, 3)\n",
    "        \n",
    "        target_question_features = sample[0] # 50 x 200 x 38\n",
    "        positive_question_features = sample[1] # 50 x 200 x 38\n",
    "        N_negative_question_features = sample[2:] #20 x 50 x 200 x 38\n",
    "        \n",
    "        #Determine lengths to know how many vectors to take the average across.\n",
    "        target_question_lengths = find_start_of_padding_for_batch(target_question_features.data)\n",
    "        positive_question_lengths = find_start_of_padding_for_batch(positive_question_features.data)\n",
    "        N_negative_questions_lengths = [find_start_of_padding_for_batch(negative.data) for negative in N_negative_question_features]\n",
    "        \n",
    "        #RUN THROUGH NET\n",
    "        target_question_net_output = net(target_question_features)\n",
    "        positive_question_net_output = net(positive_question_features)\n",
    "        N_negative_question_net_outputs = [net(negative) for negative in N_negative_question_features]\n",
    "        \n",
    "        #CREATE MASKS\n",
    "        target_questions_masks = [create_mask(_) for _ in target_question_lengths] #DIM = 50 x 100 x 38\n",
    "        positive_questions_masks = [create_mask(_) for _ in positive_question_lengths] #DIM = 50 x 100 x 38\n",
    "        N_negative_questions_masks = [[create_mask(length) for length in length_list] #DIM = 50 x 20 x 100 x 38\n",
    "                                      for length_list in N_negative_questions_lengths]\n",
    "        \n",
    "        #APPLY MASKS\n",
    "        #Should the multiplicands, the masks, be Float Tensors or Variables? May have to be float tensors to ensure\n",
    "        #    pytorch's directed graph back-prop is maintained.\n",
    "\n",
    "        target_question_net_output_masked = target_question_net_output * Variable(torch.FloatTensor(target_questions_masks))\n",
    "        positive_question_net_output_masked = positive_question_net_output * Variable(torch.FloatTensor(positive_questions_masks))\n",
    "        N_negative_questions_net_output_masked = [N_negative_question_net_outputs[idx] * \n",
    "                                                  Variable(torch.FloatTensor(N_negative_questions_masks[idx]))\n",
    "                                                  for idx in range(NUM_NEGATIVE_SAMPLES)]\n",
    "\n",
    "        #SUM OVER WORDS\n",
    "        target_question_net_output_masked_summed = torch.sum(target_question_net_output_masked, dim = 2) #DIM = 50 x 100\n",
    "        positive_question_net_output_masked_summed = torch.sum(positive_question_net_output_masked, dim = 2) #DIM = 50 x 100\n",
    "        N_negative_questions_net_output_masked_summed = [torch.sum(\n",
    "                                                        N_negative_questions_net_output_masked[idx], dim = 2\n",
    "                                                        )for idx in range(NUM_NEGATIVE_SAMPLES)] #DIM = 20 x 50 x 100\n",
    "\n",
    "        \n",
    "        cosine_similarity_pos = cos(target_question_net_output_masked_summed, positive_question_net_output_masked_summed)\n",
    "        # ^ DIM = 50\n",
    "        cosine_similarities_neg = [cos(target_question_net_output_masked_summed, N_negative_questions_net_output_masked_summed[idx])\n",
    "                                  for idx in range(NUM_NEGATIVE_SAMPLES)]\n",
    "        # ^ DIM = 20 x 50\n",
    "        #\n",
    "\n",
    "        cosine_similarities = torch.stack([cosine_similarity_pos] + cosine_similarities_neg) # DIM = 21 x 50\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cosine_similarities = torch.t(cosine_similarities)\n",
    "        label = torch.squeeze(label)\n",
    "        loss = criterion(cosine_similarities, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    torch.save(net, \"Net_Epoch_1e5_{}\".format(epoch))\n",
    "    print \"\"\n",
    "    print \"Loss after epoch \" + str(epoch) + \" :\" + str(running_loss)\n",
    "    data = evaluate_model_on_test(net, 'askubuntu/dev.txt')\n",
    "    eval = Evaluation(data)\n",
    "    print \"MAP: \",eval.MAP()\n",
    "    print \"MRR: \", eval.MRR()\n",
    "    print \"P@1: \",eval.Precision(1)\n",
    "    print \"P@5: \",eval.Precision(5)\n",
    "    print \"test\"\n",
    "    data = evaluate_model_on_test(net, 'askubuntu/test.txt')\n",
    "    eval = Evaluation(data)\n",
    "    print \"MAP: \",eval.MAP()\n",
    "    print \"MRR: \", eval.MRR()\n",
    "    print \"P@1: \",eval.Precision(1)\n",
    "    print \"P@5: \",eval.Precision(5)\n",
    "    \n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Evaluation():\n",
    "\n",
    "    def __init__(self,data):\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "\n",
    "    def Precision(self,precision_at):\n",
    "        scores = []\n",
    "        for item in self.data:\n",
    "            temp = item[:precision_at]\n",
    "            if any(val==1 for val in item):\n",
    "                scores.append(sum([1 if val==1 else 0 for val in temp])*1.0 / len(temp) if len(temp) > 0 else 0.0)\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "\n",
    "    def MAP(self):\n",
    "        scores = []\n",
    "        missing_MAP = 0\n",
    "        for item in self.data:\n",
    "            temp = []\n",
    "            count = 0.0\n",
    "            for i,val in enumerate(item):\n",
    "                if val == 1:\n",
    "                    count += 1.0\n",
    "                    temp.append(count/(i+1))\n",
    "            if len(temp) > 0:\n",
    "                scores.append(sum(temp) / len(temp))\n",
    "            else:\n",
    "                missing_MAP += 1\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "\n",
    "    def MRR(self):\n",
    "\n",
    "        scores = []\n",
    "        for item in self.data:\n",
    "            for i,val in enumerate(item):\n",
    "                if val == 1:\n",
    "                    scores.append(1.0/(i+1))\n",
    "                    break\n",
    "\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"Net_Epoch_5\")\n",
    "data = evaluate_model_on_test(model, 'askubuntu/dev.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  0.551681464457\n",
      "MRR:  0.687877506776\n",
      "P@1:  0.559139784946\n",
      "P@5:  0.412903225806\n"
     ]
    }
   ],
   "source": [
    "eval = Evaluation(data)\n",
    "print \"MAP: \",eval.MAP()\n",
    "print \"MRR: \", eval.MRR()\n",
    "print \"P@1: \",eval.Precision(1)\n",
    "print \"P@5: \",eval.Precision(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LAMBDA = 0.5\n",
    "#-------------------------------DOMAIN ADAPTATION----------------------------------#\n",
    "class DCNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(200, hidden_dim, KERNEL_SIZE, padding = PADDING),\n",
    "#             nn.ReLU()\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.layer2 = \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------DOMAIN ADAPTATION (MAY BE UNNECESSARY) ----------------------------------#\n",
    "class GradReverse(torch.autograd.Function):\n",
    "    def forward(self, x):\n",
    "        return x.view_as(x)\n",
    "    def backward(self, grad_output):\n",
    "        return (grad_output * -LAMBDA) # need tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss1_fn = nn.MultiMarginLoss(p=1, margin=MARGIN, weight = None, size_average=True)\n",
    "loss2_fn = nn.BCELoss()\n",
    "optimizer1 = torch.optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
    "optimizer2 = torch.optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
    "#optimizer2 needs a negative learning rate\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for idx,(sample, label) in enumerate(train_loader):\n",
    "    #ITERATE OVER SAMPLES FROM BOTH DATASETS\n",
    "    #HOW DO WE GET LABELS FROM THE TARGET DATASET TO ASSESS ACCURACY AND BACK-PROP\n",
    "    \n",
    "        sample = Variable(sample, requires_grad = True)\n",
    "        label = Variable(label, requires_grad = True)\n",
    "        cosine_sims = sample_and_label_to_cosine_sims(sample, label)\n",
    "        \n",
    "        cosine_sims = torch.t(cosine_sims)\n",
    "        label = torch.squeeze(label)\n",
    "        \n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        \n",
    "        loss1 = loss1_fn(cosine_sims, label)\n",
    "        #loss2 = loss2_fn(TODO)\n",
    "        total_loss = loss1 - LAMBDA * loss2\n",
    "        \n",
    "        total_loss.backward()\n",
    "\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        running_loss += total_loss.data[0]\n",
    "    print \"Loss after epoch \" + str(epoch) + \" :\" + str(running_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22853"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_titles_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.ConcatDataset"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-dd4aba7ff684>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "type(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22853"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "len(train_titles_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
