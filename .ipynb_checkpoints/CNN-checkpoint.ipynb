{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_list(filename):\n",
    "    if filename.endswith('gz'):\n",
    "        with gzip.open(filename,'r')as f:\n",
    "            text_tokens = f.readlines()\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            text_tokens = f.readlines()\n",
    "    text_tokens = [token.replace('\\n','').split('\\t') for token in text_tokens]\n",
    "    text_tokens = [[token[0], token[1].split(' '), token[2].split(' ')] for token in text_tokens]\n",
    "                   \n",
    "    return text_tokens\n",
    "\n",
    "#Sample:question_id, similar_question_id, negative_question_id\n",
    "def convert_to_samples(filename):\n",
    "    my_list=convert_to_list(filename)\n",
    "    new_samples=[]\n",
    "    for original_sample in my_list:\n",
    "        for similar in original_sample[1]:\n",
    "            new_samples.append([original_sample[0], similar, original_sample[2][0]])# change this to include all negative \n",
    "                                                                                     # examples later\n",
    "    return new_samples\n",
    "def make_lookup_table_for_training_data(filename):\n",
    "    lookup={}\n",
    "    text_token_list=convert_to_list(filename)\n",
    "    for token in text_token_list:\n",
    "        lookup[token[0]] = {'title':token[1],'question':token[2]}\n",
    "    return lookup\n",
    "        \n",
    "#takes  sample_ids of [[q1,p1,n1],[q2,p2,n2]....]\n",
    "#outputs titles like [[q1_title, p1_title, n1_title],[q2_title,p2_title,n2_title]...]\n",
    "def convert_sampleids_to_titles(sample_ids,lookup):\n",
    "    #each sample_id [question_id, pos_id, neg_id]\n",
    "    titles = []\n",
    "    for sample_id in sample_ids:\n",
    "        #sample_id : question_id, similar_question_id, negative_question_id\n",
    "        try:\n",
    "            titles.append([lookup[str(identity)]['title'] for identity in sample_id])\n",
    "        except:\n",
    "            print sample_id\n",
    "    return titles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_tokenized.txt.gz has id \\t title \\t question body\n",
    "text_tokenized='askubuntu/text_tokenized.txt.gz'\n",
    "\n",
    "#train_random.txt\n",
    "#(1) the query question ID, (2) the list of similar question IDs, and (3) the list of randomly selected question IDs.\n",
    "train_random_filename='askubuntu/train_random.txt'\n",
    "\n",
    "#Each line contains (1) the query question ID, (2) the list of similar question IDs, (3) the list of 20 candidate question IDs and (4) the associated BM25 scores of these questions computed by the Lucene search engine. The second field (the set of similar questions) is a subset of the third field.\n",
    "dev_filename='askubuntu/dev.txt'\n",
    "test_filename='askubuntu/test.txt'\n",
    "\n",
    "train_samples = convert_to_samples(train_random_filename)\n",
    "dev_samples = convert_to_samples(dev_filename)\n",
    "test_samples = convert_to_samples(test_filename)\n",
    "\n",
    "lookup = make_lookup_table_for_training_data(text_tokenized)\n",
    "train_list = convert_to_list(train_random_filename)\n",
    "train_titles_only = convert_sampleids_to_titles(train_samples, lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = 'askubuntu/vector/vectors_pruned.200.txt.gz'\n",
    "f = gzip.open(word_embeddings, 'r')\n",
    "wv_text = [ ]\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    wv_text.append(line.strip())\n",
    "\n",
    "word_to_vec = {}\n",
    "\n",
    "for line in wv_text:\n",
    "    parts = line.split()\n",
    "    word = parts[0]\n",
    "    vector = np.array([float(v) for v in parts[1:]])\n",
    "    word_to_vec[word] = vector\n",
    "f.close()\n",
    "\n",
    "def extract_features(word):\n",
    "    word=word.encode('utf-8')\n",
    "    return word_to_vec.get(word,[0.0 for i in range(200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.81100000e-03,  -1.91690000e-02,   3.72350000e-02,\n",
       "         1.88340000e-02,   7.88700000e-03,  -6.64640000e-02,\n",
       "         8.59540000e-02,  -8.24730000e-02,   1.83440000e-02,\n",
       "        -8.74610000e-02,  -1.11447000e-01,  -3.71180000e-02,\n",
       "        -8.30300000e-03,   1.40190000e-02,   8.45250000e-02,\n",
       "         2.03400000e-03,  -2.82450000e-02,  -1.62900000e-02,\n",
       "         7.20170000e-02,   6.04480000e-02,  -1.39264000e-01,\n",
       "        -1.63540000e-02,   1.74352000e-01,   3.68500000e-03,\n",
       "        -9.88580000e-02,  -4.12110000e-02,  -4.93310000e-02,\n",
       "        -2.85100000e-02,  -9.08550000e-02,  -2.40180000e-02,\n",
       "        -9.21600000e-03,  -3.20450000e-02,  -3.30730000e-02,\n",
       "        -2.61440000e-02,   1.22893000e-01,  -3.29470000e-02,\n",
       "        -9.37550000e-02,   1.87461000e-01,   1.35400000e-03,\n",
       "         7.04130000e-02,   3.89100000e-03,  -2.50700000e-02,\n",
       "        -1.38250000e-02,   5.29970000e-02,  -9.65200000e-02,\n",
       "         5.73710000e-02,  -1.60480000e-02,   3.27330000e-02,\n",
       "         5.89300000e-02,   4.26530000e-02,  -1.06370000e-02,\n",
       "         4.93350000e-02,   8.89100000e-02,  -2.64200000e-03,\n",
       "         2.14790000e-02,  -1.32110000e-01,   8.38400000e-02,\n",
       "        -1.45993000e-01,  -5.50810000e-02,  -4.46940000e-02,\n",
       "         2.99750000e-02,   7.34200000e-03,  -1.41483000e-01,\n",
       "        -1.34992000e-01,   7.93700000e-03,   1.45378000e-01,\n",
       "        -3.50880000e-02,   6.77400000e-02,  -1.71040000e-02,\n",
       "         1.76290000e-02,   4.40340000e-02,  -7.18210000e-02,\n",
       "         1.02433000e-01,   3.35520000e-02,   2.11950000e-02,\n",
       "         2.98280000e-02,  -1.54515000e-01,  -2.36050000e-02,\n",
       "         3.20910000e-02,  -1.22121000e-01,  -4.24300000e-02,\n",
       "         1.00603000e-01,  -6.18580000e-02,   3.81820000e-02,\n",
       "        -4.36510000e-02,  -3.56280000e-02,  -6.03380000e-02,\n",
       "         1.80000000e-04,   2.48590000e-02,   7.88110000e-02,\n",
       "        -4.44400000e-02,  -2.40390000e-02,  -4.79100000e-02,\n",
       "         1.14525000e-01,   5.66190000e-02,  -5.52450000e-02,\n",
       "        -4.29180000e-02,  -1.61904000e-01,   1.26530000e-02,\n",
       "        -6.81030000e-02,  -3.02740000e-02,   5.56300000e-03,\n",
       "        -2.04090000e-01,   3.45900000e-03,  -4.42810000e-02,\n",
       "        -3.35830000e-02,   1.15700000e-03,  -3.03010000e-02,\n",
       "        -1.24050000e-02,  -9.53550000e-02,  -3.93550000e-02,\n",
       "         6.22650000e-02,  -4.42020000e-02,   8.81300000e-03,\n",
       "        -4.53580000e-02,  -5.34300000e-03,  -1.47241000e-01,\n",
       "         1.08890000e-02,   1.23100000e-02,   1.21187000e-01,\n",
       "         1.01320000e-02,  -3.62640000e-02,  -6.49760000e-02,\n",
       "         5.65480000e-02,   4.79890000e-02,   1.92400000e-03,\n",
       "         3.64970000e-02,  -2.26600000e-03,   1.00555000e-01,\n",
       "         3.12180000e-02,  -4.39490000e-02,  -1.24266000e-01,\n",
       "        -1.14630000e-02,  -1.15645000e-01,  -9.02710000e-02,\n",
       "        -8.23100000e-03,   3.45000000e-04,  -4.42670000e-02,\n",
       "        -5.25660000e-02,  -3.55760000e-02,  -7.44500000e-03,\n",
       "        -8.52170000e-02,  -1.80300000e-02,  -6.08760000e-02,\n",
       "         1.16299000e-01,   7.59800000e-03,  -2.05000000e-02,\n",
       "        -7.56390000e-02,  -9.87520000e-02,  -1.76250000e-02,\n",
       "        -1.02854000e-01,  -1.51860000e-01,  -9.26320000e-02,\n",
       "         6.58400000e-03,  -2.72720000e-02,  -5.68420000e-02,\n",
       "        -5.13840000e-02,  -4.56000000e-03,  -1.53380000e-02,\n",
       "         4.77810000e-02,   1.58880000e-02,   9.67640000e-02,\n",
       "         3.90780000e-02,   5.76590000e-02,  -1.34597000e-01,\n",
       "         5.30760000e-02,   1.08641000e-01,  -1.65740000e-02,\n",
       "        -3.55110000e-02,   1.21346000e-01,  -5.96160000e-02,\n",
       "        -2.46370000e-02,   9.78130000e-02,  -9.35810000e-02,\n",
       "        -1.11922000e-01,  -1.29078000e-01,   1.11417000e-01,\n",
       "        -1.19060000e-02,  -9.07660000e-02,  -6.25830000e-02,\n",
       "        -7.62640000e-02,   1.75780000e-02,  -1.09919000e-01,\n",
       "        -3.83820000e-02,   4.49640000e-02,  -1.06710000e-02,\n",
       "         3.48420000e-02,   8.67690000e-02,  -1.06745000e-01,\n",
       "         2.37510000e-02,   1.18238000e-01,   4.80480000e-02,\n",
       "         5.87720000e-02,  -8.39270000e-02,  -9.64800000e-03,\n",
       "        -3.85590000e-02,  -1.38668000e-01,   2.74070000e-02,\n",
       "        -6.86650000e-02,   3.87500000e-03])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features('laptop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maximum_title_and_body_length(lookup_table):\n",
    "    max_len_title = -1\n",
    "    max_len_question = -1\n",
    "    max_len_question_id = 0\n",
    "    for key, dict_val in lookup_table.iteritems():\n",
    "        len_title = len(dict_val['title'])\n",
    "        len_question = len(dict_val['question'])\n",
    "        if len_title > max_len_title:\n",
    "             max_len_title = len_title\n",
    "        if len_question > max_len_question:\n",
    "            max_len_question = len_question\n",
    "            max_len_question_id = key\n",
    "    return max_len_title, max_len_question\n",
    "\n",
    "def title_to_feature_matrix(title_word_list):\n",
    "    feature_matrix = []\n",
    "    for word in title_word_list:\n",
    "        word_features = extract_features(word)\n",
    "        feature_matrix.append(word_features)\n",
    "    #Pad the feature with zeros to ensure all inputs to the net have the same dimension\n",
    "    feature_matrix += [[0.] * NUM_FEATURES_PER_WORD] * (MAX_TITLE_LENGTH - len(title_word_list))\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TITLE_LENGTH, MAX_BODY_LENGTH = find_maximum_title_and_body_length(lookup)\n",
    "NUM_FEATURES_PER_WORD = 200\n",
    "INPUT_DIM = (MAX_TITLE_LENGTH, NUM_FEATURES_PER_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system', 'running', 'in', 'low', 'graphic', 'mode', '(', 'ubuntu', 'without', 'monitor', ')']\n",
      "[array([ 0.101999, -0.104434, -0.012801,  0.068122, -0.055403,  0.04133 ,\n",
      "        0.146494,  0.025313,  0.001449,  0.067583,  0.015489, -0.011643,\n",
      "        0.079966,  0.020786,  0.012335,  0.084839, -0.01937 ,  0.013663,\n",
      "        0.093786, -0.127042,  0.035466,  0.00035 ,  0.16625 ,  0.0992  ,\n",
      "       -0.010311,  0.00118 , -0.002777,  0.000727, -0.052451,  0.035446,\n",
      "        0.030237, -0.003303,  0.043869,  0.011238,  0.080022,  0.088194,\n",
      "        0.016981, -0.011537,  0.046581, -0.010625,  0.110818,  0.083209,\n",
      "        0.002172, -0.052772, -0.046101, -0.091841, -0.173323, -0.068792,\n",
      "       -0.046995,  0.116353,  0.025877,  0.002772, -0.119018, -0.018811,\n",
      "       -0.142059, -0.095646,  0.134619, -0.093484, -0.066167, -0.018144,\n",
      "        0.036287, -0.053314, -0.063813,  0.066953,  0.064343, -0.015581,\n",
      "       -0.060295,  0.054359, -0.003971, -0.01272 ,  0.037697, -0.002866,\n",
      "       -0.031938,  0.020962,  0.003657, -0.115319, -0.015997,  0.019431,\n",
      "       -0.148824,  0.002926, -0.008929,  0.00637 ,  0.066673,  0.062609,\n",
      "       -0.035746, -0.022361,  0.035271, -0.037284,  0.012711, -0.041654,\n",
      "        0.073218, -0.03207 ,  0.066648,  0.100522,  0.081701,  0.056682,\n",
      "       -0.065367, -0.066492,  0.045817, -0.091373,  0.070867, -0.015822,\n",
      "       -0.142668, -0.076061, -0.050016,  0.109206,  0.146333,  0.020278,\n",
      "       -0.150571,  0.062407,  0.095597, -0.015765, -0.099709, -0.094262,\n",
      "        0.076413, -0.042294,  0.064178, -0.030585,  0.0394  , -0.020219,\n",
      "       -0.104913, -0.158812, -0.13863 , -0.016303,  0.105947, -0.124963,\n",
      "        0.018038,  0.100143,  0.096721, -0.019731,  0.021967, -0.026572,\n",
      "       -0.034726, -0.080405, -0.10199 , -0.046495, -0.008799, -0.063096,\n",
      "       -0.101189,  0.088648,  0.065453, -0.112169,  0.005518,  0.031643,\n",
      "       -0.0009  ,  0.082253, -0.044522,  0.029979,  0.041824,  0.054816,\n",
      "       -0.172532,  0.063712, -0.053284, -0.039316, -0.041853,  0.081542,\n",
      "        0.01462 ,  0.111374, -0.049716,  0.101462,  0.133848,  0.023641,\n",
      "        0.067786, -0.012393, -0.071079, -0.059076,  0.022999,  0.067595,\n",
      "        0.045803,  0.069867, -0.071729, -0.077833,  0.040031, -0.052803,\n",
      "       -0.010621,  0.032286, -0.027394,  0.020886, -0.027118,  0.057691,\n",
      "       -0.11414 ,  0.031601,  0.138761, -0.092127, -0.10512 , -0.047159,\n",
      "       -0.063696,  0.013717, -0.056682,  0.028589,  0.07369 , -0.100235,\n",
      "       -0.076341,  0.086897,  0.001546, -0.009197,  0.066538,  0.034353,\n",
      "       -0.013605, -0.037034]), array([ 0.00388 , -0.07965 , -0.044619,  0.095483,  0.018962, -0.102882,\n",
      "        0.068198,  0.024965,  0.079551, -0.025361,  0.008779, -0.028416,\n",
      "       -0.036122, -0.029932,  0.060615,  0.134367,  0.00268 ,  0.052231,\n",
      "        0.069835,  0.002229, -0.035813, -0.015651,  0.117665, -0.054685,\n",
      "       -0.163839, -0.080867, -0.058061,  0.011638, -0.059016,  0.022754,\n",
      "       -0.101207, -0.042567, -0.003332, -0.00827 ,  0.042733,  0.127133,\n",
      "       -0.034841,  0.060726, -0.072197,  0.024961, -0.083112,  0.062282,\n",
      "        0.056597, -0.040748, -0.041448,  0.067093, -0.021447, -0.008941,\n",
      "        0.108094,  0.215883, -0.017016,  0.129096,  0.046976, -0.083868,\n",
      "       -0.119737, -0.128715,  0.061541,  0.007091,  0.02475 , -0.077852,\n",
      "        0.025128,  0.084817, -0.079773, -0.015781,  0.015165,  0.038227,\n",
      "       -0.093509, -0.005912, -0.066311, -0.082167,  0.026287, -0.027127,\n",
      "       -0.099174, -0.057534,  0.07809 , -0.031725, -0.082358,  0.092151,\n",
      "       -0.062761, -0.057227, -0.039513, -0.070926,  0.076663,  0.039114,\n",
      "       -0.09476 ,  0.08074 , -0.006493, -0.07187 , -0.038054, -0.051068,\n",
      "        0.080741, -0.16395 , -0.020684,  0.071567, -0.024986, -0.086429,\n",
      "        0.048642, -0.093903,  0.111597, -0.091059, -0.067198,  0.015919,\n",
      "       -0.056595, -0.039347, -0.004299,  0.035799,  0.064422, -0.10211 ,\n",
      "       -0.06716 ,  0.00678 ,  0.084334,  0.030276, -0.052083, -0.115978,\n",
      "       -0.02813 ,  0.039364, -0.030335,  0.142537,  0.089607, -0.052713,\n",
      "       -0.024517, -0.054463, -0.128749,  0.136861,  0.026066, -0.100804,\n",
      "        0.027548,  0.078032,  0.002797, -0.012969, -0.029679, -0.006017,\n",
      "       -0.065239, -0.117699, -0.03631 , -0.026369, -0.032712,  0.151833,\n",
      "        0.003665,  0.01676 , -0.113898, -0.125908, -0.024295, -0.078071,\n",
      "       -0.05755 ,  0.044985,  0.056722, -0.01657 , -0.022082,  0.059265,\n",
      "       -0.130928, -0.006132,  0.041053,  0.027046, -0.040606,  0.010729,\n",
      "       -0.098985,  0.054571,  0.007403, -0.032288, -0.095592,  0.059317,\n",
      "        0.150953, -0.025789, -0.157158,  0.066734,  0.068776, -0.114391,\n",
      "        0.056354,  0.027638, -0.072304,  0.034054, -0.040835, -0.094889,\n",
      "       -0.02832 , -0.025317, -0.028249,  0.087764, -0.055906,  0.008868,\n",
      "       -0.099194, -0.083996, -0.005506,  0.056577, -0.063547,  0.024206,\n",
      "        0.08743 ,  0.05876 , -0.065384, -0.022398, -0.036675,  0.039332,\n",
      "       -0.059289, -0.030598, -0.145673, -0.018715,  0.008487, -0.021587,\n",
      "        0.023161, -0.135637]), array([-0.026436,  0.013091, -0.037213,  0.094008,  0.039496, -0.030626,\n",
      "        0.106052, -0.009825, -0.022424,  0.047754, -0.057013, -0.093403,\n",
      "       -0.004164, -0.19094 , -0.015362,  0.080511,  0.102038,  0.002089,\n",
      "       -0.065617, -0.078104,  0.008235, -0.034415,  0.140582,  0.054233,\n",
      "       -0.035552,  0.073125,  0.028581,  0.010852, -0.086031,  0.159688,\n",
      "       -0.068408,  0.02846 , -0.025463, -0.019506, -0.012018,  0.046946,\n",
      "       -0.117865, -0.154361,  0.094758, -0.088441,  0.103795, -0.020399,\n",
      "       -0.024594, -0.030091,  0.111771, -0.071276,  0.001699, -0.047573,\n",
      "        0.071711, -0.017758,  0.036481,  0.090264, -0.024416, -0.004795,\n",
      "       -0.057171, -0.018091,  0.106388, -0.033435, -0.103498,  0.003763,\n",
      "       -0.023196,  0.134001, -0.000989, -0.038822, -0.067019, -0.024269,\n",
      "        0.061651,  0.134481,  0.017688, -0.04837 , -0.076996,  0.038803,\n",
      "       -0.077502,  0.018709,  0.079743, -0.176662,  0.061871, -0.012845,\n",
      "       -0.211144, -0.03169 , -0.01794 , -0.021725,  0.130845,  0.086637,\n",
      "       -0.036727,  0.102625,  0.028945,  0.080068,  0.052419, -0.03998 ,\n",
      "        0.018075, -0.003494,  0.060203,  0.061711,  0.015685,  0.007035,\n",
      "       -0.090737, -0.05987 ,  0.14456 , -0.026704,  0.06975 , -0.039409,\n",
      "        0.004252, -0.060115,  0.021842,  0.023024,  0.043173, -0.072555,\n",
      "       -0.073805,  0.111846,  0.09557 ,  0.051618,  0.007698,  0.119024,\n",
      "       -0.008947,  0.078138,  0.10921 , -0.034764,  0.016862,  0.086231,\n",
      "       -0.048445,  0.091912, -0.105846, -0.053167, -0.025768,  0.010783,\n",
      "        0.143678, -0.025646, -0.013372, -0.09503 , -0.02382 ,  0.095026,\n",
      "        0.025879,  0.002002, -0.043   , -0.021211, -0.083977, -0.013721,\n",
      "       -0.037787,  0.069072,  0.063854, -0.078255,  0.048954,  0.020652,\n",
      "       -0.047754,  0.072937,  0.093265, -0.089096,  0.034124,  0.037444,\n",
      "       -0.102587,  0.036748, -0.026785,  0.046209,  0.044752,  0.058883,\n",
      "        0.073965, -0.043178,  0.047407, -0.069607,  0.067044, -0.069949,\n",
      "        0.009842, -0.007241,  0.012123,  0.014683,  0.039771,  0.021326,\n",
      "        0.053001,  0.054209,  0.010526, -0.010304,  0.195658,  0.039969,\n",
      "       -0.172944,  0.093299,  0.054482,  0.000761,  0.12833 , -0.043136,\n",
      "        0.05843 ,  0.032064, -0.00345 , -0.027876, -0.044506,  0.039456,\n",
      "       -0.145303,  0.109133, -0.01433 , -0.09122 , -0.052948, -0.089657,\n",
      "       -0.110393, -0.011801,  0.027094, -0.018522,  0.099411, -0.059916,\n",
      "        0.027431,  0.020814]), array([  3.21910000e-02,  -6.83880000e-02,   4.54900000e-02,\n",
      "         2.23330000e-02,   2.48390000e-02,  -6.94100000e-03,\n",
      "        -4.62760000e-02,  -5.70180000e-02,   6.62130000e-02,\n",
      "         1.18511000e-01,  -5.27200000e-02,   5.10140000e-02,\n",
      "         5.54610000e-02,  -1.99680000e-02,  -5.03690000e-02,\n",
      "         5.62230000e-02,   5.35270000e-02,  -5.62400000e-02,\n",
      "         1.10485000e-01,  -4.60410000e-02,   1.28550000e-01,\n",
      "         6.29200000e-03,  -6.26760000e-02,  -1.73808000e-01,\n",
      "         8.86500000e-03,  -2.67860000e-02,   8.80520000e-02,\n",
      "         3.58180000e-02,  -4.62970000e-02,   5.32570000e-02,\n",
      "         3.50860000e-02,  -4.18190000e-02,   9.02530000e-02,\n",
      "        -1.03925000e-01,   6.30630000e-02,   6.13630000e-02,\n",
      "         6.98160000e-02,   2.08450000e-02,  -7.22940000e-02,\n",
      "         4.70730000e-02,   1.68132000e-01,   1.46762000e-01,\n",
      "         8.13200000e-02,  -6.94900000e-03,  -1.49511000e-01,\n",
      "         7.13870000e-02,  -1.20798000e-01,  -3.75380000e-02,\n",
      "         1.66400000e-03,   1.25810000e-02,  -2.03490000e-02,\n",
      "        -1.22206000e-01,  -1.14938000e-01,   2.04340000e-02,\n",
      "        -8.57830000e-02,  -6.06030000e-02,   5.75680000e-02,\n",
      "        -3.32320000e-02,  -1.14558000e-01,   4.24870000e-02,\n",
      "        -5.98880000e-02,   2.00330000e-02,  -1.61730000e-02,\n",
      "        -7.20440000e-02,   1.01556000e-01,   4.64620000e-02,\n",
      "        -1.44000000e-04,   1.04545000e-01,  -6.77430000e-02,\n",
      "         8.34660000e-02,   1.53521000e-01,  -7.23060000e-02,\n",
      "         2.18370000e-02,   3.87750000e-02,  -8.89800000e-03,\n",
      "         1.07400000e-02,   3.59910000e-02,  -1.54300000e-03,\n",
      "        -1.18786000e-01,  -1.55630000e-02,  -4.95250000e-02,\n",
      "        -4.03680000e-02,  -4.47980000e-02,   5.50700000e-03,\n",
      "         3.91730000e-02,   4.92160000e-02,   8.32430000e-02,\n",
      "         6.12550000e-02,  -8.41700000e-02,   7.10800000e-02,\n",
      "         4.05110000e-02,   3.66010000e-02,  -1.34595000e-01,\n",
      "         4.11120000e-02,   7.92060000e-02,  -5.60770000e-02,\n",
      "        -5.56040000e-02,  -9.84020000e-02,   6.40940000e-02,\n",
      "        -1.31892000e-01,  -1.05462000e-01,  -3.37830000e-02,\n",
      "        -4.34630000e-02,  -9.58040000e-02,   1.21370000e-02,\n",
      "         1.65920000e-01,   5.35780000e-02,  -3.14840000e-02,\n",
      "        -8.90990000e-02,  -8.65520000e-02,  -9.67100000e-02,\n",
      "         7.10270000e-02,  -4.01080000e-02,   7.36760000e-02,\n",
      "         5.12300000e-02,   7.04020000e-02,   7.12040000e-02,\n",
      "        -2.46060000e-02,  -2.22930000e-02,  -3.47360000e-02,\n",
      "         3.98590000e-02,  -7.58130000e-02,   2.21460000e-02,\n",
      "        -3.73200000e-03,   4.01670000e-02,  -9.80150000e-02,\n",
      "         4.94380000e-02,   1.39138000e-01,   1.11044000e-01,\n",
      "         3.91670000e-02,   2.21710000e-02,   6.89430000e-02,\n",
      "        -3.42950000e-02,  -6.94310000e-02,  -3.15490000e-02,\n",
      "        -1.27990000e-02,  -8.69200000e-03,   1.98510000e-02,\n",
      "         2.57080000e-02,  -1.77100000e-02,   6.78220000e-02,\n",
      "        -1.92915000e-01,  -1.13730000e-02,   5.07600000e-02,\n",
      "        -8.19530000e-02,  -3.43180000e-02,  -3.14900000e-02,\n",
      "         8.79000000e-04,  -2.56720000e-02,   1.74000000e-02,\n",
      "        -1.38281000e-01,  -5.41860000e-02,  -6.31110000e-02,\n",
      "        -5.74810000e-02,   9.56840000e-02,   1.45460000e-02,\n",
      "        -2.23110000e-02,   1.24280000e-02,   2.64210000e-02,\n",
      "         6.75520000e-02,  -1.10913000e-01,  -8.46490000e-02,\n",
      "         1.01722000e-01,  -2.13580000e-02,   8.11510000e-02,\n",
      "         1.00440000e-02,   9.85390000e-02,   9.86680000e-02,\n",
      "         1.46015000e-01,   8.68950000e-02,  -9.06900000e-03,\n",
      "        -3.25790000e-02,  -1.56780000e-02,   3.34750000e-02,\n",
      "        -2.09480000e-02,  -3.63810000e-02,  -5.33470000e-02,\n",
      "        -1.09579000e-01,  -2.98100000e-02,  -4.19300000e-03,\n",
      "        -2.61890000e-02,  -3.51170000e-02,   2.12730000e-02,\n",
      "        -4.67150000e-02,   3.17560000e-02,   2.52000000e-04,\n",
      "        -8.08840000e-02,  -3.39750000e-02,  -5.40510000e-02,\n",
      "        -6.72960000e-02,   8.11700000e-03,  -6.75720000e-02,\n",
      "        -7.91610000e-02,   3.37380000e-02,  -3.97920000e-02,\n",
      "        -1.58042000e-01,  -2.18120000e-02,  -9.71700000e-03,\n",
      "        -5.15180000e-02,  -1.80157000e-01]), array([ 0.046651, -0.120626, -0.022432, -0.068384,  0.147493, -0.100691,\n",
      "        0.02227 ,  0.066006,  0.12397 , -0.103217,  0.071457, -0.027324,\n",
      "        0.097673,  0.099346,  0.01639 , -0.018216,  0.019738, -0.071476,\n",
      "        0.129146, -0.008121, -0.143066, -0.045171,  0.047754,  0.000896,\n",
      "       -0.009875, -0.043761,  0.107505, -0.061695,  0.080593,  0.065744,\n",
      "        0.107505,  0.067167,  0.044765, -0.014697,  0.015817,  0.063342,\n",
      "       -0.120149,  0.042353,  0.085771,  0.105098, -0.085691,  0.120069,\n",
      "        0.036673,  0.039687, -0.07139 ,  0.055244, -0.047602, -0.050881,\n",
      "        0.044074, -0.051483, -0.049493, -0.08213 , -0.077391,  0.010173,\n",
      "        0.050366,  0.053499, -0.158101,  0.006791, -0.075187, -0.102242,\n",
      "       -0.057682, -0.038183, -0.068636, -0.012367, -0.021908, -0.042927,\n",
      "       -0.084998,  0.010615,  0.116935, -0.071306,  0.011186, -0.054045,\n",
      "        0.062465,  0.073422,  0.106466, -0.002411,  0.035274,  0.074192,\n",
      "        0.003164,  0.109627, -0.025629,  0.025241, -0.065735, -0.157421,\n",
      "        0.023873, -0.001969,  0.044695, -0.02269 , -0.016277, -0.070737,\n",
      "        0.074526,  0.09127 ,  0.04216 ,  0.060802,  0.033725, -0.047236,\n",
      "       -0.001967,  0.018586, -0.089302,  0.032687,  0.031296, -0.11045 ,\n",
      "       -0.006642, -0.018272,  0.013169, -0.009495, -0.070165,  0.028623,\n",
      "       -0.058423,  0.042011,  0.038826,  0.030031,  0.009837,  0.1284  ,\n",
      "        0.027549,  0.108867,  0.176753,  0.093908, -0.051989,  0.017548,\n",
      "       -0.069222, -0.034528, -0.039473,  0.02122 , -0.006693,  0.092971,\n",
      "       -0.001584,  0.084064, -0.01838 ,  0.104085, -0.020613, -0.074375,\n",
      "        0.011503, -0.04627 , -0.033986, -0.071412, -0.040415, -0.086161,\n",
      "       -0.122887,  0.020142,  0.019027, -0.020406,  0.042241,  0.060364,\n",
      "        0.044864,  0.176987,  0.037943,  0.099365, -0.05324 , -0.026059,\n",
      "        0.012318, -0.172261, -0.012617, -0.034628,  0.1446  , -0.082503,\n",
      "       -0.037303,  0.072873,  0.146605,  0.00368 ,  0.057287,  0.084241,\n",
      "        0.047519,  0.087969, -0.029129, -0.067517,  0.095286,  0.103102,\n",
      "        0.009538,  0.020197, -0.001653, -0.028415,  0.033009, -0.010777,\n",
      "       -0.05897 , -0.122777, -0.031559,  0.024303,  0.018873, -0.165574,\n",
      "        0.042205,  0.091532, -0.051098, -0.059523,  0.133224,  0.020239,\n",
      "        0.065122,  0.058785, -0.08637 , -0.012571,  0.031274,  0.094609,\n",
      "       -0.115158, -0.033759, -0.003687,  0.030974, -0.050146,  0.086991,\n",
      "       -0.026024, -0.070144]), array([ 0.042915, -0.090805, -0.086365,  0.097907,  0.087843,  0.082639,\n",
      "        0.035119,  0.070854, -0.02723 , -0.047617, -0.000899, -0.064342,\n",
      "        0.022034,  0.102585,  0.116411, -0.014506, -0.006446, -0.012442,\n",
      "        0.043184,  0.069139,  0.050178,  0.021067,  0.003405,  0.009849,\n",
      "       -0.131472,  0.010321, -0.033593, -0.047017, -0.033196,  0.081696,\n",
      "       -0.043607,  0.009286,  0.091186, -0.135718, -0.0249  ,  0.031905,\n",
      "        0.042254,  0.090054,  0.014637, -0.062582, -0.025227, -0.027844,\n",
      "       -0.043417,  0.143225, -0.118907, -0.00531 ,  0.021075, -0.041615,\n",
      "        0.08117 , -0.010012,  0.026049, -0.059226, -0.015461, -0.013159,\n",
      "        0.125075, -0.049967, -0.147714, -0.015597,  0.003822, -0.03906 ,\n",
      "       -0.111861,  0.068866, -0.137901, -0.054949,  0.069431, -0.084212,\n",
      "       -0.008509, -0.000965, -0.113775, -0.040004,  0.061372,  0.046335,\n",
      "       -0.084691, -0.097451, -0.024422, -0.095321, -0.060318, -0.037143,\n",
      "       -0.035831, -0.02183 , -0.078435, -0.091864, -0.120862,  0.008723,\n",
      "       -0.039385,  0.009848,  0.018661, -0.162381,  0.104207,  0.122426,\n",
      "       -0.07519 ,  0.066233,  0.133954,  0.092328, -0.022828, -0.090634,\n",
      "        0.062232,  0.046063, -0.041126, -0.18804 , -0.033543, -0.062511,\n",
      "       -0.076265, -0.112503, -0.076013, -0.021935, -0.011561, -0.021148,\n",
      "       -0.099236, -0.035295,  0.07973 ,  0.045689, -0.005717,  0.02598 ,\n",
      "       -0.032804, -0.016126, -0.068529,  0.075364,  0.055305,  0.03459 ,\n",
      "        0.014734,  0.017372, -0.024842, -0.036019, -0.052635, -0.12241 ,\n",
      "       -0.025454,  0.23201 ,  0.113856,  0.126492,  0.039511,  0.058218,\n",
      "        0.011417, -0.142975, -0.095228,  0.039731, -0.044688, -0.038571,\n",
      "        0.027978, -0.066606, -0.060034, -0.014556,  0.129703,  0.027661,\n",
      "       -0.014176, -0.011394, -0.018627,  0.010725, -0.044406,  0.046791,\n",
      "       -0.010816, -0.08598 , -0.104181, -0.01513 ,  0.033412,  0.021945,\n",
      "       -0.170546,  0.031575, -0.019536,  0.009547, -0.067403,  0.01934 ,\n",
      "        0.028661,  0.1842  ,  0.013243,  0.011778,  0.08924 , -0.086883,\n",
      "        0.0063  , -0.042025,  0.029586, -0.071053,  0.073574,  0.000509,\n",
      "        0.056074,  0.024445, -0.027354,  0.107845,  0.06839 , -0.022298,\n",
      "       -0.029875, -0.035308,  0.023653, -0.042231, -0.164611,  0.044834,\n",
      "        0.088532, -0.048089, -0.060814, -0.08823 ,  0.024668,  0.096611,\n",
      "       -0.057882, -0.045595, -0.071158,  0.048986,  0.000753, -0.058298,\n",
      "        0.05976 ,  0.019894]), [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], array([-0.04749 ,  0.046277, -0.111991,  0.048031, -0.088803, -0.005961,\n",
      "        0.001037,  0.082144,  0.164154,  0.061668, -0.058946, -0.086221,\n",
      "        0.046795,  0.057804, -0.005059,  0.043721, -0.069975,  0.004375,\n",
      "        0.03442 ,  0.028032, -0.095232, -0.015182,  0.122969,  0.096178,\n",
      "       -0.10014 , -0.001811, -0.094759, -0.056707,  0.050773,  0.011385,\n",
      "       -0.103219,  0.082944, -0.044291, -0.040729,  0.098751, -0.094091,\n",
      "       -0.022944,  0.098297,  0.017656,  0.045522, -0.016555, -0.016721,\n",
      "       -0.006344,  0.117215,  0.019426,  0.114181,  0.026091,  0.009715,\n",
      "        0.039369,  0.016337, -0.013044,  0.053059,  0.039523,  0.04373 ,\n",
      "       -0.006484, -0.162806,  0.06786 , -0.108397, -0.002484, -0.011144,\n",
      "        0.017113,  0.046007, -0.158539, -0.169634,  0.01068 , -0.028988,\n",
      "        0.038388, -0.041861, -0.155772, -0.068314,  0.011181, -0.064349,\n",
      "        0.036227,  0.015447,  0.024303, -0.083836, -0.103294, -0.05462 ,\n",
      "        0.022987, -0.015032,  0.106493,  0.041027, -0.069554, -0.010377,\n",
      "        0.0205  ,  0.041265,  0.040773, -0.091836, -0.033256, -0.028598,\n",
      "       -0.107077, -0.166977, -0.00182 , -0.115762, -0.119077, -0.045848,\n",
      "        0.057638, -0.04613 , -0.047456, -0.069754, -0.082049,  0.03692 ,\n",
      "       -0.131057,  0.107396, -0.044836,  0.083402,  0.024791,  0.051067,\n",
      "       -0.080123, -0.024726,  0.011707,  0.06444 , -0.072125,  0.015714,\n",
      "       -0.010816, -0.071834, -0.006811,  0.150166,  0.029001,  0.051925,\n",
      "        0.01357 ,  0.009681,  0.018343, -0.028535, -0.025913,  0.026879,\n",
      "        0.026102,  0.138434,  0.004272,  0.00067 , -0.004793,  0.000487,\n",
      "       -0.121033, -0.031016, -0.080158, -0.027939,  0.0268  , -0.009852,\n",
      "       -0.027925, -0.090251, -0.058967, -0.144027, -0.097509, -0.10055 ,\n",
      "        0.060126,  0.023589, -0.04944 , -0.106517, -0.068263,  0.094442,\n",
      "       -0.010947, -0.07258 , -0.109378,  0.007919, -0.056013, -0.02568 ,\n",
      "        0.005764,  0.036164,  0.001357,  0.035982,  0.002773,  0.004489,\n",
      "        0.087407,  0.028779, -0.129109,  0.005592,  0.035706,  0.032574,\n",
      "       -0.058835,  0.128846, -0.039504,  0.098751, -0.054298, -0.092317,\n",
      "       -0.088511,  0.050168,  0.012876,  0.02803 , -0.068595,  0.066229,\n",
      "       -0.105726, -0.047025, -0.17159 ,  0.10301 ,  0.08725 ,  0.077978,\n",
      "        0.04367 ,  0.101637,  0.083282,  0.092384,  0.062794,  0.056819,\n",
      "        0.063767, -0.001363,  0.010495,  0.02367 , -0.169971,  0.087138,\n",
      "       -0.021439, -0.046638]), array([ 0.09447 , -0.129502,  0.005414,  0.089832, -0.018835, -0.024807,\n",
      "        0.002988, -0.002734, -0.011974,  0.109499, -0.056582,  0.045579,\n",
      "        0.063806,  0.062068,  0.080407,  0.062628, -0.025184, -0.04997 ,\n",
      "        0.126034,  0.031948, -0.074577, -0.064089,  0.019285,  0.055115,\n",
      "        0.00764 ,  0.11775 ,  0.093885,  0.083901, -0.004826,  0.10895 ,\n",
      "        0.074659,  0.047943, -0.004648, -0.018698, -0.049313, -0.043714,\n",
      "        0.070614,  0.022479, -0.091317,  0.137191, -0.009522,  0.030384,\n",
      "       -0.09448 ,  0.043998, -0.002285, -0.009328, -0.108735,  0.050736,\n",
      "        0.048176,  0.0509  , -0.020572,  0.015758, -0.123313,  0.008684,\n",
      "       -0.184168,  0.054845,  0.098294, -0.014412, -0.069023, -0.057339,\n",
      "       -0.002699,  0.089771, -0.131167,  0.051981, -0.003037, -0.030402,\n",
      "       -0.024185,  0.109905,  0.022711,  0.005139,  0.114397, -0.013687,\n",
      "       -0.118316, -0.097543,  0.039057,  0.066807,  0.029175,  0.006266,\n",
      "       -0.034658, -0.046525,  0.03293 ,  0.054475,  0.055167,  0.008437,\n",
      "       -0.038304, -0.042091,  0.070239, -0.013307, -0.005689, -0.051561,\n",
      "        0.043952, -0.001356,  0.027741,  0.082937,  0.027558, -0.030325,\n",
      "       -0.150187, -0.061645,  0.134522,  0.002179, -0.024102, -0.065427,\n",
      "       -0.009124,  0.039387,  0.078117,  0.03092 ,  0.116248, -0.026306,\n",
      "        0.007138,  0.024762, -0.015974, -0.04603 , -0.149427, -0.01136 ,\n",
      "        0.08748 ,  0.024286,  0.008156,  0.04025 , -0.084542,  0.122436,\n",
      "        0.09398 , -0.014411,  0.019537, -0.116866,  0.107505, -0.109424,\n",
      "       -0.027201,  0.079293,  0.09712 ,  0.031067,  0.095918,  0.036811,\n",
      "       -0.072701, -0.027809,  0.017176, -0.06934 , -0.152289, -0.018396,\n",
      "       -0.07244 , -0.014057,  0.025013, -0.05147 ,  0.013648,  0.04682 ,\n",
      "       -0.014833, -0.04238 , -0.045896, -0.034473, -0.02531 , -0.010605,\n",
      "       -0.119883,  0.032092, -0.102089, -0.079986,  0.116914,  0.182893,\n",
      "       -0.076132, -0.052366,  0.032213,  0.09964 ,  0.061018, -0.01908 ,\n",
      "        0.133489,  0.053821, -0.061191,  0.023838, -0.096349,  0.03617 ,\n",
      "       -0.090353,  0.002342,  0.018911, -0.145014,  0.044722, -0.139646,\n",
      "        0.019418, -0.037091, -0.003028,  0.030316,  0.070737,  0.063842,\n",
      "       -0.040504, -0.158155,  0.140359,  0.07844 , -0.156577, -0.077366,\n",
      "       -0.09815 ,  0.082047, -0.065671, -0.036843,  0.041521, -0.067239,\n",
      "       -0.007128,  0.021635,  0.043416,  0.015562,  0.078258,  0.081246,\n",
      "       -0.05032 , -0.044658]), array([ 0.083753, -0.064225,  0.078454, -0.036437,  0.041804,  0.011449,\n",
      "        0.092104,  0.026402,  0.043346,  0.113107, -0.103908, -0.028816,\n",
      "        0.154309,  0.058778,  0.060656, -0.084662,  0.056404, -0.015459,\n",
      "       -0.019393,  0.109066,  0.047658, -0.115179,  0.112736, -0.090308,\n",
      "       -0.108659,  0.013312,  0.037042,  0.001211, -0.049267,  0.106369,\n",
      "       -0.039223, -0.04373 ,  0.026544, -0.083778,  0.046984, -0.038339,\n",
      "       -0.020344,  0.142012, -0.02941 ,  0.014319, -0.037761, -0.00243 ,\n",
      "        0.110985, -0.055282, -0.091515,  0.023311, -0.00806 ,  0.00516 ,\n",
      "       -0.11036 ,  0.030037,  0.033257,  0.057081,  0.113957,  0.006578,\n",
      "        0.050597,  0.010635,  0.103278,  0.037023, -0.021536, -0.031211,\n",
      "        0.063461,  0.040476,  0.042523,  0.008747, -0.032171,  0.091938,\n",
      "       -0.015692,  0.056772,  0.082118, -0.086153, -0.038829,  0.052964,\n",
      "        0.046897,  0.055469,  0.07239 , -0.03228 ,  0.005476,  0.103758,\n",
      "       -0.130873, -0.085863, -0.028305, -0.01778 , -0.079592, -0.012535,\n",
      "       -0.014126, -0.125203, -0.080914,  0.003481, -0.027551,  0.024683,\n",
      "        0.029139,  0.045113, -0.142949,  0.084401,  0.119589, -0.040942,\n",
      "        0.068869, -0.043062,  0.049165, -0.087067, -0.009544, -0.080968,\n",
      "       -0.080053,  0.011095, -0.043214, -0.04276 ,  0.001431, -0.092506,\n",
      "        0.027426, -0.053949,  0.091724,  0.062803,  0.062247, -0.08046 ,\n",
      "       -0.071547,  0.087946, -0.01523 ,  0.094374,  0.03758 ,  0.031555,\n",
      "       -0.124067, -0.164138, -0.036092,  0.033485, -0.028015, -0.1427  ,\n",
      "        0.039326,  0.042219,  0.054642,  0.119066,  0.006435, -0.005872,\n",
      "       -0.099113, -0.023833, -0.096012,  0.011048, -0.092009,  0.058169,\n",
      "        0.017273,  0.02563 ,  0.017583, -0.121083,  0.045973,  0.034871,\n",
      "       -0.013333,  0.16827 ,  0.0199  , -0.008468, -0.062637,  0.002704,\n",
      "       -0.041369, -0.122723,  0.002352,  0.083951,  0.170611, -0.069487,\n",
      "       -0.098302,  0.064703, -0.05549 ,  0.179242, -0.14687 ,  0.172907,\n",
      "        0.05968 ,  0.075513, -0.061575,  0.044155,  0.077362, -0.043704,\n",
      "        0.022111,  0.073868, -0.016495, -0.049321, -0.038741, -0.156428,\n",
      "       -0.015253, -0.098722,  0.055132, -0.011333,  0.020784, -0.09949 ,\n",
      "       -0.02688 ,  0.03523 , -0.004815,  0.01171 ,  0.002074,  0.040539,\n",
      "       -0.015652,  0.101453, -0.044818, -0.050932,  0.027189,  0.014463,\n",
      "       -0.08172 , -0.017036, -0.051483, -0.0657  , -0.056354, -0.048252,\n",
      "        0.004631, -0.111785]), [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print train_titles_only[0][0]\n",
    "print title_to_feature_matrix(train_titles_only[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(200, 200, KERNEL_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(KERNEL_SIZE)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.1020 -0.1044 -0.0128  ...   0.0344 -0.0136 -0.0370\n",
      " 0.0039 -0.0796 -0.0446  ...  -0.0216  0.0232 -0.1356\n",
      "-0.0264  0.0131 -0.0372  ...  -0.0599  0.0274  0.0208\n",
      "          ...                          ...          \n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 38x200]\n",
      "\n",
      "torch.Size([38, 200])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 3D tensor as input, got 2D tensor instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-c52609f1c410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mnegative_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_to_feature_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtarget_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mpositive_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mnegative_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alexander/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-1903a21a7183>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         )\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alexander/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alexander/anaconda2/lib/python2.7/site-packages/torch/nn/modules/container.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alexander/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alexander/anaconda2/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 154\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alexander/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \"\"\"\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected 3D tensor as input, got {}D tensor instead.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     f = ConvNd(_single(stride), _single(padding), _single(dilation), False,\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 3D tensor as input, got 2D tensor instead."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "KERNEL_SIZE = 3\n",
    "INPUT_SIZE = 200\n",
    "HIDDEN_SIZE = 400\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 1\n",
    "net = CNN(INPUT_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "criterion = nn.MultiMarginLoss(p=1, margin=1, weight=None, size_average=True) #HAHA just put these in to look smart \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# ----TRAINING\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for sample in train_titles_only:\n",
    "        target_title = sample[0]\n",
    "        positive_title = sample[1]\n",
    "        negative_title = sample[2]\n",
    "        \n",
    "        print target_features\n",
    "        target_features = torch.FloatTensor(title_to_feature_matrix(target_title))\n",
    "        print target_features.shape\n",
    "        positive_features = torch.FloatTensor(title_to_feature_matrix(positive_title))\n",
    "        negative_features = torch.FloatTensor(title_to_feature_matrix(negative_title))\n",
    "        \n",
    "        target_vec = net(target_features)\n",
    "        positive_vec = net(positive_features)\n",
    "        negative_vec = net(negative_features)\n",
    "        \n",
    "        cos_sim_positive = cosine_similarity(target_vec, positive_vec)\n",
    "        cos_sim_negative = cosine_similarity(target_vec, negative_vec)\n",
    "        \n",
    "        cos_sims = [cos_sim_positive, cos_sim_negative]\n",
    "        \n",
    "        max_idx = np.argmax(cos_sims) #use axis = 1 when we use more negative examples later\n",
    "        \n",
    "        y = 0 \n",
    "        \"\"\"\n",
    "        because we know the 0th index in cos_sims is always the example we expect to most closely match \n",
    "        the target question\n",
    "        \"\"\"\n",
    "        \n",
    "        running_loss = 0.0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(max_idx, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        print \"Done batch \" + str(batch_num)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print \"Loss after epoch \" + str(epoch) + \" :\" + str(loss.data[0])\n",
    "# ----END TRAINING\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['system', 'running', 'in', 'low', 'graphic', 'mode', '(', 'ubuntu', 'without', 'monitor', ')'], ['getting', 'system', 'to', 'boot', 'in', 'headless', 'mode', 'set-up', 'without', 'display', 'problems'], ['using', 'apt-get', 'partially', 'through', 'a', 'proxy', 'server']]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
