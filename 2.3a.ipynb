{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------IMPORTS-------------------------------------#\n",
    "import torch\n",
    "import gzip\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------HELPER FUNCTIONS-------------------------------------#\n",
    "NUM_NEGATIVE_SAMPLES=20\n",
    "\n",
    "def N_random_values_in_list(full_list, N):\n",
    "    x=0\n",
    "    lower_bound  = 0\n",
    "    upper_bound = len(full_list)-1\n",
    "    sample_list=[]\n",
    "    random_nums=[]\n",
    "    while x < min(N,len(full_list)):\n",
    "        random_num = randint(lower_bound, upper_bound) # inclusive range\n",
    "        if random_num in random_nums:\n",
    "            continue\n",
    "        else:\n",
    "            random_nums.append(random_num)\n",
    "            x += 1\n",
    "    return [full_list[i] for i in random_nums]\n",
    "\n",
    "def convert_to_list(filename, is_android = False):\n",
    "    if filename.endswith('gz'):\n",
    "        with gzip.open(filename,'r')as f:\n",
    "            text_tokens = f.readlines()\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            text_tokens = f.readlines()\n",
    "    \n",
    "    text_tokens = [token.replace('\\n','').split('\\t') for token in text_tokens]\n",
    "    if is_android:\n",
    "        text_tokens = [token[0].split(' ') for token in text_tokens]\n",
    "    else:\n",
    "        text_tokens = [[token[0], token[1].split(' '), token[2].split(' ')] for token in text_tokens]\n",
    "                   \n",
    "    return text_tokens\n",
    "\n",
    "#Sample:question_id, similar_question_id, negative_question_id\n",
    "def convert_to_samples_ubuntu(filename):\n",
    "    my_list=convert_to_list(filename)\n",
    "    new_samples=[]\n",
    "    for original_sample in my_list:\n",
    "        for similar in original_sample[1]:\n",
    "            random_negative_samples = N_random_values_in_list(original_sample[2],NUM_NEGATIVE_SAMPLES)\n",
    "            new_samples.append([original_sample[0], similar, random_negative_samples])# change this to include all negative \n",
    "                                                                                 # examples later\n",
    "    return new_samples\n",
    "\n",
    "def convert_to_samples_android(pos_filename, neg_filename):\n",
    "    pos_list = convert_to_list(pos_filename, is_android = True)\n",
    "    neg_list = convert_to_list(neg_filename, is_android = True)\n",
    "\n",
    "    target_id_to_list_of_negative_ids = {}\n",
    "    for id_pair in neg_list:\n",
    "        target_id, negative_id = id_pair\n",
    "        if target_id in target_id_to_list_of_negative_ids:\n",
    "            target_id_to_list_of_negative_ids[target_id].append(negative_id)\n",
    "        else:\n",
    "            target_id_to_list_of_negative_ids[target_id] = [negative_id]\n",
    "    new_samples = []\n",
    "    for id_pair in pos_list:\n",
    "        target_id, positive_id = id_pair\n",
    "        negative_ids = target_id_to_list_of_negative_ids[target_id]\n",
    "        new_sample = [target_id, positive_id, negative_ids]\n",
    "        new_samples.append(new_sample)\n",
    "    return new_samples\n",
    "def make_lookup_table(filename):\n",
    "    lookup={}\n",
    "    text_token_list=convert_to_list(filename)\n",
    "    for token in text_token_list:\n",
    "        lookup[token[0]] = {'title':token[1],'question':token[2]}\n",
    "    return lookup\n",
    "        \n",
    "#takes  sample_ids of [[q1,p1,n1],[q2,p2,n2]....]\n",
    "#outputs titles like [[q1_title, p1_title, n1_title],[q2_title,p2_title,n2_title]...]\n",
    "def convert_sampleids_to_titles(sample_ids,lookup):\n",
    "    #each sample_id [question_id, pos_id, [neg_ids]]\n",
    "    #print type(sample_ids)==list, \"first\"\n",
    "   \n",
    "    titles = []\n",
    "    for sample_id in sample_ids:\n",
    "         #flatten list: [question_id, pos_id, [neg_ids]] --> [question_id, pos_id, neg_id1, neg_id2, ...]\n",
    "        sample_id= sample_id[:2]+sample_id[2][:]\n",
    "        #sample_id : question_id, similar_question_id, negative_question_id\n",
    "        try:\n",
    "            titles.append([[item.lower() for item in lookup[str(_)]['title']] for _ in sample_id])\n",
    "        except:\n",
    "            print \"Lookup failed\"\n",
    "    return titles\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "def extract_features(word):\n",
    "    try:\n",
    "        word=remove_non_ascii(word)\n",
    "        word=word.encode('utf-8')\n",
    "    except:\n",
    "        print(word)\n",
    "    return word_to_vec.get(word,[0.0 for i in range(200)])\n",
    "\n",
    "def find_maximum_title_and_body_length(lookup_table):\n",
    "    max_len_title = -1\n",
    "    max_len_question = -1\n",
    "    max_len_question_id = 0\n",
    "    for key, dict_val in lookup_table.iteritems():\n",
    "        len_title = len(dict_val['title'])\n",
    "        len_question = len(dict_val['question'])\n",
    "        if len_title > max_len_title:\n",
    "             max_len_title = len_title\n",
    "        if len_question > max_len_question:\n",
    "            max_len_question = len_question\n",
    "            max_len_question_id = key\n",
    "    return max_len_title, max_len_question\n",
    "\n",
    "def title_to_feature_matrix(title_word_list):\n",
    "    feature_matrix = []\n",
    "    for idx, word in enumerate(title_word_list):\n",
    "        if idx == PARAMETER_MAX_TITLE_LENGTH:\n",
    "            break\n",
    "        else:\n",
    "            word_features = extract_features(word)\n",
    "            feature_matrix.append(word_features)\n",
    "        \n",
    "    #Pad the feature with zeros to ensure all inputs to the net have the same dimension\n",
    "    feature_matrix += [[0] * NUM_FEATURES_PER_WORD] * (PARAMETER_MAX_TITLE_LENGTH - len(title_word_list))\n",
    "    #print np.array(feature_matrix).T.shape\n",
    "    return np.array(feature_matrix).T\n",
    "\n",
    "#array is structured like a batch of features 50x200x38\n",
    "def find_start_of_padding_for_batch(batch):\n",
    "    vec_lengths_in_batch = []\n",
    "    for batch_num in range(0, len(batch)):\n",
    "        single_vec = batch[batch_num]\n",
    "        length = find_start_of_padding_single_vec(single_vec) + 1\n",
    "        vec_lengths_in_batch.append(length)\n",
    "    return vec_lengths_in_batch\n",
    "\n",
    "#batch = 200x38\n",
    "def find_start_of_padding_single_vec(single_vec):\n",
    "    for idx in range(len(single_vec[0])-1, -1, -1):\n",
    "        if single_vec[0][idx] != 0.:\n",
    "            return idx\n",
    "    #if the whole sequence is 0s\n",
    "    return 0\n",
    "def create_mask(word_length):\n",
    "    return np.array([[1. / word_length] * CNN_HIDDEN_DIM] * word_length + [[0] * CNN_HIDDEN_DIM] * (MAX_TITLE_LENGTH - word_length)).T\n",
    "#-------------------------------------CREATE DATA BATCHER-------------------------------------#\n",
    "# where samples[0] = 1 (target) + 1 (positive) + n (negative) \n",
    "def create_data_loader(ubuntu_samples, android_samples, shuffle_data = True):\n",
    "    \n",
    "    features = []\n",
    "    #0: ubuntu dataset\n",
    "    #1: android dataset\n",
    "    for idx, samples in enumerate([ubuntu_samples, android_samples]):\n",
    "        for sample in samples:\n",
    "            from_dataset = idx\n",
    "            target_title = sample[0]\n",
    "            positive_title = sample[1]\n",
    "            negative_titles = sample[2:22] #:22 because there are more negatives in the android dataset, but a TA\n",
    "                                           # said the extras aren't necessary\n",
    "\n",
    "            target_features = title_to_feature_matrix(target_title)\n",
    "            positive_features = title_to_feature_matrix(positive_title)\n",
    "            n_negative_features = [title_to_feature_matrix(negative_title) for negative_title in negative_titles]\n",
    "\n",
    "            from_dataset = [[from_dataset] * MAX_TITLE_LENGTH] * NUM_FEATURES_PER_WORD\n",
    "            all_features = [from_dataset, target_features, positive_features] + n_negative_features\n",
    "            \n",
    "            features.append(all_features)\n",
    "    \n",
    "    targets = torch.LongTensor(len(features)).zero_()\n",
    "    dataset = data_utils.TensorDataset(torch.FloatTensor(np.array(features)), targets)\n",
    "    data_loader = data_utils.DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = shuffle_data)\n",
    "    return data_loader\n",
    "\n",
    "def create_data_loader_test(android_samples, shuffle_data = True):\n",
    "    features = []\n",
    "\n",
    "    for sample in android_samples:\n",
    "        target_title = sample[0]\n",
    "        positive_title = sample[1]\n",
    "        negative_titles = sample[2:22] #:22 because there are more negatives in the android dataset, but a TA\n",
    "                                       # said the extras aren't necessary\n",
    "\n",
    "        target_features = title_to_feature_matrix(target_title)\n",
    "        positive_features = title_to_feature_matrix(positive_title)\n",
    "        n_negative_features = [title_to_feature_matrix(negative_title) for negative_title in negative_titles]\n",
    "\n",
    "        all_features = [target_features, positive_features] + n_negative_features\n",
    "\n",
    "        features.append(all_features)\n",
    "        \n",
    "    return features\n",
    "\n",
    "def sample_and_label_to_cosine_sims(sample, label, model = None):\n",
    "    #RE-ORDER DIMENSIONS OF THE SAMPLE\n",
    "    sample = sample.permute(1, 0, 2, 3)\n",
    "    if model != None:\n",
    "        net = model\n",
    "    \n",
    "    target_question_features = sample[0] # 50 x 200 x 38\n",
    "    positive_question_features = sample[1] # 50 x 200 x 38\n",
    "    N_negative_question_features = sample[2:] #20 x 50 x 200 x 38\n",
    "\n",
    "    #Determine lengths to know how many vectors to take the average across.\n",
    "    target_question_lengths = find_start_of_padding_for_batch(target_question_features.data)\n",
    "    positive_question_lengths = find_start_of_padding_for_batch(positive_question_features.data)\n",
    "    N_negative_questions_lengths = [find_start_of_padding_for_batch(negative.data) for negative in N_negative_question_features]\n",
    "\n",
    "    #RUN THROUGH NET\n",
    "    target_question_net_output = net(target_question_features)\n",
    "    positive_question_net_output = net(positive_question_features)\n",
    "    N_negative_question_net_outputs = [net(negative) for negative in N_negative_question_features]\n",
    "\n",
    "    #CREATE MASKS\n",
    "    target_questions_masks = [create_mask(_) for _ in target_question_lengths] #DIM = 50 x 100 x 38\n",
    "    positive_questions_masks = [create_mask(_) for _ in positive_question_lengths] #DIM = 50 x 100 x 38\n",
    "    N_negative_questions_masks = [[create_mask(length) for length in length_list] #DIM = 50 x 20 x 100 x 38\n",
    "                                  for length_list in N_negative_questions_lengths]\n",
    "\n",
    "    #APPLY MASKS\n",
    "    #Should the multiplicands, the masks, be Float Tensors or Variables? May have to be float tensors to ensure\n",
    "    #    pytorch's directed graph back-prop is maintained.\n",
    "\n",
    "    target_question_net_output_masked = target_question_net_output * Variable(torch.FloatTensor(target_questions_masks))\n",
    "    positive_question_net_output_masked = positive_question_net_output * Variable(torch.FloatTensor(positive_questions_masks))\n",
    "    N_negative_questions_net_output_masked = [N_negative_question_net_outputs[idx] * \n",
    "                                              Variable(torch.FloatTensor(N_negative_questions_masks[idx]))\n",
    "                                              for idx in range(NUM_NEGATIVE_SAMPLES)]\n",
    "    #SUM OVER WORDS\n",
    "    target_question_net_output_masked_summed = torch.sum(target_question_net_output_masked, dim = 2) #DIM = 50 x 100\n",
    "    positive_question_net_output_masked_summed = torch.sum(positive_question_net_output_masked, dim = 2) #DIM = 50 x 100\n",
    "    N_negative_questions_net_output_masked_summed = [torch.sum(\n",
    "                                                    N_negative_questions_net_output_masked[idx], dim = 2\n",
    "                                                    )for idx in range(NUM_NEGATIVE_SAMPLES)] #DIM = 20 x 50 x 100\n",
    "\n",
    "    net_outputs_batch = torch.stack([target_question_net_output_masked_summed, positive_question_net_output_masked_summed] + N_negative_questions_net_output_masked_summed)\n",
    "    \n",
    "    cosine_similarity_pos = cos(target_question_net_output_masked_summed, positive_question_net_output_masked_summed)\n",
    "    # ^ DIM = 50\n",
    "    cosine_similarities_neg = [cos(target_question_net_output_masked_summed, N_negative_questions_net_output_masked_summed[idx])\n",
    "                              for idx in range(NUM_NEGATIVE_SAMPLES)]\n",
    "    # ^ DIM = 20 x 50\n",
    "    #\n",
    "\n",
    "    cosine_similarities = torch.stack([cosine_similarity_pos] + cosine_similarities_neg) # DIM = 21 x 50\n",
    "    return cosine_similarities, net_outputs_batch\n",
    "# helper class used for computing information retrieval metrics, including MAP / MRR / and Precision @ x\n",
    "class Evaluation():\n",
    "\n",
    "    def __init__(self,data):\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "\n",
    "    def Precision(self,precision_at):\n",
    "        scores = []\n",
    "        for item in self.data:\n",
    "            temp = item[:precision_at]\n",
    "            if any(val==1 for val in item):\n",
    "                scores.append(sum([1 if val==1 else 0 for val in temp])*1.0 / len(temp) if len(temp) > 0 else 0.0)\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "\n",
    "    def MAP(self):\n",
    "        scores = []\n",
    "        missing_MAP = 0\n",
    "        for item in self.data:\n",
    "            temp = []\n",
    "            count = 0.0\n",
    "            for i,val in enumerate(item):\n",
    "                if val == 1:\n",
    "                    count += 1.0\n",
    "                    temp.append(count/(i+1))\n",
    "            if len(temp) > 0:\n",
    "                scores.append(sum(temp) / len(temp))\n",
    "            else:\n",
    "                missing_MAP += 1\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "\n",
    "    def MRR(self):\n",
    "\n",
    "        scores = []\n",
    "        for item in self.data:\n",
    "            for i,val in enumerate(item):\n",
    "                if val == 1:\n",
    "                    scores.append(1.0/(i+1))\n",
    "                    break\n",
    "\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "def mse_loss(input, target):\n",
    "    return torch.sum((input - target) ** 2) / input.data.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text_tokenized.txt.gz has id \\t title \\t question body\n",
    "ubuntu_text_tokenized_filename = 'askubuntu/text_tokenized.txt.gz'\n",
    "android_text_tokenized_filename = 'Android/corpus.tsv.gz'\n",
    "ubuntu_data_lookup_table = make_lookup_table(ubuntu_text_tokenized_filename)\n",
    "android_data_lookup_table = make_lookup_table(android_text_tokenized_filename)\n",
    "MAX_TITLE_LENGTH, MAX_BODY_LENGTH = find_maximum_title_and_body_length(ubuntu_data_lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------GENERATE EMBEDDINGS-------------------------------------#\n",
    "\n",
    "word_embeddings = 'askubuntu/vector/vectors_pruned.200.txt.gz'\n",
    "f = gzip.open(word_embeddings, 'r')\n",
    "wv_text = [ ]\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    wv_text.append(line.strip())\n",
    "\n",
    "word_to_vec = {}\n",
    "\n",
    "for line in wv_text:\n",
    "    parts = line.split()\n",
    "    word = parts[0]\n",
    "    vector = np.array([float(v) for v in parts[1:]])\n",
    "    word_to_vec[word] = vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "android_dev_pos_filename = \"Android/dev.pos.txt\"\n",
    "android_dev_neg_filename = \"Android/dev.neg.txt\"\n",
    "\n",
    "android_test_pos_filename = \"Android/test.pos.txt\"\n",
    "android_test_neg_filename = \"Android/test.neg.txt\"\n",
    "\n",
    "android_dev_samples = convert_to_samples_android(android_dev_pos_filename, android_dev_neg_filename)\n",
    "android_test_samples = convert_to_samples_android(android_test_pos_filename, android_test_neg_filename)\n",
    "\n",
    "android_dev_titles_only = convert_sampleids_to_titles(android_dev_samples, android_data_lookup_table)\n",
    "android_test_titles_only = convert_sampleids_to_titles(android_test_samples, android_data_lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "words = [word for titles in android_dev_titles_only for title in titles for word in title]\n",
    "transformer = TfidfVectorizer(ngram_range = (1,1), analyzer='word')\n",
    "X_train_counts_word = transformer.fit_transform(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "cos_sims = []\n",
    "for sample in android_test_titles_only:\n",
    "    question_embeddings = []\n",
    "    for question in sample:\n",
    "        question_embedding = transformer.transform(question)\n",
    "        question_embedding = np.sum(question_embedding, axis = 0)\n",
    "        question_embeddings.append(question_embedding)\n",
    "    target_vec = question_embeddings[0]\n",
    "    cos_sims_sample = []\n",
    "    for idx in range(1, len(question_embeddings)):\n",
    "        vec = question_embeddings[idx]\n",
    "        cos = 1. - cosine(target_vec, vec)\n",
    "        cos_sims_sample.append(cos)\n",
    "    cos_sims.append(cos_sims_sample)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "targets = []\n",
    "for cos_sim_list in cos_sims:\n",
    "    for idx, cos_sim in enumerate(cos_sim_list):\n",
    "        outputs.append(cos_sim)\n",
    "        if idx == 0:\n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Value (0.05): 0.414222526082\n"
     ]
    }
   ],
   "source": [
    "# from AUC import AUCMeter\n",
    "auc_evaluator = AUCMeter()\n",
    "correct = 0.\n",
    "total = 0.\n",
    "for idx in range(len(outputs)):\n",
    "    out = outputs[idx]\n",
    "    targ = targets[idx]\n",
    "    auc_evaluator.add(out, targ)\n",
    "auc_evaluator.scores = np.array(outputs)\n",
    "auc_evaluator.targets = np.array(targets)\n",
    "print \"AUC Value (0.05): {}\".format(auc_evaluator.value(0.05))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Code took from PyTorchNet (https://github.com/pytorch/tnt)\n",
    "\n",
    "'''\n",
    "\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Meter(object):\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def add(self):\n",
    "        pass\n",
    "\n",
    "    def value(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AUCMeter(Meter):\n",
    "    \"\"\"\n",
    "    The AUCMeter measures the area under the receiver-operating characteristic\n",
    "    (ROC) curve for binary classification problems. The area under the curve (AUC)\n",
    "    can be interpreted as the probability that, given a randomly selected positive\n",
    "    example and a randomly selected negative example, the positive example is\n",
    "    assigned a higher score by the classification model than the negative example.\n",
    "\n",
    "    The AUCMeter is designed to operate on one-dimensional Tensors `output`\n",
    "    and `target`, where (1) the `output` contains model output scores that ought to\n",
    "    be higher when the model is more convinced that the example should be positively\n",
    "    labeled, and smaller when the model believes the example should be negatively\n",
    "    labeled (for instance, the output of a signoid function); and (2) the `target`\n",
    "    contains only values 0 (for negative examples) and 1 (for positive examples).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AUCMeter, self).__init__()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # self.scores = torch.DoubleTensor(torch.DoubleStorage()).numpy()\n",
    "        # self.targets = torch.LongTensor(torch.LongStorage()).numpy()\n",
    "        self.scores = []\n",
    "        self.targets = []\n",
    "\n",
    "    def add(self, output, target):\n",
    "        if torch.is_tensor(output):\n",
    "            output = output.cpu().squeeze().numpy()\n",
    "        if torch.is_tensor(target):\n",
    "            target = target.cpu().squeeze().numpy()\n",
    "        # elif isinstance(target, numbers.Number):\n",
    "        #     target = np.asarray([target])\n",
    "        # assert np.ndim(output) == 1, \\\n",
    "        #     'wrong output size (1D expected)'\n",
    "        # assert np.ndim(target) == 1, \\\n",
    "        #     'wrong target size (1D expected)'\n",
    "        # assert output.shape[0] == target.shape[0], \\\n",
    "        #     'number of outputs and targets does not match'\n",
    "        # assert np.all(np.add(np.equal(target, 1), np.equal(target, 0))), \\\n",
    "        #     'targets should be binary (0, 1)'\n",
    "\n",
    "        # self.scores = np.append(self.scores, output)\n",
    "        # self.targets = np.append(self.targets, target)\n",
    "        self.sortind = None\n",
    "        self.scores.append(output)\n",
    "        self.scores.append(target)\n",
    "\n",
    "\n",
    "    def value(self, max_fpr=1.0):\n",
    "        # self.scores = torch.FloatTensor(self.scores)\n",
    "        # self.target = torch.LongTensor(self.targets)\n",
    "        \n",
    "        assert max_fpr > 0\n",
    "\n",
    "        # case when number of elements added are 0\n",
    "        if self.scores.shape[0] == 0:\n",
    "            return 0.5\n",
    "\n",
    "        # sorting the arrays\n",
    "        if self.sortind is None:\n",
    "            scores, sortind = torch.sort(torch.from_numpy(self.scores), dim=0, descending=True)\n",
    "            scores = scores.numpy()\n",
    "            self.sortind = sortind.numpy()\n",
    "        else:\n",
    "            scores, sortind = self.scores, self.sortind\n",
    "\n",
    "        # creating the roc curve\n",
    "        tpr = np.zeros(shape=(scores.size + 1), dtype=np.float64)\n",
    "        fpr = np.zeros(shape=(scores.size + 1), dtype=np.float64)\n",
    "\n",
    "        for i in range(1, scores.size + 1):\n",
    "            if self.targets[sortind[i - 1]] == 1:\n",
    "                tpr[i] = tpr[i - 1] + 1\n",
    "                fpr[i] = fpr[i - 1]\n",
    "            else:\n",
    "                tpr[i] = tpr[i - 1]\n",
    "                fpr[i] = fpr[i - 1] + 1\n",
    "\n",
    "        tpr /= (self.targets.sum() * 1.0)\n",
    "        fpr /= ((self.targets - 1.0).sum() * -1.0)\n",
    "\n",
    "        for n in range(1, scores.size + 1):\n",
    "            if fpr[n] >= max_fpr:\n",
    "                break\n",
    "\n",
    "        # calculating area under curve using trapezoidal rule\n",
    "        #n = tpr.shape[0]\n",
    "        h = fpr[1:n] - fpr[0:n - 1]\n",
    "        sum_h = np.zeros(fpr.shape)\n",
    "        sum_h[0:n - 1] = h\n",
    "        sum_h[1:n] += h\n",
    "        area = (sum_h * tpr).sum() / 2.0\n",
    "\n",
    "        return area / max_fpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
