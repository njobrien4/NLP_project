{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import gzip\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import linalg as LA\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "\n",
    "#torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------HELPER FUNCTIONS-------------------------------------#\n",
    "NUM_NEGATIVE_SAMPLES=20\n",
    "\n",
    "def N_random_values_in_list(full_list, N):\n",
    "    x=0\n",
    "    lower_bound  = 0\n",
    "    upper_bound = len(full_list)-1\n",
    "    sample_list=[]\n",
    "    random_nums=[]\n",
    "    while x < min(N,len(full_list)):\n",
    "        random_num = randint(lower_bound, upper_bound) # inclusive range\n",
    "        if random_num in random_nums:\n",
    "            continue\n",
    "        else:\n",
    "            random_nums.append(random_num)\n",
    "            x += 1\n",
    "    return [full_list[i] for i in random_nums]\n",
    "\n",
    "def convert_to_list(filename):\n",
    "    if filename.endswith('gz'):\n",
    "        with gzip.open(filename,'r')as f:\n",
    "            text_tokens = f.readlines()\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            text_tokens = f.readlines()\n",
    "    text_tokens = [token.replace('\\n','').split('\\t') for token in text_tokens]\n",
    "    text_tokens = [[token[0], token[1].split(' '), token[2].split(' ')] for token in text_tokens]\n",
    "                   \n",
    "    return text_tokens\n",
    "\n",
    "#Sample:question_id, similar_question_id, negative_question_id\n",
    "def convert_to_samples(filename):\n",
    "    my_list=convert_to_list(filename)\n",
    "    new_samples=[]\n",
    "    for original_sample in my_list:\n",
    "        for similar in original_sample[1]:\n",
    "            random_negative_samples = N_random_values_in_list(original_sample[2],NUM_NEGATIVE_SAMPLES)\n",
    "            new_samples.append([original_sample[0], similar, random_negative_samples])# change this to include all negative \n",
    "                                                                                     # examples later\n",
    "    return new_samples\n",
    "def make_lookup_table_for_training_data(filename):\n",
    "    lookup={}\n",
    "    text_token_list=convert_to_list(filename)\n",
    "    for token in text_token_list:\n",
    "        lookup[token[0]] = {'title':token[1],'question':token[2]}\n",
    "    return lookup\n",
    "        \n",
    "#takes  sample_ids of [[q1,p1,n1],[q2,p2,n2]....]\n",
    "#outputs titles like [[q1_title, p1_title, n1_title],[q2_title,p2_title,n2_title]...]\n",
    "def convert_sampleids_to_titles(sample_ids,lookup, is_dev_or_test = False):\n",
    "    #each sample_id [question_id, pos_id, [neg_ids]]\n",
    "    #print type(sample_ids)==list, \"first\"\n",
    "   \n",
    "    titles = []\n",
    "   # print \"sample_ids\", sample_ids\n",
    "    for sample_id in sample_ids:\n",
    "        if is_dev_or_test:\n",
    "            #omit similar questions, only needed for evaluation\n",
    "            sample_id=[sample_id[0]]+sample_id[2][:]\n",
    "         #flatten list: [question_id, pos_id, [neg_ids]] --> [question_id, pos_id, neg_id1, neg_id2, ...]\n",
    "        else:\n",
    "            sample_id= sample_id[:2]+sample_id[2][:]\n",
    "        #sample_id : question_id, similar_question_id, negative_question_id\n",
    "        #try:\n",
    "        current=[]\n",
    "        for identity in sample_id:\n",
    "            try:\n",
    "                current.append(lookup[str(identity)]['title'])\n",
    "            except:\n",
    "                print identity\n",
    "        titles.append(current)\n",
    "           # print type(sample_id)==list\n",
    "        #except:\n",
    "        #    print sample_id, \"is sample id\", type(sample_id)==list\n",
    "    return titles\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "def extract_features(word):\n",
    "    try:\n",
    "        word=remove_non_ascii(word)\n",
    "        word=word.encode('utf-8')\n",
    "    except:\n",
    "        print(word)\n",
    "    return word_to_vec.get(word,None)\n",
    "\n",
    "def find_maximum_title_and_body_length(lookup_table):\n",
    "    max_len_title = -1\n",
    "    max_len_question = -1\n",
    "    max_len_question_id = 0\n",
    "    for key, dict_val in lookup_table.iteritems():\n",
    "        len_title = len(dict_val['title'])\n",
    "        len_question = len(dict_val['question'])\n",
    "        if len_title > max_len_title:\n",
    "             max_len_title = len_title\n",
    "        if len_question > max_len_question:\n",
    "            max_len_question = len_question\n",
    "            max_len_question_id = key\n",
    "    return max_len_title, max_len_question\n",
    "\n",
    "def title_to_feature_matrix(title_word_list):\n",
    "    feature_matrix = []\n",
    "    total_count_of_embeds=0\n",
    "    for idx, word in enumerate(title_word_list):\n",
    "        if idx == PARAMETER_MAX_TITLE_LENGTH:\n",
    "            break\n",
    "        else:\n",
    "            word_features = extract_features(word)\n",
    "            if word_features is not None:\n",
    "                feature_matrix.append(word_features)\n",
    "                total_count_of_embeds+=1\n",
    "        \n",
    "    #Pad the feature with zeros to ensure all inputs to the net have the same dimension\n",
    "    feature_matrix += [[0] * NUM_FEATURES_PER_WORD] * (PARAMETER_MAX_TITLE_LENGTH - total_count_of_embeds)\n",
    "   # print np.array(feature_matrix).T.shape\n",
    "    \n",
    "    return np.array(feature_matrix).T\n",
    "\n",
    "\n",
    "\n",
    "#array is structured like a batch of features 50x200x38\n",
    "def find_start_of_padding_for_batch(batch):\n",
    "    vec_lengths_in_batch = []\n",
    "    for batch_num in range(0, len(batch)):\n",
    "        single_vec = batch[batch_num]\n",
    "        length = find_start_of_padding_single_vec(single_vec) + 1\n",
    "        vec_lengths_in_batch.append(length)\n",
    "    return vec_lengths_in_batch\n",
    "\n",
    "#batch = 200x38\n",
    "def find_start_of_padding_single_vec(single_vec):\n",
    "    for idx in range(len(single_vec[0])-1, -1, -1):\n",
    "        if single_vec[0][idx] != 0.:\n",
    "            return idx\n",
    "    #if the whole sequence is 0s\n",
    "    return 0\n",
    "def create_mask(word_length):\n",
    "    return np.array([[1. / word_length] * HIDDEN_SIZE] * word_length + [[0] * HIDDEN_SIZE] * (MAX_TITLE_LENGTH - word_length)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------LOAD DATA-------------------------------------#\n",
    "\n",
    "word_embeddings = 'askubuntu/vector/vectors_pruned.200.txt.gz'\n",
    "f = gzip.open(word_embeddings, 'r')\n",
    "wv_text = [ ]\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    wv_text.append(line.strip())\n",
    "\n",
    "word_to_vec = {}\n",
    "\n",
    "for line in wv_text:\n",
    "    parts = line.split()\n",
    "    word = parts[0]\n",
    "    vector = np.array([float(v) for v in parts[1:]])\n",
    "    word_to_vec[word] = vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#text_tokenized.txt.gz has id \\t title \\t question body\n",
    "text_tokenized='askubuntu/text_tokenized.txt.gz'\n",
    "\n",
    "#train_random.txt\n",
    "#(1) the query question ID, (2) the list of similar question IDs, and (3) the list of randomly selected question IDs.\n",
    "train_random_filename='askubuntu/train_random.txt'\n",
    "\n",
    "#Each line contains (1) the query question ID, (2) the list of similar question IDs, (3) the list of 20 candidate question IDs and (4) the associated BM25 scores of these questions computed by the Lucene search engine. The second field (the set of similar questions) is a subset of the third field.\n",
    "dev_filename='askubuntu/dev.txt'\n",
    "test_filename='askubuntu/test.txt'\n",
    "\n",
    "train_samples = convert_to_samples(train_random_filename)\n",
    "#dev_samples = convert_to_samples(dev_filename)\n",
    "#test_samples = convert_to_samples(test_filename)\n",
    "\n",
    "lookup = make_lookup_table_for_training_data(text_tokenized)\n",
    "train_titles_only = convert_sampleids_to_titles(train_samples, lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------NET GLOBAL VARIABLES-------------------------------------#\n",
    "#-------------------------------------GLOBAL VARIABLES-------------------------------------#\n",
    "NUM_TRAINING_EXAMPLES = 22850 #FOR DATA BATCHER, WHEN DEPLOYED SHOULD BE ALL TRAINING EXAMPLES\n",
    "PARAMETER_MAX_TITLE_LENGTH = 38\n",
    "NUM_FEATURES_PER_WORD = 200 #DO NOT CHANGE. FIXED at 200\n",
    "MAX_TITLE_LENGTH, MAX_BODY_LENGTH = find_maximum_title_and_body_length(lookup)\n",
    "KERNEL_SIZE = 3 #MAKE SURE THIS NUMBER IS ODD SO THAT THE PADDING MAKES SENSE\n",
    "PADDING = (KERNEL_SIZE - 1) / 2\n",
    "INPUT_SIZE = 200\n",
    "HIDDEN_SIZE = 150\n",
    "LEARNING_RATE = 1e-5\n",
    "MARGIN = 0.2\n",
    "NUM_EPOCHS = 25\n",
    "BATCH_SIZE = 50\n",
    "NUM_BATCHES = NUM_TRAINING_EXAMPLES/BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made training data loader 0\n",
      "made training data loader 50\n",
      "made training data loader 100\n",
      "made training data loader 150\n",
      "made training data loader 200\n",
      "made training data loader 250\n",
      "made training data loader 300\n",
      "made training data loader 350\n",
      "made training data loader 400\n",
      "made training data loader 450\n",
      "made training data loader 500\n",
      "made training data loader 550\n",
      "made training data loader 600\n",
      "made training data loader 650\n",
      "made training data loader 700\n",
      "made training data loader 750\n",
      "made training data loader 800\n",
      "made training data loader 850\n",
      "made training data loader 900\n",
      "made training data loader 950\n",
      "made training data loader 1000\n",
      "made training data loader 1050\n",
      "made training data loader 1100\n",
      "made training data loader 1150\n",
      "made training data loader 1200\n",
      "made training data loader 1250\n",
      "made training data loader 1300\n",
      "made training data loader 1350\n",
      "made training data loader 1400\n",
      "made training data loader 1450\n",
      "made training data loader 1500\n",
      "made training data loader 1550\n",
      "made training data loader 1600\n",
      "made training data loader 1650\n",
      "made training data loader 1700\n",
      "made training data loader 1750\n",
      "made training data loader 1800\n",
      "made training data loader 1850\n",
      "made training data loader 1900\n",
      "made training data loader 1950\n",
      "made training data loader 2000\n",
      "made training data loader 2050\n",
      "made training data loader 2100\n",
      "made training data loader 2150\n",
      "made training data loader 2200\n",
      "made training data loader 2250\n",
      "made training data loader 2300\n",
      "made training data loader 2350\n",
      "made training data loader 2400\n",
      "made training data loader 2450\n",
      "made training data loader 2500\n",
      "made training data loader 2550\n",
      "made training data loader 2600\n",
      "made training data loader 2650\n",
      "made training data loader 2700\n",
      "made training data loader 2750\n",
      "made training data loader 2800\n",
      "made training data loader 2850\n",
      "made training data loader 2900\n",
      "made training data loader 2950\n",
      "made training data loader 3000\n",
      "made training data loader 3050\n",
      "made training data loader 3100\n",
      "made training data loader 3150\n",
      "made training data loader 3200\n",
      "made training data loader 3250\n",
      "made training data loader 3300\n",
      "made training data loader 3350\n",
      "made training data loader 3400\n",
      "made training data loader 3450\n",
      "made training data loader 3500\n",
      "made training data loader 3550\n",
      "made training data loader 3600\n",
      "made training data loader 3650\n",
      "made training data loader 3700\n",
      "made training data loader 3750\n",
      "made training data loader 3800\n",
      "made training data loader 3850\n",
      "made training data loader 3900\n",
      "made training data loader 3950\n",
      "made training data loader 4000\n",
      "made training data loader 4050\n",
      "made training data loader 4100\n",
      "made training data loader 4150\n",
      "made training data loader 4200\n",
      "made training data loader 4250\n",
      "made training data loader 4300\n",
      "made training data loader 4350\n",
      "made training data loader 4400\n",
      "made training data loader 4450\n",
      "made training data loader 4500\n",
      "made training data loader 4550\n",
      "made training data loader 4600\n",
      "made training data loader 4650\n",
      "made training data loader 4700\n",
      "made training data loader 4750\n",
      "made training data loader 4800\n",
      "made training data loader 4850\n",
      "made training data loader 4900\n",
      "made training data loader 4950\n",
      "made training data loader 5000\n",
      "made training data loader 5050\n",
      "made training data loader 5100\n",
      "made training data loader 5150\n",
      "made training data loader 5200\n",
      "made training data loader 5250\n",
      "made training data loader 5300\n",
      "made training data loader 5350\n",
      "made training data loader 5400\n",
      "made training data loader 5450\n",
      "made training data loader 5500\n",
      "made training data loader 5550\n",
      "made training data loader 5600\n",
      "made training data loader 5650\n",
      "made training data loader 5700\n",
      "made training data loader 5750\n",
      "made training data loader 5800\n",
      "made training data loader 5850\n",
      "made training data loader 5900\n",
      "made training data loader 5950\n",
      "made training data loader 6000\n",
      "made training data loader 6050\n",
      "made training data loader 6100\n",
      "made training data loader 6150\n",
      "made training data loader 6200\n",
      "made training data loader 6250\n",
      "made training data loader 6300\n",
      "made training data loader 6350\n",
      "made training data loader 6400\n",
      "made training data loader 6450\n",
      "made training data loader 6500\n",
      "made training data loader 6550\n",
      "made training data loader 6600\n",
      "made training data loader 6650\n",
      "made training data loader 6700\n",
      "made training data loader 6750\n",
      "made training data loader 6800\n",
      "made training data loader 6850\n",
      "made training data loader 6900\n",
      "made training data loader 6950\n",
      "made training data loader 7000\n",
      "made training data loader 7050\n",
      "made training data loader 7100\n",
      "made training data loader 7150\n",
      "made training data loader 7200\n",
      "made training data loader 7250\n",
      "made training data loader 7300\n",
      "made training data loader 7350\n",
      "made training data loader 7400\n",
      "made training data loader 7450\n",
      "made training data loader 7500\n",
      "made training data loader 7550\n",
      "made training data loader 7600\n",
      "made training data loader 7650\n",
      "made training data loader 7700\n",
      "made training data loader 7750\n",
      "made training data loader 7800\n",
      "made training data loader 7850\n",
      "made training data loader 7900\n",
      "made training data loader 7950\n",
      "made training data loader 8000\n",
      "made training data loader 8050\n",
      "made training data loader 8100\n",
      "made training data loader 8150\n",
      "made training data loader 8200\n",
      "made training data loader 8250\n",
      "made training data loader 8300\n",
      "made training data loader 8350\n",
      "made training data loader 8400\n",
      "made training data loader 8450\n",
      "made training data loader 8500\n",
      "made training data loader 8550\n",
      "made training data loader 8600\n",
      "made training data loader 8650\n",
      "made training data loader 8700\n",
      "made training data loader 8750\n",
      "made training data loader 8800\n",
      "made training data loader 8850\n",
      "made training data loader 8900\n",
      "made training data loader 8950\n",
      "made training data loader 9000\n",
      "made training data loader 9050\n",
      "made training data loader 9100\n",
      "made training data loader 9150\n",
      "made training data loader 9200\n",
      "made training data loader 9250\n",
      "made training data loader 9300\n",
      "made training data loader 9350\n",
      "made training data loader 9400\n",
      "made training data loader 9450\n",
      "made training data loader 9500\n",
      "made training data loader 9550\n",
      "made training data loader 9600\n",
      "made training data loader 9650\n",
      "made training data loader 9700\n",
      "made training data loader 9750\n",
      "made training data loader 9800\n",
      "made training data loader 9850\n",
      "made training data loader 9900\n",
      "made training data loader 9950\n",
      "made training data loader 10000\n",
      "made training data loader 10050\n",
      "made training data loader 10100\n",
      "made training data loader 10150\n",
      "made training data loader 10200\n",
      "made training data loader 10250\n",
      "made training data loader 10300\n",
      "made training data loader 10350\n",
      "made training data loader 10400\n",
      "made training data loader 10450\n",
      "made training data loader 10500\n",
      "made training data loader 10550\n",
      "made training data loader 10600\n",
      "made training data loader 10650\n",
      "made training data loader 10700\n",
      "made training data loader 10750\n",
      "made training data loader 10800\n",
      "made training data loader 10850\n",
      "made training data loader 10900\n",
      "made training data loader 10950\n",
      "made training data loader 11000\n",
      "made training data loader 11050\n",
      "made training data loader 11100\n",
      "made training data loader 11150\n",
      "made training data loader 11200\n",
      "made training data loader 11250\n",
      "made training data loader 11300\n",
      "made training data loader 11350\n",
      "made training data loader 11400\n",
      "made training data loader 11450\n",
      "made training data loader 11500\n",
      "made training data loader 11550\n",
      "made training data loader 11600\n",
      "made training data loader 11650\n",
      "made training data loader 11700\n",
      "made training data loader 11750\n",
      "made training data loader 11800\n",
      "made training data loader 11850\n",
      "made training data loader 11900\n",
      "made training data loader 11950\n",
      "made training data loader 12000\n",
      "made training data loader 12050\n",
      "made training data loader 12100\n",
      "made training data loader 12150\n",
      "made training data loader 12200\n",
      "made training data loader 12250\n",
      "made training data loader 12300\n",
      "made training data loader 12350\n",
      "made training data loader 12400\n",
      "made training data loader 12450\n",
      "made training data loader 12500\n",
      "made training data loader 12550\n",
      "made training data loader 12600\n",
      "made training data loader 12650\n",
      "made training data loader 12700\n",
      "made training data loader 12750\n",
      "made training data loader 12800\n",
      "made training data loader 12850\n",
      "made training data loader 12900\n",
      "made training data loader 12950\n",
      "made training data loader 13000\n",
      "made training data loader 13050\n",
      "made training data loader 13100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made training data loader 13150\n",
      "made training data loader 13200\n",
      "made training data loader 13250\n",
      "made training data loader 13300\n",
      "made training data loader 13350\n",
      "made training data loader 13400\n",
      "made training data loader 13450\n",
      "made training data loader 13500\n",
      "made training data loader 13550\n",
      "made training data loader 13600\n",
      "made training data loader 13650\n",
      "made training data loader 13700\n",
      "made training data loader 13750\n",
      "made training data loader 13800\n",
      "made training data loader 13850\n",
      "made training data loader 13900\n",
      "made training data loader 13950\n",
      "made training data loader 14000\n",
      "made training data loader 14050\n",
      "made training data loader 14100\n",
      "made training data loader 14150\n",
      "made training data loader 14200\n",
      "made training data loader 14250\n",
      "made training data loader 14300\n",
      "made training data loader 14350\n",
      "made training data loader 14400\n",
      "made training data loader 14450\n",
      "made training data loader 14500\n",
      "made training data loader 14550\n",
      "made training data loader 14600\n",
      "made training data loader 14650\n",
      "made training data loader 14700\n",
      "made training data loader 14750\n",
      "made training data loader 14800\n",
      "made training data loader 14850\n",
      "made training data loader 14900\n",
      "made training data loader 14950\n",
      "made training data loader 15000\n",
      "made training data loader 15050\n",
      "made training data loader 15100\n",
      "made training data loader 15150\n",
      "made training data loader 15200\n",
      "made training data loader 15250\n",
      "made training data loader 15300\n",
      "made training data loader 15350\n",
      "made training data loader 15400\n",
      "made training data loader 15450\n",
      "made training data loader 15500\n",
      "made training data loader 15550\n",
      "made training data loader 15600\n",
      "made training data loader 15650\n",
      "made training data loader 15700\n",
      "made training data loader 15750\n",
      "made training data loader 15800\n",
      "made training data loader 15850\n",
      "made training data loader 15900\n",
      "made training data loader 15950\n",
      "made training data loader 16000\n",
      "made training data loader 16050\n",
      "made training data loader 16100\n",
      "made training data loader 16150\n",
      "made training data loader 16200\n",
      "made training data loader 16250\n",
      "made training data loader 16300\n",
      "made training data loader 16350\n",
      "made training data loader 16400\n",
      "made training data loader 16450\n",
      "made training data loader 16500\n",
      "made training data loader 16550\n",
      "made training data loader 16600\n",
      "made training data loader 16650\n",
      "made training data loader 16700\n",
      "made training data loader 16750\n",
      "made training data loader 16800\n",
      "made training data loader 16850\n",
      "made training data loader 16900\n",
      "made training data loader 16950\n",
      "made training data loader 17000\n",
      "made training data loader 17050\n",
      "made training data loader 17100\n",
      "made training data loader 17150\n",
      "made training data loader 17200\n",
      "made training data loader 17250\n",
      "made training data loader 17300\n",
      "made training data loader 17350\n",
      "made training data loader 17400\n",
      "made training data loader 17450\n",
      "made training data loader 17500\n",
      "made training data loader 17550\n",
      "made training data loader 17600\n",
      "made training data loader 17650\n",
      "made training data loader 17700\n",
      "made training data loader 17750\n",
      "made training data loader 17800\n",
      "made training data loader 17850\n",
      "made training data loader 17900\n",
      "made training data loader 17950\n",
      "made training data loader 18000\n",
      "made training data loader 18050\n",
      "made training data loader 18100\n",
      "made training data loader 18150\n",
      "made training data loader 18200\n",
      "made training data loader 18250\n",
      "made training data loader 18300\n",
      "made training data loader 18350\n",
      "made training data loader 18400\n",
      "made training data loader 18450\n",
      "made training data loader 18500\n",
      "made training data loader 18550\n",
      "made training data loader 18600\n",
      "made training data loader 18650\n",
      "made training data loader 18700\n",
      "made training data loader 18750\n",
      "made training data loader 18800\n",
      "made training data loader 18850\n",
      "made training data loader 18900\n",
      "made training data loader 18950\n",
      "made training data loader 19000\n",
      "made training data loader 19050\n",
      "made training data loader 19100\n",
      "made training data loader 19150\n",
      "made training data loader 19200\n",
      "made training data loader 19250\n",
      "made training data loader 19300\n",
      "made training data loader 19350\n",
      "made training data loader 19400\n",
      "made training data loader 19450\n",
      "made training data loader 19500\n",
      "made training data loader 19550\n",
      "made training data loader 19600\n",
      "made training data loader 19650\n",
      "made training data loader 19700\n",
      "made training data loader 19750\n",
      "made training data loader 19800\n",
      "made training data loader 19850\n",
      "made training data loader 19900\n",
      "made training data loader 19950\n",
      "made training data loader 20000\n",
      "made training data loader 20050\n",
      "made training data loader 20100\n",
      "made training data loader 20150\n",
      "made training data loader 20200\n",
      "made training data loader 20250\n",
      "made training data loader 20300\n",
      "made training data loader 20350\n",
      "made training data loader 20400\n",
      "made training data loader 20450\n",
      "made training data loader 20500\n",
      "made training data loader 20550\n",
      "made training data loader 20600\n",
      "made training data loader 20650\n",
      "made training data loader 20700\n",
      "made training data loader 20750\n",
      "made training data loader 20800\n",
      "made training data loader 20850\n",
      "made training data loader 20900\n",
      "made training data loader 20950\n",
      "made training data loader 21000\n",
      "made training data loader 21050\n",
      "made training data loader 21100\n",
      "made training data loader 21150\n",
      "made training data loader 21200\n",
      "made training data loader 21250\n",
      "made training data loader 21300\n",
      "made training data loader 21350\n",
      "made training data loader 21400\n",
      "made training data loader 21450\n",
      "made training data loader 21500\n",
      "made training data loader 21550\n",
      "made training data loader 21600\n",
      "made training data loader 21650\n",
      "made training data loader 21700\n",
      "made training data loader 21750\n",
      "made training data loader 21800\n",
      "made training data loader 21850\n",
      "made training data loader 21900\n",
      "made training data loader 21950\n",
      "made training data loader 22000\n",
      "made training data loader 22050\n",
      "made training data loader 22100\n",
      "made training data loader 22150\n",
      "made training data loader 22200\n",
      "made training data loader 22250\n",
      "made training data loader 22300\n",
      "made training data loader 22350\n",
      "made training data loader 22400\n",
      "made training data loader 22450\n",
      "made training data loader 22500\n",
      "made training data loader 22550\n",
      "made training data loader 22600\n",
      "made training data loader 22650\n",
      "made training data loader 22700\n",
      "made training data loader 22750\n",
      "made training data loader 22800\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------CREATE DATA BATCHER-------------------------------------#\n",
    "#for each tuple of titles make a feature vector that is num_titles x 200 x 38\n",
    "# where num_titles = 1 (target) + 1 (positive) + n (negative) \n",
    "\n",
    "\n",
    "def create_dataset(samples, shuffle_data = True, is_test_or_dev = False):\n",
    "    features = []\n",
    "    for sample in samples:\n",
    "        target_title = sample[0]\n",
    "        positive_title = sample[1]\n",
    "        negative_titles = sample[2:]\n",
    "\n",
    "        target_features = title_to_feature_matrix(target_title)\n",
    "        positive_features = title_to_feature_matrix(positive_title)\n",
    "        if len(negative_titles)==0:\n",
    "            all_features=[target_features, positive_features]\n",
    "        else:\n",
    "            n_negative_features = [title_to_feature_matrix(negative_title) for negative_title in negative_titles]\n",
    "            all_features = [target_features, positive_features] + n_negative_features\n",
    "        features.append(all_features)            \n",
    "    targets = torch.LongTensor(len(features)).zero_()\n",
    "    dataset = data_utils.TensorDataset(torch.FloatTensor(features), targets)\n",
    "    return dataset\n",
    "\n",
    "train_data_loaders=[]\n",
    "for i in range(0,NUM_TRAINING_EXAMPLES,50):\n",
    "    train_data_loader = create_dataset(train_titles_only[i:i+50])\n",
    "    print \"made training data loader\", i\n",
    "    train_data_loaders.append(train_data_loader)\n",
    "final_train_dataset=data_utils.ConcatDataset(train_data_loaders)\n",
    "final_train_data_loader=data_utils.DataLoader(final_train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "# features = []\n",
    "# for i in range(len(train_titles_only[:NUM_TRAINING_EXAMPLES])):   # we should include all but this is just for simplicity \n",
    "#     if i%1000 == 0:\n",
    "#         print i\n",
    "\n",
    "#     sample = train_titles_only[i]\n",
    "    \n",
    "#     target_title = sample[0]\n",
    "#     positive_title = sample[1]\n",
    "#     negative_titles = sample[2:]\n",
    "    \n",
    "# #     print \"\\n\"\n",
    "# #     print \"Target title: {}\".format(\" \".join(target_title))\n",
    "# #     print \"Positive title: {}\".format(\" \".join(positive_title))\n",
    "# #     for negative in negative_titles:\n",
    "# #         print \"Negative title: {}\".format(\" \".join(negative))\n",
    "# #     print \"\\n\"\n",
    "# #     a\n",
    "# #     if i > 10:\n",
    "# #         a\n",
    "# #     print \"target_title: {}\".format(target_title)\n",
    "# #     print \"positive title: {}\".format(positive_title)\n",
    "# #     print \"negative titles: {}\".format(negative_titles)\n",
    "    \n",
    "#     target_features = title_to_feature_matrix(target_title)\n",
    "#     positive_features = title_to_feature_matrix(positive_title)\n",
    "#     n_negative_features = [title_to_feature_matrix(negative_title) for negative_title in negative_titles]\n",
    "    \n",
    "# #     print \"Target features shape: {}\".format(target_features.shape)\n",
    "# #     print \"Positive features shape: {}\".format(positive_features.shape)\n",
    "# #     print \"Negative features[0] shape: {}\".format(n_negative_features[0].shape)\n",
    "# #     print \"Num negative features: {}\".format(len(n_negative_features)) \n",
    "#     all_features = [target_features, positive_features] + n_negative_features\n",
    "#     features.append(all_features)\n",
    "\n",
    "\n",
    "# print \"Done Loop\"\n",
    "# features = np.array(features) \n",
    "# targets = torch.LongTensor(len(features), 1).zero_()\n",
    "# training_dataset = data_utils.TensorDataset(torch.FloatTensor(features), targets)\n",
    "# train_loader = data_utils.DataLoader(training_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "# print \"Succesfully made the data batcher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3,3)\n",
    "inputs = [autograd.Variable(torch.randn(1, 3))\n",
    "          for _ in range(5)]  # make a sequence of length 5\n",
    "hidden = (\n",
    "          autograd.Variable(torch.randn(1, 1, 3)),\n",
    "          autograd.Variable(torch.randn(1, 1, 3))\n",
    "         )\n",
    "for i in inputs:\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(BATCH_SIZE, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(BATCH_SIZE, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        lstm_out, self.hidden = self.lstm(sequence, self.hidden)\n",
    "        return self.hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper class used for computing information retrieval metrics, including MAP / MRR / and Precision @ x\n",
    "\n",
    "\n",
    "class Evaluation():\n",
    "\n",
    "    def __init__(self,data):\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "\n",
    "    def Precision(self,precision_at):\n",
    "        scores = []\n",
    "        for item in self.data:\n",
    "            temp = item[:precision_at]\n",
    "            if any(val==1 for val in item):\n",
    "                scores.append(sum([1 if val==1 else 0 for val in temp])*1.0 / len(temp) if len(temp) > 0 else 0.0)\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "\n",
    "    def MAP(self):\n",
    "        scores = []\n",
    "        missing_MAP = 0\n",
    "        for item in self.data:\n",
    "            temp = []\n",
    "            count = 0.0\n",
    "            for i,val in enumerate(item):\n",
    "                if val == 1:\n",
    "                    count += 1.0\n",
    "                    temp.append(count/(i+1))\n",
    "            if len(temp) > 0:\n",
    "                scores.append(sum(temp) / len(temp))\n",
    "            else:\n",
    "                missing_MAP += 1\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "\n",
    "    def MRR(self):\n",
    "\n",
    "        scores = []\n",
    "        for item in self.data:\n",
    "            for i,val in enumerate(item):\n",
    "                if val == 1:\n",
    "                    scores.append(1.0/(i+1))\n",
    "                    break\n",
    "\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "\n",
    "def evaluate_model_on_test(model, test_data_filename):\n",
    "    test_data_loader =convert_to_list(test_data_filename)\n",
    "    \n",
    "    model.eval() #change model to 'eval' mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    p5 = 0\n",
    "    MRR = 0\n",
    "    all_data = []\n",
    "    for sample in test_data_loader:\n",
    "        #print sample\n",
    "        target_id=sample[0]\n",
    "        similar_ids=sample[1]\n",
    "        #print similar_ids, 'similar'\n",
    "        candidate_ids=sample[2]\n",
    "        #print candidate_ids, 'candidates'\n",
    "        titles = convert_sampleids_to_titles([sample], lookup,is_dev_or_test=True)[0]\n",
    "\n",
    "        #print titles\n",
    "        twenty_one_features = [title_to_feature_matrix(title) for title in titles]\n",
    "        \n",
    "        cosine_sims = sample_to_cosine_sims(twenty_one_features, model = model)\n",
    "        #print cosine_sims\n",
    "        #_, predicted = torch.max(cosine_sims, 0)\n",
    "       # total += label.size(0)\n",
    "       # label = label.squeeze()\n",
    "        \n",
    "        #correct += ((predicted == label).sum().data.numpy().squeeze())\n",
    "        cosine_sims = [cos_sim.data.numpy()[0] for cos_sim in cosine_sims]\n",
    "        #cosine_sims = cosine_sims.data.numpy().T\n",
    "        #print cosine_sims, \"is cos sims\"\n",
    "        #print candidate_ids, 'is candidates'\n",
    "        gold_labels = [1 if identity in similar_ids else 0 for identity in candidate_ids]\n",
    "        #print gold_labels, \"is gold\"\n",
    "        \n",
    "        index_order = sorted(range(len(cosine_sims)), reverse = True,key=lambda k: cosine_sims[k])\n",
    "        #print index_order, \"is index order\"\n",
    "        \n",
    "        data = [gold_labels[index] for index in index_order]\n",
    "        all_data.append(data)\n",
    "#         for instance_idx in range(len(cosine_sims)):\n",
    "#             cosine_sims_at_idx = cosine_sims[instance_idx]            \n",
    "#             print \"Cosine sims at idx: {}\".format(cosine_sims_at_idx)\n",
    "#             a\n",
    "#             loc = np.squeeze(np.where(cosine_sims_at_idx.argsort()[::-1] == 0))\n",
    "#             data_instance = [0] * len(cosine_sims_at_idx)\n",
    "#             data_instance[loc] = 1\n",
    "#             data.append(data_instance)\n",
    "        \n",
    "#         for instance_idx in range(len(cosine_sims)):\n",
    "#             cosine_sims_at_idx = cosine_sims[instance_idx]\n",
    "#             top_5_indices = cosine_sims_at_idx.argsort()[-5:][::-1]\n",
    "#             label_at_idx = 0\n",
    "#             if label_at_idx in top_5_indices:\n",
    "#                 p5 += 1\n",
    "            \n",
    "#             MRR += 1. / (1. + np.squeeze(np.where(cosine_sims_at_idx.argsort()[::-1] == 0)))\n",
    "    return all_data \n",
    "\n",
    "def sample_to_cosine_sims(sample, model = None):\n",
    "    #h_0 (num_layers * num_directions, batch, hidden_size): \n",
    "    #tensor containing the initial hidden state for each element in the batch.\n",
    "    h0=autograd.Variable(torch.zeros(1, BATCH_SIZE,HIDDEN_DIM))\n",
    "\n",
    "    #c_0 (num_layers * num_directions, batch, hidden_size): \n",
    "    #tensor containing the initial cell state for each element in the batch.\n",
    "    c0=autograd.Variable(torch.zeros(1,BATCH_SIZE,HIDDEN_DIM))\n",
    "    #RE-ORDER DIMENSIONS OF THE SAMPLE\n",
    "    sample = Variable(torch.FloatTensor(sample))\n",
    "    #print sample.unsqueeze(0)\n",
    "    # 1x21x200x38\n",
    "    sample = sample.unsqueeze(0).permute(1, 0, 2, 3)\n",
    "    if model != None:\n",
    "        lstm = model\n",
    "    #21 x 1 x 200 x 38\n",
    "    target_question_features = sample[0] # 1 x 1 x 200 x 38\n",
    "   # print target_question_features.data.numpy()shape, 'Is target\"'\n",
    "    other_question_features = sample[1:] # 20 x 1 x 200 x 38\n",
    "   # print other_question_features.data.numpy()shape, \"is other\"\n",
    "\n",
    "    #Determine lengths to know how many vectors to take the average across.\n",
    "    target_question_lengths = find_start_of_padding_for_batch(target_question_features.data)\n",
    "    other_questions_lengths = [find_start_of_padding_for_batch(negative.data) for negative in other_question_features]\n",
    "\n",
    "    #RUN THROUGH NET\n",
    "    \n",
    "#     target_question_lstm_outs, target_question_lstm_hiddens = lstm(target_question_features,(h0,c0)) \n",
    "#         positive_question_lstm_outs, positive_question_lstm_hiddens = lstm(positive_question_features,(h0,c0))\n",
    "#        # \n",
    "#         N_negative_question_lstm_output_tuple_list = [lstm(negative,(h0,c0)) for negative in N_negative_question_features]\n",
    "#         N_negative_question_lstm_outs = [negative[0] for negative in N_negative_question_lstm_output_tuple_list]\n",
    "#         N_negative_question_lstm_hiddens = [negative[1] for negative in N_negative_question_lstm_output_tuple_list]\n",
    "        \n",
    "        \n",
    "#         #do permutations to make outputs 50 x 200 x 38\n",
    "#         target_question_lstm_outs=target_question_lstm_outs.permute(1,2,0)\n",
    "#         positive_question_lstm_outs=positive_question_lstm_outs.permute(1,2,0)\n",
    "#         N_negative_question_lstm_outs=[negative.permute(1,2,0) for negative in N_negative_question_lstm_outs]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    target_question_lstm_outs, target_question_lstm_hiddens = lstm(target_question_features.permute(2,0,1), (h0,c0))\n",
    "    N_other_question_output_tuples = [lstm(other.permute(2,0,1), (h0,c0)) for other in other_question_features]\n",
    "    N_other_question_lstm_outs=[other[0] for other in N_other_question_output_tuples]\n",
    "    N_other_question_lstm_hiddens=[other[1] for other in N_other_question_output_tuples]\n",
    "    \n",
    "    \n",
    "    target_question_lstm_outs=target_question_lstm_outs.permute(1,2,0)\n",
    "    N_other_question_lstm_outs=[other.permute(1,2,0) for other in N_other_question_lstm_outs]\n",
    "    \n",
    "    #CREATE MASKS\n",
    "    target_questions_masks = [create_mask(_) for _ in target_question_lengths] #DIM = 50 x 100 x 38\n",
    "    other_questions_masks = [[create_mask(length) for length in length_list] #DIM = 50 x 20 x 100 x 38\n",
    "                                  for length_list in other_questions_lengths]\n",
    "\n",
    "    #APPLY MASKS\n",
    "    #Should the multiplicands, the masks, be Float Tensors or Variables? May have to be float tensors to ensure\n",
    "    #    pytorch's directed graph back-prop is maintained.\n",
    "\n",
    "\n",
    "    target_question_net_output_masked = target_question_lstm_outs  * Variable(torch.FloatTensor(target_questions_masks))\n",
    "    other_questions_net_output_masked = [N_other_question_lstm_outs[idx] * \n",
    "                                              Variable(torch.FloatTensor(other_questions_masks[idx]))\n",
    "                                              for idx in range(20)]\n",
    "\n",
    "    #SUM OVER WORDS\n",
    "    target_question_net_output_masked_summed = torch.sum(target_question_net_output_masked, dim = 2) #DIM = 50 x 100\n",
    "    other_questions_net_output_masked_summed = [torch.sum(\n",
    "                                                    other_questions_net_output_masked[idx], dim = 2\n",
    "                                                    )for idx in range(20)] #DIM = 20 x 50 x 100\n",
    "\n",
    "\n",
    "    #cosine_similarity_pos = cos(target_question_net_output_masked_summed, positive_question_net_output_masked_summed)\n",
    "    # ^ DIM = 50\n",
    "    cosine_similarities = [cos(target_question_net_output_masked_summed, other_questions_net_output_masked_summed[idx])\n",
    "                              for idx in range(20)]\n",
    "    \n",
    "    # ^ DIM = 20 x 50\n",
    "    #\n",
    "    \n",
    "\n",
    "    #cosine_similarities = torch.stack([cosine_similarity_pos] + cosine_similarities_neg) # DIM = 21 x 50\n",
    "    return cosine_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_train_data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bceb839ccd49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_train_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#Sample shape: [50, 22, 200, 38]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_train_data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM, HIDDEN_DIM = 200, HIDDEN_SIZE\n",
    "#lstm = LSTM(EMBEDDING_DIM, HIDDEN_DIM)\n",
    "#lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM)\n",
    "lstm = torch.load(\"LSTM_Epoch_6\")\n",
    "\n",
    "LEARNING_RATE=1e-5\n",
    "\n",
    "criterion = nn.MultiMarginLoss(p=1, margin=MARGIN, weight = None, size_average=True) #HAHA just put these in to look smart \n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "#h_0 (num_layers * num_directions, batch, hidden_size): \n",
    "#tensor containing the initial hidden state for each element in the batch.\n",
    "h0=autograd.Variable(torch.zeros(1, BATCH_SIZE,HIDDEN_DIM))\n",
    "\n",
    "#c_0 (num_layers * num_directions, batch, hidden_size): \n",
    "#tensor containing the initial cell state for each element in the batch.\n",
    "c0=autograd.Variable(torch.zeros(1,BATCH_SIZE,HIDDEN_DIM))\n",
    "\n",
    "for epoch in range(0,NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for idx,(sample, label) in enumerate(final_train_data_loader):\n",
    "        print idx, \n",
    "        #Sample shape: [50, 22, 200, 38]\n",
    "        #50 - Batch size, 22 - Num questions per data point\n",
    "        \n",
    "        #CNN : BATCH X WORD EMBEDDINGS X WORD\n",
    "        #LSTM : WORD X BATCH X WORD EMBEDDING\n",
    "        \n",
    "        sample = Variable(sample, requires_grad = True)\n",
    "        label = Variable(label, requires_grad = True)\n",
    "\n",
    "        #RE-ORDER DIMENSIONS OF THE SAMPLE\n",
    "        sample = sample.permute(1, 0, 2, 3)\n",
    "\n",
    "        batch_cos_similarities = []\n",
    "        batch_num = idx\n",
    "        \n",
    "        target_question_features = sample[0] # 50 x 200 x 38\n",
    "        positive_question_features = sample[1] # 50 x 200 x 38\n",
    "        N_negative_question_features = sample[2:] #20 x 50 x 200 x 38\n",
    "        \n",
    "        #Determine lengths to know how many vectors to take the average across.\n",
    "        target_question_lengths = find_start_of_padding_for_batch(target_question_features.data)\n",
    "        positive_question_lengths = find_start_of_padding_for_batch(positive_question_features.data)\n",
    "        N_negative_questions_lengths = [find_start_of_padding_for_batch(negative.data) for negative in N_negative_question_features]\n",
    "        \n",
    "        target_questions_masks = [create_mask(_) for _ in target_question_lengths] #DIM = 50 x 100 x 38\n",
    "        positive_questions_masks = [create_mask(_) for _ in positive_question_lengths] #DIM = 50 x 100 x 38\n",
    "        N_negative_questions_masks = [[create_mask(length) for length in length_list] #DIM = 50 x 20 x 100 x 38\n",
    "                                      for length_list in N_negative_questions_lengths]\n",
    "        \n",
    "        target_question_features=target_question_features.permute(2,0,1)\n",
    "        positive_question_features=positive_question_features.permute(2,0,1)\n",
    "        N_negative_question_features=N_negative_question_features.permute(0,3,1,2)\n",
    "        \n",
    "\n",
    "        # outs are #38x50x200 out, hiddens are 1 x 50 x 200\n",
    "        # each of the 38 outs are the hidden state at time step t (what we want for mean pooling)\n",
    "        # hiddens is just the last hidden state (don't really need)\n",
    "        target_question_lstm_outs, target_question_lstm_hiddens = lstm(target_question_features,(h0,c0)) \n",
    "        positive_question_lstm_outs, positive_question_lstm_hiddens = lstm(positive_question_features,(h0,c0))\n",
    "       # \n",
    "        N_negative_question_lstm_output_tuple_list = [lstm(negative,(h0,c0)) for negative in N_negative_question_features]\n",
    "        N_negative_question_lstm_outs = [negative[0] for negative in N_negative_question_lstm_output_tuple_list]\n",
    "        N_negative_question_lstm_hiddens = [negative[1] for negative in N_negative_question_lstm_output_tuple_list]\n",
    "        \n",
    "        \n",
    "        #do permutations to make outputs 50 x 200 x 38\n",
    "        target_question_lstm_outs=target_question_lstm_outs.permute(1,2,0)\n",
    "        positive_question_lstm_outs=positive_question_lstm_outs.permute(1,2,0)\n",
    "        N_negative_question_lstm_outs=[negative.permute(1,2,0) for negative in N_negative_question_lstm_outs]\n",
    "        \n",
    "        #apply mask\n",
    "        target_question_net_output_masked = target_question_lstm_outs * Variable(torch.FloatTensor(target_questions_masks))\n",
    "        positive_question_net_output_masked = positive_question_lstm_outs * Variable(torch.FloatTensor(positive_questions_masks))\n",
    "        N_negative_questions_net_output_masked = [N_negative_question_lstm_outs[idx] * \n",
    "                                                  Variable(torch.FloatTensor(N_negative_questions_masks[idx]))\n",
    "                                                  for idx in range(NUM_NEGATIVE_SAMPLES)]\n",
    "\n",
    "        #AVG OVER WORDS\n",
    "        target_question_net_output_masked_avged = torch.mean(target_question_net_output_masked, dim = 2) #DIM = 50 x 100\n",
    "        positive_question_net_output_masked_avged = torch.mean(positive_question_net_output_masked, dim = 2) #DIM = 50 x 100\n",
    "        N_negative_questions_net_output_masked_avged = [torch.mean(\n",
    "                                                        N_negative_questions_net_output_masked[idx], dim = 2\n",
    "                                                        )for idx in range(NUM_NEGATIVE_SAMPLES)] #DIM = 20 x 50 x 100\n",
    "        \n",
    "        cosine_similarity_pos = cos(target_question_net_output_masked_avged, positive_question_net_output_masked_avged)\n",
    "        # ^ DIM = 50\n",
    "        cosine_similarities_neg = [cos(target_question_net_output_masked_avged, N_negative_questions_net_output_masked_avged[idx]) for idx in range(NUM_NEGATIVE_SAMPLES)]\n",
    "        \n",
    "        # ^ DIM = 20 x 50\n",
    "        \n",
    "\n",
    "        cosine_similarities = torch.stack([cosine_similarity_pos] + cosine_similarities_neg) # DIM = 21 x 50\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cosine_similarities = torch.t(cosine_similarities)\n",
    "        label = torch.squeeze(label)\n",
    "        loss = criterion(cosine_similarities, label)\n",
    "        \n",
    "        loss.backward()\n",
    "            \n",
    "        running_loss += loss.data[0]\n",
    "            \n",
    "        optimizer.step()\n",
    "       # if batch_num % 10 == 0:\n",
    "            #print \"Epoch: {}, Batch: {}, Loss: {}\".format(epoch, batch_num, loss.data[0])\n",
    "\n",
    "\n",
    "    torch.save(lstm, \"LSTM_Epoch_{}\".format(epoch))\n",
    "    print \"\"\n",
    "    print \"Loss after epoch \" + str(epoch) + \" :\" + str(running_loss)\n",
    "    data = evaluate_model_on_test(lstm, 'askubuntu/dev.txt')\n",
    "    eval = Evaluation(data)\n",
    "    print \"MAP: \",eval.MAP()\n",
    "    print \"MRR: \", eval.MRR()\n",
    "    print \"P@1: \",eval.Precision(1)\n",
    "    print \"P@5: \",eval.Precision(5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f10d12dd5a00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LSTM_Epoch_6\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"ok\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_on_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'askubuntu/dev.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0meval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"MAP: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a2c8fea4d5ee>\u001b[0m in \u001b[0;36mevaluate_model_on_test\u001b[0;34m(model, test_data_filename)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtwenty_one_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtitle_to_feature_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mcosine_sims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_to_cosine_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_one_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m#print cosine_sims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#_, predicted = torch.max(cosine_sims, 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a2c8fea4d5ee>\u001b[0m in \u001b[0;36msample_to_cosine_sims\u001b[0;34m(sample, model)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mtarget_question_lstm_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_question_lstm_hiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_question_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mN_other_question_output_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mother_question_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mN_other_question_lstm_outs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mN_other_question_output_tuples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mN_other_question_lstm_hiddens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mN_other_question_output_tuples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/modules/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mLSTMCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mgates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mingate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36maddmm\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36m_blas\u001b[0;34m(cls, args, inplace)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/autograd/_functions/blas.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[0;32m---> 26\u001b[0;31m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm = torch.load(\"LSTM_Epoch_6\")\n",
    "print \"ok\"\n",
    "data = evaluate_model_on_test(lstm, 'askubuntu/dev.txt')\n",
    "eval = Evaluation(data)\n",
    "print \"MAP: \",eval.MAP()\n",
    "print \"MRR: \", eval.MRR()\n",
    "print \"P@1: \",eval.Precision(1)\n",
    "print \"P@5: \",eval.Precision(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.utils.data' from '/Users/nicoleobrien/anaconda/envs/python2/lib/python2.7/site-packages/torch/utils/data/__init__.pyc'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toy_batch(seed=11, shape=(25, 1000, 123), classes=10):\n",
    "    batch_size, max_len, features = shape\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Samples\n",
    "    bX = np.float32(np.random.uniform(-1, 1, (shape)))\n",
    "    b_lenX = np.int32(np.linspace(max_len, max_len, batch_size))\n",
    "    # print('::: Lengths of samples in batch: {}'.format(b_lenX))\n",
    "\n",
    "    # Targets\n",
    "    bY = np.int32(np.random.randint(low=0, high=classes - 1, size=batch_size))\n",
    "\n",
    "    return bX, b_lenX, bY, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bX, b_lenX, bY, classes = toy_batch()\n",
    "batch_size, max_len, features = bX.shape\n",
    "\n",
    "# PyTorch compatibility: time first, batch second\n",
    "bX = np.transpose(bX, (1, 0, 2))\n",
    "\n",
    "# Create symbolic vars\n",
    "bX = Variable(torch.from_numpy(bX))\n",
    "bX = rnn_utils.pack_padded_sequence(bX, b_lenX[::-1])  # Pack those sequences for masking, plz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
