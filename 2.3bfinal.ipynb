{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------IMPORTS-------------------------------------#\n",
    "import torch\n",
    "import gzip\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import linalg as LA\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------GLOBAL VARIABLES-------------------------------------#\n",
    "NUM_TRAINING_EXAMPLES = 1000 #22853 #FOR DATA BATCHER, WHEN DEPLOYED SHOULD BE ALL TRAINING EXAMPLES\n",
    "PARAMETER_MAX_TITLE_LENGTH = 38\n",
    "NUM_FEATURES_PER_WORD = 300 #DO NOT CHANGE. FIXED at 300 (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------HELPER FUNCTIONS-------------------------------------#\n",
    "NUM_NEGATIVE_SAMPLES=20\n",
    "\n",
    "def N_random_values_in_list(full_list, N):\n",
    "    x=0\n",
    "    lower_bound  = 0\n",
    "    upper_bound = len(full_list)-1\n",
    "    sample_list=[]\n",
    "    random_nums=[]\n",
    "    while x < min(N,len(full_list)):\n",
    "        random_num = randint(lower_bound, upper_bound) # inclusive range\n",
    "        if random_num in random_nums:\n",
    "            continue\n",
    "        else:\n",
    "            random_nums.append(random_num)\n",
    "            x += 1\n",
    "    return [full_list[i] for i in random_nums]\n",
    "\n",
    "def convert_to_list(filename, is_android = False):\n",
    "    if filename.endswith('gz'):\n",
    "        with gzip.open(filename,'r')as f:\n",
    "            text_tokens = f.readlines()\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            text_tokens = f.readlines()\n",
    "    \n",
    "    text_tokens = [token.replace('\\n','').split('\\t') for token in text_tokens]\n",
    "    if is_android:\n",
    "        text_tokens = [token[0].split(' ') for token in text_tokens]\n",
    "    else:\n",
    "        text_tokens = [[token[0], token[1].split(' '), token[2].split(' ')] for token in text_tokens]\n",
    "                   \n",
    "    return text_tokens\n",
    "\n",
    "#Sample:question_id, similar_question_id, negative_question_id\n",
    "def convert_to_samples_ubuntu(filename):\n",
    "    my_list=convert_to_list(filename)\n",
    "    new_samples=[]\n",
    "    for original_sample in my_list:\n",
    "        for similar in original_sample[1]:\n",
    "            random_negative_samples = N_random_values_in_list(original_sample[2],NUM_NEGATIVE_SAMPLES)\n",
    "            new_samples.append([original_sample[0], similar, random_negative_samples])# change this to include all negative \n",
    "                                                                                 # examples later\n",
    "    return new_samples\n",
    "\n",
    "def convert_to_samples_android(pos_filename, neg_filename):\n",
    "    pos_list = convert_to_list(pos_filename, is_android = True)\n",
    "    neg_list = convert_to_list(neg_filename, is_android = True)\n",
    "\n",
    "    target_id_to_list_of_negative_ids = {}\n",
    "    for id_pair in neg_list:\n",
    "        target_id, negative_id = id_pair\n",
    "        if target_id in target_id_to_list_of_negative_ids:\n",
    "            target_id_to_list_of_negative_ids[target_id].append(negative_id)\n",
    "        else:\n",
    "            target_id_to_list_of_negative_ids[target_id] = [negative_id]\n",
    "    new_samples = []\n",
    "    for id_pair in pos_list:\n",
    "        target_id, positive_id = id_pair\n",
    "        negative_ids = target_id_to_list_of_negative_ids[target_id]\n",
    "        new_sample = [target_id, positive_id, negative_ids]\n",
    "        new_samples.append(new_sample)\n",
    "    return new_samples\n",
    "def make_lookup_table(filename):\n",
    "    lookup={}\n",
    "    text_token_list=convert_to_list(filename)\n",
    "    for token in text_token_list:\n",
    "        lookup[token[0]] = {'title':token[1],'question':token[2]}\n",
    "    return lookup\n",
    "        \n",
    "#takes  sample_ids of [[q1,p1,n1],[q2,p2,n2]....]\n",
    "#outputs titles like [[q1_title, p1_title, n1_title],[q2_title,p2_title,n2_title]...]\n",
    "def convert_sampleids_to_titles(sample_ids,lookup):\n",
    "    #each sample_id [question_id, pos_id, [neg_ids]]\n",
    "    #print type(sample_ids)==list, \"first\"\n",
    "   \n",
    "    titles = []\n",
    "    for sample_id in sample_ids:\n",
    "         #flatten list: [question_id, pos_id, [neg_ids]] --> [question_id, pos_id, neg_id1, neg_id2, ...]\n",
    "        sample_id= sample_id[:2]+sample_id[2][:]\n",
    "        #sample_id : question_id, similar_question_id, negative_question_id\n",
    "        try:\n",
    "            titles.append([[item.lower() for item in lookup[str(_)]['title']] for _ in sample_id])\n",
    "        except:\n",
    "            print \"Lookup failed\"\n",
    "    return titles\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "def extract_features(word):\n",
    "    try:\n",
    "        word=remove_non_ascii(word)\n",
    "        word=word.encode('utf-8')\n",
    "    except:\n",
    "        print(word)\n",
    "    return word_to_vec.get(word, [0.0 for i in range(NUM_FEATURES_PER_WORD)])\n",
    "\n",
    "def find_maximum_title_and_body_length(lookup_table):\n",
    "    max_len_title = -1\n",
    "    max_len_question = -1\n",
    "    max_len_question_id = 0\n",
    "    for key, dict_val in lookup_table.iteritems():\n",
    "        len_title = len(dict_val['title'])\n",
    "        len_question = len(dict_val['question'])\n",
    "        if len_title > max_len_title:\n",
    "             max_len_title = len_title\n",
    "        if len_question > max_len_question:\n",
    "            max_len_question = len_question\n",
    "            max_len_question_id = key\n",
    "    return max_len_title, max_len_question\n",
    "\n",
    "def title_to_feature_matrix(title_word_list):\n",
    "    feature_matrix = []\n",
    "    for idx, word in enumerate(title_word_list):\n",
    "        if idx == PARAMETER_MAX_TITLE_LENGTH:\n",
    "            print \"Here\"\n",
    "            break\n",
    "        else:\n",
    "            word_features = extract_features(word)\n",
    "#             print \"Word features length: {}\".format(len(word_features))\n",
    "            feature_matrix.append(word_features)\n",
    "    \n",
    "    #Pad the feature with zeros to ensure all inputs to the net have the same dimension\n",
    "    feature_matrix += [[0] * NUM_FEATURES_PER_WORD] * (PARAMETER_MAX_TITLE_LENGTH - len(title_word_list))\n",
    "#     print \"Feature matrix shape: {}\".format(np.array(feature_matrix).T.shape)\n",
    "    return np.array(feature_matrix).T\n",
    "\n",
    "#array is structured like a batch of features 50x200x38\n",
    "def find_start_of_padding_for_batch(batch):\n",
    "    vec_lengths_in_batch = []\n",
    "    for batch_num in range(0, len(batch)):\n",
    "        single_vec = batch[batch_num]\n",
    "        length = find_start_of_padding_single_vec(single_vec) + 1\n",
    "        vec_lengths_in_batch.append(length)\n",
    "    return vec_lengths_in_batch\n",
    "\n",
    "#batch = 200x38\n",
    "def find_start_of_padding_single_vec(single_vec):\n",
    "    for idx in range(len(single_vec[0])-1, -1, -1):\n",
    "        if single_vec[0][idx] != 0.:\n",
    "            return idx\n",
    "    #if the whole sequence is 0s\n",
    "    return 0\n",
    "def create_mask(word_length):\n",
    "    return np.array([[1. / word_length] * CNN_HIDDEN_DIM] * word_length + [[0] * CNN_HIDDEN_DIM] * (MAX_TITLE_LENGTH - word_length)).T\n",
    "#-------------------------------------CREATE DATA BATCHER-------------------------------------#\n",
    "# where samples[0] = 1 (target) + 1 (positive) + n (negative) \n",
    "def create_data_loader(ubuntu_samples, android_samples, shuffle_data = True):\n",
    "    \n",
    "    features = []\n",
    "    #0: ubuntu dataset\n",
    "    #1: android dataset\n",
    "    for idxe, samples in enumerate([ubuntu_samples, android_samples]):\n",
    "        for sample in samples:\n",
    "            from_dataset = idx\n",
    "            target_title = sample[0]\n",
    "            positive_title = sample[1]\n",
    "            negative_titles = sample[2:22] #:22 because there are more negatives in the android dataset, but a TA\n",
    "                                           # said the extras aren't necessary\n",
    "\n",
    "            target_features = title_to_feature_matrix(target_title)\n",
    "            positive_features = title_to_feature_matrix(positive_title)\n",
    "            n_negative_features = [title_to_feature_matrix(negative_title) for negative_title in negative_titles]\n",
    "\n",
    "            from_dataset = [[from_dataset] * MAX_TITLE_LENGTH] * NUM_FEATURES_PER_WORD\n",
    "            all_features = [from_dataset, target_features, positive_features] + n_negative_features\n",
    "            \n",
    "            features.append(all_features)\n",
    "    \n",
    "    targets = torch.LongTensor(len(features)).zero_()\n",
    "    dataset = data_utils.TensorDataset(torch.FloatTensor(np.array(features)), targets)\n",
    "    data_loader = data_utils.DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = shuffle_data)\n",
    "    return data_loader\n",
    "\n",
    "def create_data_loader_test(android_samples, shuffle_data = True):\n",
    "    features = []\n",
    "\n",
    "    for sample in android_samples:\n",
    "        target_title = sample[0]\n",
    "        positive_title = sample[1]\n",
    "        negative_titles = sample[2:] #:22 because there are more negatives in the android dataset, but a TA\n",
    "                                       # said the extras aren't necessary\n",
    "\n",
    "        target_features = title_to_feature_matrix(target_title)\n",
    "        positive_features = title_to_feature_matrix(positive_title)\n",
    "        n_negative_features = [title_to_feature_matrix(negative_title) for negative_title in negative_titles]\n",
    "\n",
    "        all_features = [target_features, positive_features] + n_negative_features\n",
    "\n",
    "        features.append(all_features)\n",
    "        \n",
    "    return features\n",
    "\n",
    "def sample_and_label_to_cosine_sims(sample, model = None):\n",
    "    #RE-ORDER DIMENSIONS OF THE SAMPLE\n",
    "    sample = sample.permute(1, 0, 2, 3)\n",
    "    if model != None:\n",
    "        net = model\n",
    "    \n",
    "    target_question_features = sample[0] # 50 x 200 x 38\n",
    "    positive_question_features = sample[1] # 50 x 200 x 38\n",
    "    N_negative_question_features = sample[2:] #20 x 50 x 200 x 38\n",
    "\n",
    "    #Determine lengths to know how many vectors to take the average across.\n",
    "    target_question_lengths = find_start_of_padding_for_batch(target_question_features.data)\n",
    "    positive_question_lengths = find_start_of_padding_for_batch(positive_question_features.data)\n",
    "    N_negative_questions_lengths = [find_start_of_padding_for_batch(negative.data) for negative in N_negative_question_features]\n",
    "\n",
    "    #RUN THROUGH NET\n",
    "    target_question_net_output = net(target_question_features)\n",
    "    positive_question_net_output = net(positive_question_features)\n",
    "    N_negative_question_net_outputs = [net(negative) for negative in N_negative_question_features]\n",
    "\n",
    "    #CREATE MASKS\n",
    "    target_questions_masks = [create_mask(_) for _ in target_question_lengths] #DIM = 50 x 100 x 38\n",
    "    positive_questions_masks = [create_mask(_) for _ in positive_question_lengths] #DIM = 50 x 100 x 38\n",
    "    N_negative_questions_masks = [[create_mask(length) for length in length_list] #DIM = 50 x 20 x 100 x 38\n",
    "                                  for length_list in N_negative_questions_lengths]\n",
    "\n",
    "    #APPLY MASKS\n",
    "    #Should the multiplicands, the masks, be Float Tensors or Variables? May have to be float tensors to ensure\n",
    "    #    pytorch's directed graph back-prop is maintained.\n",
    "    target_question_net_output_masked = target_question_net_output * Variable(torch.FloatTensor(target_questions_masks))\n",
    "    positive_question_net_output_masked = positive_question_net_output * Variable(torch.FloatTensor(positive_questions_masks))\n",
    "    N_negative_questions_net_output_masked = [N_negative_question_net_outputs[idx] * \n",
    "                                              Variable(torch.FloatTensor(N_negative_questions_masks[idx]))\n",
    "                                              for idx in range(NUM_NEGATIVE_SAMPLES)]\n",
    "    #SUM OVER WORDS\n",
    "    target_question_net_output_masked_summed = torch.sum(target_question_net_output_masked, dim = 2) #DIM = 50 x 100\n",
    "    positive_question_net_output_masked_summed = torch.sum(positive_question_net_output_masked, dim = 2) #DIM = 50 x 100\n",
    "    N_negative_questions_net_output_masked_summed = [torch.sum(\n",
    "                                                    N_negative_questions_net_output_masked[idx], dim = 2\n",
    "                                                    )for idx in range(NUM_NEGATIVE_SAMPLES)] #DIM = 20 x 50 x 100\n",
    "\n",
    "    net_outputs_batch = torch.stack([target_question_net_output_masked_summed, positive_question_net_output_masked_summed] + N_negative_questions_net_output_masked_summed)\n",
    "    \n",
    "    cosine_similarity_pos = cos(target_question_net_output_masked_summed, positive_question_net_output_masked_summed)\n",
    "    # ^ DIM = 50\n",
    "    cosine_similarities_neg = [cos(target_question_net_output_masked_summed, N_negative_questions_net_output_masked_summed[idx])\n",
    "                              for idx in range(NUM_NEGATIVE_SAMPLES)]\n",
    "    # ^ DIM = 20 x 50\n",
    "    #\n",
    "\n",
    "    cosine_similarities = torch.stack([cosine_similarity_pos] + cosine_similarities_neg) # DIM = 21 x 50\n",
    "    return cosine_similarities, net_outputs_batch\n",
    "# helper class used for computing information retrieval metrics, including MAP / MRR / and Precision @ x\n",
    "class Evaluation():\n",
    "\n",
    "    def __init__(self,data):\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "\n",
    "    def Precision(self,precision_at):\n",
    "        scores = []\n",
    "        for item in self.data:\n",
    "            temp = item[:precision_at]\n",
    "            if any(val==1 for val in item):\n",
    "                scores.append(sum([1 if val==1 else 0 for val in temp])*1.0 / len(temp) if len(temp) > 0 else 0.0)\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "\n",
    "    def MAP(self):\n",
    "        scores = []\n",
    "        missing_MAP = 0\n",
    "        for item in self.data:\n",
    "            temp = []\n",
    "            count = 0.0\n",
    "            for i,val in enumerate(item):\n",
    "                if val == 1:\n",
    "                    count += 1.0\n",
    "                    temp.append(count/(i+1))\n",
    "            if len(temp) > 0:\n",
    "                scores.append(sum(temp) / len(temp))\n",
    "            else:\n",
    "                missing_MAP += 1\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "\n",
    "    def MRR(self):\n",
    "\n",
    "        scores = []\n",
    "        for item in self.data:\n",
    "            for i,val in enumerate(item):\n",
    "                if val == 1:\n",
    "                    scores.append(1.0/(i+1))\n",
    "                    break\n",
    "\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "def mse_loss(input, target):\n",
    "    return torch.sum((input - target) ** 2) / input.data.nelement()\n",
    "# where samples[0] = 1 (target) + 1 (positive) + n (negative) \n",
    "def create_data_loader_ubuntu(samples, shuffle_data = True, is_test_or_dev = False):\n",
    "    features = []\n",
    "    count = 0\n",
    "    start = time.time()\n",
    "    for sample in samples: #REMOVE THE 2 WHEN RUNNING FOR REAL\n",
    "        target_title = sample[0]\n",
    "        positive_title = sample[1]\n",
    "        negative_titles = sample[2:]\n",
    "\n",
    "        target_features = title_to_feature_matrix(target_title)\n",
    "        positive_features = title_to_feature_matrix(positive_title)\n",
    "        n_negative_features = [title_to_feature_matrix(negative_title) for negative_title in negative_titles]\n",
    "#         print \"Target features length: {}\".format(len(target_features))\n",
    "#         print \"Positive feeatures length: {}\".format(len(positive_features))\n",
    "#         print \"Num negative features: {}\".format(len(n_negative_features))\n",
    "#         print \"Negative features length: {}\".format(len(n_negative_features[0]))\n",
    "        \n",
    "        all_features = [target_features, positive_features] + n_negative_features            \n",
    "        features.append(all_features)\n",
    "        count += 1\n",
    "        if count == 10:\n",
    "            print \"Time to iterate over 10 samples: {}\".format(time.time() - start)\n",
    "            \n",
    "    targets = torch.LongTensor(len(features)).zero_()\n",
    "#     print \"Targets shape: {}\".format(targets.shape)\n",
    "\n",
    "    ft = torch.FloatTensor(features)\n",
    "    dataset = data_utils.TensorDataset(ft, targets)\n",
    "    data_loader = data_utils.DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = shuffle_data)\n",
    "    return data_loader\n",
    "\n",
    "def create_dataset(samples, shuffle_data = True, is_test_or_dev = False):\n",
    "    features = []\n",
    "    for sample in samples:\n",
    "        target_title = sample[0]\n",
    "        positive_title = sample[1]\n",
    "        negative_titles = sample[2:]\n",
    "\n",
    "        target_features = title_to_feature_matrix(target_title)\n",
    "        positive_features = title_to_feature_matrix(positive_title)\n",
    "        if len(negative_titles)==0:\n",
    "            all_features=[target_features, positive_features]\n",
    "        else:\n",
    "            n_negative_features = [title_to_feature_matrix(negative_title) for negative_title in negative_titles]\n",
    "            all_features = [target_features, positive_features] + n_negative_features\n",
    "        features.append(all_features)\n",
    "    \n",
    "    targets = torch.LongTensor(len(features)).zero_()\n",
    "    dataset = data_utils.TensorDataset(torch.FloatTensor(features), targets)\n",
    "    return dataset\n",
    "\n",
    "def create_data_loader_large_size_ubuntu(samples):\n",
    "    train_data_loaders=[]\n",
    "    for i in range(0,NUM_TRAINING_EXAMPLES,50):\n",
    "        train_data_loader = create_dataset(samples[i:i+50])\n",
    "        print \"made training data loader\", i\n",
    "        train_data_loaders.append(train_data_loader)\n",
    "    final_train_dataset=data_utils.ConcatDataset(train_data_loaders)\n",
    "    final_train_data_loader=data_utils.DataLoader(final_train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    return final_train_data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ubuntu_text_tokenized_filename = 'askubuntu/text_tokenized.txt.gz'\n",
    "android_text_tokenized_filename = 'Android/corpus.tsv.gz'\n",
    "ubuntu_data_lookup_table = make_lookup_table(ubuntu_text_tokenized_filename)\n",
    "android_data_lookup_table = make_lookup_table(android_text_tokenized_filename)\n",
    "MAX_TITLE_LENGTH, MAX_BODY_LENGTH = find_maximum_title_and_body_length(ubuntu_data_lookup_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 0.000262975692749\n"
     ]
    }
   ],
   "source": [
    "start = time.time() #------------- TAKES 0.0003 SECONDS ---------------#\n",
    "#-------------------------------------NET GLOBAL VARIABLES-------------------------------------#\n",
    "#text_tokenized.txt.gz has id \\t title \\t question body\n",
    "\n",
    "KERNEL_SIZE = 3 #MAKE SURE THIS NUMBER IS ODD SO THAT THE PADDING MAKES SENSE\n",
    "PADDING = (KERNEL_SIZE - 1) / 2\n",
    "\n",
    "CNN_INPUT_DIM = 300\n",
    "CNN_HIDDEN_DIM = 125 #*\n",
    "#NUM LAYERS\n",
    "LEARNING_RATE = 1e-3 \n",
    "MARGIN = 0.1 #*\n",
    "NUM_EPOCHS = 8\n",
    "BATCH_SIZE = 50 \n",
    "NUM_BATCHES = NUM_TRAINING_EXAMPLES/BATCH_SIZE\n",
    "end = time.time()\n",
    "print \"Time Elapsed: {}\".format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 403.961707115\n"
     ]
    }
   ],
   "source": [
    "start = time.time() # ---------------TAKES 7 MINUTES ---------------#\n",
    "#-------------------------------------GENERATE EMBEDDINGS-------------------------------------#\n",
    "import zipfile\n",
    "# word_embeddings = 'askubuntu/vector/vectors_pruned.200.txt.gz'\n",
    "word_embeddings = 'glove.txt'\n",
    "start = time.time()\n",
    "# f = gzip.open(word_embeddings, 'r')\n",
    "f = open(word_embeddings, 'r')\n",
    "wv_text = []\n",
    "lines = f.readlines()\n",
    "#%%%%%%%%%% TAKES ABOUT 2 MINUTES\n",
    "for idx, line in enumerate(lines):\n",
    "    wv_text.append(line.strip())\n",
    "\n",
    "word_to_vec = {}\n",
    "\n",
    "#%%%%%%%%%% TAKES ABOUT 4 MINUTES\n",
    "for line in wv_text:\n",
    "    parts = line.split()\n",
    "    word = parts[0]\n",
    "    vector = np.array([float(v) for v in parts[1:]])\n",
    "    word_to_vec[word] = vector\n",
    "f.close()\n",
    "end = time.time()\n",
    "print \"Time Elapsed: {}\".format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made training data loader 0\n",
      "made training data loader 50\n",
      "made training data loader 100\n",
      "made training data loader 150\n",
      "made training data loader 200\n",
      "made training data loader 250\n",
      "made training data loader 300\n",
      "made training data loader 350\n",
      "made training data loader 400\n",
      "made training data loader 450\n",
      "made training data loader 500\n",
      "made training data loader 550\n",
      "made training data loader 600\n",
      "made training data loader 650\n",
      "made training data loader 700\n",
      "made training data loader 750\n",
      "made training data loader 800\n",
      "made training data loader 850\n",
      "made training data loader 900\n",
      "made training data loader 950\n",
      "Time Elapsed: 438.555696964\n"
     ]
    }
   ],
   "source": [
    "start = time.time() #---------------TAKES 30 MINUTES---------------------#\n",
    "#-----------------------------------LOAD UBUNTU DATA----------------------------------#\n",
    "# train_random.txt\n",
    "# (1) the query question ID, (2) the list of similar question IDs, and (3) the list of randomly selected question IDs.\n",
    "ubuntu_train_random_filename='askubuntu/train_random.txt'\n",
    "\n",
    "#Each line contains (1) the query question ID, (2) the list of similar question IDs, (3) the list of 20 candidate question IDs and (4) the associated BM25 scores of these questions computed by the Lucene search engine. The second field (the set of similar questions) is a subset of the third field.\n",
    "ubuntu_dev_filename='askubuntu/dev.txt'\n",
    "ubuntu_test_filename='askubuntu/test.txt'\n",
    "\n",
    "#sample: [question_id, similar_question_id, negative_question_ids]\n",
    "ubuntu_train_samples = convert_to_samples_ubuntu(ubuntu_train_random_filename)\n",
    "# ubuntu_dev_samples = convert_to_samples_ubuntu(ubuntu_dev_filename)\n",
    "# ubuntu_test_samples = convert_to_samples_ubuntu(ubuntu_test_filename)\n",
    "\n",
    "ubuntu_train_titles_only = convert_sampleids_to_titles(ubuntu_train_samples[:NUM_TRAINING_EXAMPLES], ubuntu_data_lookup_table)\n",
    "# ubuntu_train_data_loader = create_data_loader_ubuntu(ubuntu_train_titles_only[:NUM_TRAINING_EXAMPLES])\n",
    "ubuntu_train_data_loader = create_data_loader_large_size_ubuntu(ubuntu_train_titles_only[:NUM_TRAINING_EXAMPLES])\n",
    "# ubuntu_dev_titles_only = convert_sampleids_to_titles(ubuntu_dev_samples, ubuntu_data_lookup_table)\n",
    "# unbuntu_test_titles_only = convert_sampleids_to_titles(ubuntu_test_samples, ubuntu_data_lookup_table)\n",
    "#A FEW LOOKUP FAILURES OCCUR. THE LOOKUPS FAIL ON THE UBUNTU DATA. THERE WILL BE A FEW LESS TRAINING SAMPLES\n",
    "#IN THE UBUNTU DATA AS A RESULT, BUT THAT DOESN'T REALLY MATTER SINCE PART 1 WORKS.\n",
    "\n",
    "#------------------------------------LOAD ANDROID DATA---------------------------------#\n",
    "android_dev_pos_filename = \"Android/dev.pos.txt\"\n",
    "android_dev_neg_filename = \"Android/dev.neg.txt\"\n",
    "\n",
    "android_test_pos_filename = \"Android/test.pos.txt\"\n",
    "android_test_neg_filename = \"Android/test.neg.txt\"\n",
    "\n",
    "android_dev_samples = convert_to_samples_android(android_dev_pos_filename, android_dev_neg_filename)\n",
    "android_test_samples = convert_to_samples_android(android_test_pos_filename, android_test_neg_filename)\n",
    "\n",
    "android_dev_titles_only = convert_sampleids_to_titles(android_dev_samples, android_data_lookup_table)\n",
    "android_test_titles_only = convert_sampleids_to_titles(android_test_samples, android_data_lookup_table)\n",
    "\n",
    "# training_data_loader = create_data_loader(ubuntu_train_titles_only[:NUM_TRAINING_EXAMPLES], android_dev_titles_only[:NUM_TRAINING_EXAMPLES])\n",
    "test_features = create_data_loader_test(android_test_titles_only)\n",
    "end = time.time()\n",
    "print \"Time Elapsed: {}\".format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------CNN-------------------------------------#\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(CNN_INPUT_DIM, hidden_dim, KERNEL_SIZE, padding = PADDING),\n",
    "#             nn.ReLU()\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "Epoch Number: 0, Loss: 0.149179868167, Time Elapsed: 49.7204210758, \n",
      "AUC Value (0.05): 0.288602609981, Margin: 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ed342c810faa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alexander/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alexander/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#-------------------------------------RE-TRAIN ON GLOVE EMBEDDINGS-------------------------------------#\n",
    "MARGINS = [0.01, 0.1, 0.2, 0.3, 0.5, 2.0]\n",
    "\n",
    "for MARGIN in MARGINS:\n",
    "\n",
    "    INPUT_DIM = (MAX_TITLE_LENGTH, NUM_FEATURES_PER_WORD)\n",
    "    net = CNN(CNN_INPUT_DIM, CNN_HIDDEN_DIM)\n",
    "    criterion = nn.MultiMarginLoss(p=1, margin=MARGIN, weight = None, size_average=True) #HAHA just put these in to look smart \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    print \"Beginning training...\"\n",
    "    # ----TRAINING\n",
    "    net_name = \"iter_net\"\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        epoch_start_time = time.time()\n",
    "        for idx,(sample, label) in enumerate(ubuntu_train_data_loader):\n",
    "\n",
    "    #     for idx in range(NUM_BATCHES):\n",
    "            #Sample shape: [50, 22, 200, 38]\n",
    "            #50 - Batch size, 22 - Num questions per data point\n",
    "            sample = Variable(sample, requires_grad = True)\n",
    "            label = Variable(label, requires_grad = True)\n",
    "\n",
    "            #RE-ORDER DIMENSIONS OF THE SAMPLE\n",
    "            sample = sample.permute(1, 0, 2, 3)\n",
    "\n",
    "            target_question_features = sample[0] # 50 x 200 x 38\n",
    "            positive_question_features = sample[1] # 50 x 200 x 38\n",
    "            N_negative_question_features = sample[2:] #20 x 50 x 200 x 38\n",
    "\n",
    "            #Determine lengths to know how many vectors to take the average across.\n",
    "            target_question_lengths = find_start_of_padding_for_batch(target_question_features.data)\n",
    "            positive_question_lengths = find_start_of_padding_for_batch(positive_question_features.data)\n",
    "            N_negative_questions_lengths = [find_start_of_padding_for_batch(negative.data) for negative in N_negative_question_features]\n",
    "\n",
    "            #RUN THROUGH NET\n",
    "            target_question_net_output = net(target_question_features)\n",
    "            positive_question_net_output = net(positive_question_features)\n",
    "            N_negative_question_net_outputs = [net(negative) for negative in N_negative_question_features]\n",
    "\n",
    "            #CREATE MASKS\n",
    "            target_questions_masks = [create_mask(_) for _ in target_question_lengths] #DIM = 50 x 100 x 38\n",
    "            positive_questions_masks = [create_mask(_) for _ in positive_question_lengths] #DIM = 50 x 100 x 38\n",
    "            N_negative_questions_masks = [[create_mask(length) for length in length_list] #DIM = 50 x 20 x 100 x 38\n",
    "                                          for length_list in N_negative_questions_lengths]\n",
    "\n",
    "            #APPLY MASKS\n",
    "            #Should the multiplicands, the masks, be Float Tensors or Variables? May have to be float tensors to ensure\n",
    "            #    pytorch's directed graph back-prop is maintained.\n",
    "\n",
    "            target_question_net_output_masked = target_question_net_output * Variable(torch.FloatTensor(target_questions_masks))\n",
    "            positive_question_net_output_masked = positive_question_net_output * Variable(torch.FloatTensor(positive_questions_masks))\n",
    "            N_negative_questions_net_output_masked = [N_negative_question_net_outputs[idx] * \n",
    "                                                      Variable(torch.FloatTensor(N_negative_questions_masks[idx]))\n",
    "                                                      for idx in range(NUM_NEGATIVE_SAMPLES)]\n",
    "\n",
    "            #SUM OVER WORDS\n",
    "            target_question_net_output_masked_summed = torch.sum(target_question_net_output_masked, dim = 2) #DIM = 50 x 100\n",
    "            positive_question_net_output_masked_summed = torch.sum(positive_question_net_output_masked, dim = 2) #DIM = 50 x 100\n",
    "            N_negative_questions_net_output_masked_summed = [torch.sum(\n",
    "                                                            N_negative_questions_net_output_masked[idx], dim = 2\n",
    "                                                            )for idx in range(NUM_NEGATIVE_SAMPLES)] #DIM = 20 x 50 x 100\n",
    "\n",
    "\n",
    "            cosine_similarity_pos = cos(target_question_net_output_masked_summed, positive_question_net_output_masked_summed)\n",
    "            # ^ DIM = 50\n",
    "            cosine_similarities_neg = [cos(target_question_net_output_masked_summed, N_negative_questions_net_output_masked_summed[idx])\n",
    "                                      for idx in range(NUM_NEGATIVE_SAMPLES)]\n",
    "            # ^ DIM = 20 x 50\n",
    "            #\n",
    "\n",
    "            cosine_similarities = torch.stack([cosine_similarity_pos] + cosine_similarities_neg) # DIM = 21 x 50\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            cosine_similarities = torch.t(cosine_similarities)\n",
    "            label = torch.squeeze(label)\n",
    "            loss = criterion(cosine_similarities, label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            running_loss += loss.data[0]\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        torch.save(net, net_name)\n",
    "    #     print \"Loss after Epoch \" + str(epoch) + \" :\" + str(running_loss)\n",
    "        print \"Epoch Number: {}, Loss: {}, Time Elapsed: {}, \".format(epoch, running_loss, time.time() - epoch_start_time)\n",
    "    evaluate_auc(MARGIN)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------TAKES 12 MINUTES-----------------------------------_#\n",
    "def evaluate_auc(margin):\n",
    "    start = time.time()\n",
    "    auc_evaluator = AUCMeter()\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "#     part_1_model = torch.load(\"Direct_Transfer_Nets/All_Data_Epoch_7\")\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    loaded_net = torch.load(net_name)\n",
    "    for feature in test_features:\n",
    "        feature = Variable(torch.FloatTensor(feature))\n",
    "        feature = feature.unsqueeze(0)\n",
    "        labels = Variable(torch.LongTensor([1] + [0] * 20))\n",
    "        cosine_sims, _ = sample_and_label_to_cosine_sims(feature, model = loaded_net)\n",
    "        if (np.argmax(cosine_sims.data.numpy()) == 0):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        for idx in range(len(cosine_sims)):\n",
    "            output = cosine_sims[idx].data.numpy()[0]\n",
    "            label = labels[idx].data.numpy()[0]\n",
    "            auc_evaluator.add(output, label)\n",
    "            outputs.append(output)\n",
    "            targets.append(label)\n",
    "\n",
    "    auc_evaluator.scores = np.array(outputs)\n",
    "    auc_evaluator.targets = np.array(targets)\n",
    "    print \"AUC Value (0.05): {}, Margin: {}\".format(auc_evaluator.value(0.05), margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Code took from PyTorchNet (https://github.com/pytorch/tnt)\n",
    "\n",
    "'''\n",
    "\n",
    "import math\n",
    "import numbers\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Meter(object):\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def add(self):\n",
    "        pass\n",
    "\n",
    "    def value(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AUCMeter(Meter):\n",
    "    \"\"\"\n",
    "    The AUCMeter measures the area under the receiver-operating characteristic\n",
    "    (ROC) curve for binary classification problems. The area under the curve (AUC)\n",
    "    can be interpreted as the probability that, given a randomly selected positive\n",
    "    example and a randomly selected negative example, the positive example is\n",
    "    assigned a higher score by the classification model than the negative example.\n",
    "\n",
    "    The AUCMeter is designed to operate on one-dimensional Tensors `output`\n",
    "    and `target`, where (1) the `output` contains model output scores that ought to\n",
    "    be higher when the model is more convinced that the example should be positively\n",
    "    labeled, and smaller when the model believes the example should be negatively\n",
    "    labeled (for instance, the output of a signoid function); and (2) the `target`\n",
    "    contains only values 0 (for negative examples) and 1 (for positive examples).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AUCMeter, self).__init__()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # self.scores = torch.DoubleTensor(torch.DoubleStorage()).numpy()\n",
    "        # self.targets = torch.LongTensor(torch.LongStorage()).numpy()\n",
    "        self.scores = []\n",
    "        self.targets = []\n",
    "\n",
    "    def add(self, output, target):\n",
    "        if torch.is_tensor(output):\n",
    "            output = output.cpu().squeeze().numpy()\n",
    "        if torch.is_tensor(target):\n",
    "            target = target.cpu().squeeze().numpy()\n",
    "        # elif isinstance(target, numbers.Number):\n",
    "        #     target = np.asarray([target])\n",
    "        # assert np.ndim(output) == 1, \\\n",
    "        #     'wrong output size (1D expected)'\n",
    "        # assert np.ndim(target) == 1, \\\n",
    "        #     'wrong target size (1D expected)'\n",
    "        # assert output.shape[0] == target.shape[0], \\\n",
    "        #     'number of outputs and targets does not match'\n",
    "        # assert np.all(np.add(np.equal(target, 1), np.equal(target, 0))), \\\n",
    "        #     'targets should be binary (0, 1)'\n",
    "\n",
    "        # self.scores = np.append(self.scores, output)\n",
    "        # self.targets = np.append(self.targets, target)\n",
    "        self.sortind = None\n",
    "        self.scores.append(output)\n",
    "        self.scores.append(target)\n",
    "\n",
    "\n",
    "    def value(self, max_fpr=1.0):\n",
    "        # self.scores = torch.FloatTensor(self.scores)\n",
    "        # self.target = torch.LongTensor(self.targets)\n",
    "        \n",
    "        assert max_fpr > 0\n",
    "\n",
    "        # case when number of elements added are 0\n",
    "        if self.scores.shape[0] == 0:\n",
    "            return 0.5\n",
    "\n",
    "        # sorting the arrays\n",
    "        if self.sortind is None:\n",
    "            scores, sortind = torch.sort(torch.from_numpy(self.scores), dim=0, descending=True)\n",
    "            scores = scores.numpy()\n",
    "            self.sortind = sortind.numpy()\n",
    "        else:\n",
    "            scores, sortind = self.scores, self.sortind\n",
    "\n",
    "        # creating the roc curve\n",
    "        tpr = np.zeros(shape=(scores.size + 1), dtype=np.float64)\n",
    "        fpr = np.zeros(shape=(scores.size + 1), dtype=np.float64)\n",
    "\n",
    "        for i in range(1, scores.size + 1):\n",
    "            if self.targets[sortind[i - 1]] == 1:\n",
    "                tpr[i] = tpr[i - 1] + 1\n",
    "                fpr[i] = fpr[i - 1]\n",
    "            else:\n",
    "                tpr[i] = tpr[i - 1]\n",
    "                fpr[i] = fpr[i - 1] + 1\n",
    "\n",
    "        tpr /= (self.targets.sum() * 1.0)\n",
    "        fpr /= ((self.targets - 1.0).sum() * -1.0)\n",
    "\n",
    "        for n in range(1, scores.size + 1):\n",
    "            if fpr[n] >= max_fpr:\n",
    "                break\n",
    "\n",
    "        # calculating area under curve using trapezoidal rule\n",
    "        #n = tpr.shape[0]\n",
    "        h = fpr[1:n] - fpr[0:n - 1]\n",
    "        sum_h = np.zeros(fpr.shape)\n",
    "        sum_h[0:n - 1] = h\n",
    "        sum_h[1:n] += h\n",
    "        area = (sum_h * tpr).sum() / 2.0\n",
    "\n",
    "        return area / max_fpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
